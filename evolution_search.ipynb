{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4DiJi9l2v3dP"
   },
   "outputs": [],
   "source": [
    "## Experimental Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 949,
     "status": "ok",
     "timestamp": 1578951674638,
     "user": {
      "displayName": "cliff akosa",
      "photoUrl": "",
      "userId": "08793119188384687776"
     },
     "user_tz": -540
    },
    "id": "xqkm5Gz-OxHp",
    "outputId": "c0d492dd-f179-41b9-f17a-10f003d7663d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! Good to go!\n"
     ]
    }
   ],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.distributed import *\n",
    "from fastai.metrics import error_rate\n",
    "from fastai.callbacks import SaveModelCallback\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  print(\"Great! Good to go!\")\n",
    "else:\n",
    "  print('CUDA is not up!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GqrHvPgFOxH1"
   },
   "outputs": [],
   "source": [
    "# from gepcore.utils import cell_graph\n",
    "# from gepnet.model_v2 import get_gepnet, arch_config\n",
    "# from gepnet.utils import count_parameters\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "\n",
    "from gepcore.utils import cell_graph, convolution\n",
    "from gepcore.entity import Gene, Chromosome\n",
    "from gepcore.symbol import PrimitiveSet\n",
    "from gepnet.model_v1 import get_net, arch_config\n",
    "from gepnet.utils import count_parameters\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tRmHVNGcOxH_"
   },
   "outputs": [],
   "source": [
    "from pygraphviz import AGraph\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kX9ZLNQKOxIH"
   },
   "outputs": [],
   "source": [
    "graph = [AGraph(g) for g in glob.glob('comp_graphs/new/*.dot')]\n",
    "_, comp_graph = cell_graph.generate_comp_graph(graph)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate new chromosome\n",
    "# define primitive set\n",
    "pset = PrimitiveSet('cnn')\n",
    "\n",
    "# add cellular encoding program symbols\n",
    "pset.add_program_symbol(cell_graph.end)\n",
    "pset.add_program_symbol(cell_graph.seq)\n",
    "pset.add_program_symbol(cell_graph.cpo)\n",
    "pset.add_program_symbol(cell_graph.cpi)\n",
    "\n",
    "# add convolutional operations symbols\n",
    "conv_symbol = convolution.get_symbol()\n",
    "pset.add_cell_symbol(conv_symbol.conv3x3)\n",
    "#pset.add_cell_symbol(conv_symbol.sepconv5x5)\n",
    "#pset.add_cell_symbol(conv_symbol.dilconv3x3)\n",
    "#pset.add_cell_symbol(conv_symbol.dilconv5x5)\n",
    "pset.add_cell_symbol(conv_symbol.conv1x1)\n",
    "pset.add_cell_symbol(conv_symbol.dwconv3x3)\n",
    "#pset.add_cell_symbol(conv_symbol.maxpool3x3)\n",
    "\n",
    "def gene_gen():\n",
    "    return Gene(pset, 2)\n",
    "ch = Chromosome(gene_gen, 4)\n",
    "graph, comp_graph = cell_graph.generate_comp_graph(ch)\n",
    "\n",
    "cell_graph.save_graph(graph, 'comp_graphs/new')\n",
    "cell_graph.draw_graph(graph, 'comp_graphs/new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Vy6XZZ_dakd"
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "seed = 200\n",
    "# random.seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# # enable torch backends\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "#torch.backends.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1310,
     "status": "ok",
     "timestamp": 1578951397277,
     "user": {
      "displayName": "cliff akosa",
      "photoUrl": "",
      "userId": "08793119188384687776"
     },
     "user_tz": -540
    },
    "id": "2-n0NQqMOxIV",
    "outputId": "d83ce923-461f-42b2-8b8d-5cfd78a73afa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.78781"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = arch_config(comp_graph=comp_graph,\n",
    "                   #depth_coeff=1.0,\n",
    "                   #width_coeff=1.0,\n",
    "                   channels=40,\n",
    "                   repeat_list=[3, 4, 3],\n",
    "                   classes=10)\n",
    "\n",
    "net = get_net(conf)\n",
    "count_parameters(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33535,
     "status": "ok",
     "timestamp": 1578951487726,
     "user": {
      "displayName": "cliff akosa",
      "photoUrl": "",
      "userId": "08793119188384687776"
     },
     "user_tz": -540
    },
    "id": "FUQrGFI6OxIc",
    "outputId": "f670e392-7967-43dc-e954-d8af0ce7cba4"
   },
   "outputs": [],
   "source": [
    "size=32\n",
    "cutout_frac = 0.30\n",
    "p_cutout = 0.75\n",
    "cutout_sz = round(size*cutout_frac)\n",
    "cutout_tfm = cutout(n_holes=(2,2), length=(cutout_sz, cutout_sz), p=p_cutout)\n",
    "\n",
    "path = untar_data(URLs.CIFAR)\n",
    "tfms = get_transforms(xtra_tfms=[cutout_tfm])#do_flip=False,\n",
    "                      #xtra_tfms=[RandTransform(tfm=cutout, kwargs={'n_holes': (1,1)})])\n",
    "bs =128\n",
    "data = (ImageList.from_folder(path/'train')\n",
    "        .split_by_rand_pct(valid_pct=0.1, seed=seed) \n",
    "        .label_from_folder()\n",
    "        .transform(tfms, size=32)\n",
    "        .databunch(bs=bs, num_workers=num_cpus())\n",
    "        .normalize(cifar_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "6VPHyBIYOxIh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=ImageDataBunch;\n",
       "\n",
       "Train: LabelList (45000 items)\n",
       "x: ImageList\n",
       "Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32)\n",
       "y: CategoryList\n",
       "automobile,automobile,automobile,automobile,automobile\n",
       "Path: /home/cliff/.fastai/data/cifar10/train;\n",
       "\n",
       "Valid: LabelList (5000 items)\n",
       "x: ImageList\n",
       "Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32)\n",
       "y: CategoryList\n",
       "cat,cat,ship,ship,automobile\n",
       "Path: /home/cliff/.fastai/data/cifar10/train;\n",
       "\n",
       "Test: None, model=Network(\n",
       "  (stem_): Sequential(\n",
       "    (0): Conv2d(3, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (blocks_): Sequential(\n",
       "    (0): Cell(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (branch_0): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_1): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_1): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_2): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Cell(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (branch_0): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_1): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_1): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_2): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Cell(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (branch_0): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_1): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_1): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_2): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(40, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): Cell(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (branch_0): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_1): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_1): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_2): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Cell(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (branch_0): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_1): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_1): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_2): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Cell(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (branch_0): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_1): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_1): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_2): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Cell(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (branch_0): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_1): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_1): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_2): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (9): Cell(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (branch_0): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_1): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_1): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_2): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): Cell(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (branch_0): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_1): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_1): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_2): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): Cell(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (branch_0): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_1): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_1): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_2): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head_): Sequential(\n",
       "    (0): AdaptiveAvgPool2d(output_size=1)\n",
       "    (1): Flatten()\n",
       "    (2): Linear(in_features=160, out_features=10, bias=True)\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function error_rate at 0x7fb61b4c4268>, <function accuracy at 0x7fb61b4c4048>], true_wd=True, bn_wd=False, wd=0.01, train_bn=True, path=PosixPath('/home/cliff/.fastai/data/cifar10/train'), model_dir='/home/cliff/ResearchProjects/geppy_nn/comp_graphs/new', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[MixedPrecision\n",
       "learn: Learner(data=ImageDataBunch;\n",
       "\n",
       "Train: LabelList (45000 items)\n",
       "x: ImageList\n",
       "Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32)\n",
       "y: CategoryList\n",
       "automobile,automobile,automobile,automobile,automobile\n",
       "Path: /home/cliff/.fastai/data/cifar10/train;\n",
       "\n",
       "Valid: LabelList (5000 items)\n",
       "x: ImageList\n",
       "Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32)\n",
       "y: CategoryList\n",
       "cat,cat,ship,ship,automobile\n",
       "Path: /home/cliff/.fastai/data/cifar10/train;\n",
       "\n",
       "Test: None, model=Network(\n",
       "  (stem_): Sequential(\n",
       "    (0): Conv2d(3, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (blocks_): Sequential(\n",
       "    (0): Cell(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (branch_0): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_1): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_1): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_2): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Cell(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (branch_0): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_1): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_1): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_2): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Cell(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (branch_0): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_1): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_1): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_2): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(40, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): Cell(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (branch_0): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_1): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_1): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_2): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Cell(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (branch_0): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_1): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_1): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_2): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Cell(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (branch_0): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_1): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_1): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_2): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Cell(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (branch_0): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_1): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_1): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_2): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (9): Cell(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (branch_0): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_1): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_1): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_2): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): Cell(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (branch_0): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_1): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_1): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_2): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): Cell(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (branch_0): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_1): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_1): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (branch_2): Layer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x3_2): Sequential(\n",
       "          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head_): Sequential(\n",
       "    (0): AdaptiveAvgPool2d(output_size=1)\n",
       "    (1): Flatten()\n",
       "    (2): Linear(in_features=160, out_features=10, bias=True)\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function error_rate at 0x7fb61b4c4268>, <function accuracy at 0x7fb61b4c4048>], true_wd=True, bn_wd=False, wd=0.01, train_bn=True, path=PosixPath('/home/cliff/.fastai/data/cifar10/train'), model_dir='/home/cliff/ResearchProjects/geppy_nn/comp_graphs/new', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[...], layer_groups=[Sequential(\n",
       "  (0): Conv2d(3, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (4): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (5): ReLU(inplace=True)\n",
       "  (6): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (7): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (10): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (13): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): ReLU(inplace=True)\n",
       "  (15): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (16): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (17): ReLU(inplace=True)\n",
       "  (18): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (19): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (22): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (23): ReLU(inplace=True)\n",
       "  (24): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (25): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (26): ReLU(inplace=True)\n",
       "  (27): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (28): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (31): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (32): ReLU(inplace=True)\n",
       "  (33): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (34): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (35): ReLU(inplace=True)\n",
       "  (36): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (37): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (38): ReLU(inplace=True)\n",
       "  (39): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (40): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (41): ReLU(inplace=True)\n",
       "  (42): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (43): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (44): ReLU(inplace=True)\n",
       "  (45): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (46): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (47): ReLU(inplace=True)\n",
       "  (48): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (49): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (50): ReLU(inplace=True)\n",
       "  (51): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (52): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (53): ReLU(inplace=True)\n",
       "  (54): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (55): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (56): ReLU(inplace=True)\n",
       "  (57): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (58): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (59): ReLU(inplace=True)\n",
       "  (60): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (61): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (62): ReLU(inplace=True)\n",
       "  (63): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (64): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (65): ReLU(inplace=True)\n",
       "  (66): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (67): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (68): ReLU(inplace=True)\n",
       "  (69): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (70): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (71): ReLU(inplace=True)\n",
       "  (72): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (73): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (74): ReLU(inplace=True)\n",
       "  (75): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (76): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (77): ReLU(inplace=True)\n",
       "  (78): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (79): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (80): ReLU(inplace=True)\n",
       "  (81): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (82): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (83): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (84): Conv2d(40, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (85): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (86): ReLU(inplace=True)\n",
       "  (87): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (88): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (89): ReLU(inplace=True)\n",
       "  (90): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (91): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (92): ReLU(inplace=True)\n",
       "  (93): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (94): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (95): ReLU(inplace=True)\n",
       "  (96): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (97): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (98): ReLU(inplace=True)\n",
       "  (99): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (100): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (101): ReLU(inplace=True)\n",
       "  (102): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (103): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (104): ReLU(inplace=True)\n",
       "  (105): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (106): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (107): ReLU(inplace=True)\n",
       "  (108): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (109): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (110): ReLU(inplace=True)\n",
       "  (111): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (112): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (113): ReLU(inplace=True)\n",
       "  (114): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (115): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (116): ReLU(inplace=True)\n",
       "  (117): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (118): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (119): ReLU(inplace=True)\n",
       "  (120): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (121): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (122): ReLU(inplace=True)\n",
       "  (123): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (124): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (125): ReLU(inplace=True)\n",
       "  (126): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (127): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (128): ReLU(inplace=True)\n",
       "  (129): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (130): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (131): ReLU(inplace=True)\n",
       "  (132): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (133): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (134): ReLU(inplace=True)\n",
       "  (135): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (136): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (137): ReLU(inplace=True)\n",
       "  (138): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (139): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (140): ReLU(inplace=True)\n",
       "  (141): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (142): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (143): ReLU(inplace=True)\n",
       "  (144): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (145): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (146): ReLU(inplace=True)\n",
       "  (147): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (148): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (149): ReLU(inplace=True)\n",
       "  (150): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (151): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (152): ReLU(inplace=True)\n",
       "  (153): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (154): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (155): ReLU(inplace=True)\n",
       "  (156): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (157): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (158): ReLU(inplace=True)\n",
       "  (159): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (160): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (161): ReLU(inplace=True)\n",
       "  (162): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (163): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (164): ReLU(inplace=True)\n",
       "  (165): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (166): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (167): ReLU(inplace=True)\n",
       "  (168): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (169): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (170): ReLU(inplace=True)\n",
       "  (171): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (172): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (173): ReLU(inplace=True)\n",
       "  (174): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (175): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (176): ReLU(inplace=True)\n",
       "  (177): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (178): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (179): ReLU(inplace=True)\n",
       "  (180): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (181): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (182): ReLU(inplace=True)\n",
       "  (183): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (184): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (185): ReLU(inplace=True)\n",
       "  (186): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (187): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (188): ReLU(inplace=True)\n",
       "  (189): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (190): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (191): ReLU(inplace=True)\n",
       "  (192): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (193): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (194): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (195): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (196): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (197): ReLU(inplace=True)\n",
       "  (198): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (199): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (200): ReLU(inplace=True)\n",
       "  (201): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (202): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (203): ReLU(inplace=True)\n",
       "  (204): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (205): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (206): ReLU(inplace=True)\n",
       "  (207): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (208): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (209): ReLU(inplace=True)\n",
       "  (210): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (211): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (212): ReLU(inplace=True)\n",
       "  (213): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (214): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (215): ReLU(inplace=True)\n",
       "  (216): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (217): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (218): ReLU(inplace=True)\n",
       "  (219): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (220): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (221): ReLU(inplace=True)\n",
       "  (222): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (223): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (224): ReLU(inplace=True)\n",
       "  (225): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (226): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (227): ReLU(inplace=True)\n",
       "  (228): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (229): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (230): ReLU(inplace=True)\n",
       "  (231): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (232): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (233): ReLU(inplace=True)\n",
       "  (234): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (235): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (236): ReLU(inplace=True)\n",
       "  (237): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (238): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (239): ReLU(inplace=True)\n",
       "  (240): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (241): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (242): ReLU(inplace=True)\n",
       "  (243): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (244): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (245): ReLU(inplace=True)\n",
       "  (246): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (247): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (248): ReLU(inplace=True)\n",
       "  (249): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (250): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (251): ReLU(inplace=True)\n",
       "  (252): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (253): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (254): ReLU(inplace=True)\n",
       "  (255): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (256): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (257): ReLU(inplace=True)\n",
       "  (258): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (259): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (260): ReLU(inplace=True)\n",
       "  (261): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (262): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (263): ReLU(inplace=True)\n",
       "  (264): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (265): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (266): ReLU(inplace=True)\n",
       "  (267): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (268): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (269): ReLU(inplace=True)\n",
       "  (270): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (271): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (272): ReLU(inplace=True)\n",
       "  (273): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (274): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (275): ReLU(inplace=True)\n",
       "  (276): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (277): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (278): AdaptiveAvgPool2d(output_size=1)\n",
       "  (279): Flatten()\n",
       "  (280): Linear(in_features=160, out_features=10, bias=True)\n",
       ")], add_time=True, silent=False)\n",
       "loss_scale: 65536\n",
       "max_noskip: 1000\n",
       "dynamic: True\n",
       "clip: None\n",
       "flat_master: False\n",
       "max_scale: 16777216\n",
       "loss_fp32: True], layer_groups=[Sequential(\n",
       "  (0): Conv2d(3, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (4): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (5): ReLU(inplace=True)\n",
       "  (6): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (7): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (10): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (13): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): ReLU(inplace=True)\n",
       "  (15): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (16): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (17): ReLU(inplace=True)\n",
       "  (18): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (19): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (22): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (23): ReLU(inplace=True)\n",
       "  (24): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (25): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (26): ReLU(inplace=True)\n",
       "  (27): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (28): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (31): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (32): ReLU(inplace=True)\n",
       "  (33): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (34): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (35): ReLU(inplace=True)\n",
       "  (36): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (37): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (38): ReLU(inplace=True)\n",
       "  (39): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (40): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (41): ReLU(inplace=True)\n",
       "  (42): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (43): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (44): ReLU(inplace=True)\n",
       "  (45): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (46): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (47): ReLU(inplace=True)\n",
       "  (48): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (49): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (50): ReLU(inplace=True)\n",
       "  (51): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (52): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (53): ReLU(inplace=True)\n",
       "  (54): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (55): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (56): ReLU(inplace=True)\n",
       "  (57): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (58): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (59): ReLU(inplace=True)\n",
       "  (60): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (61): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (62): ReLU(inplace=True)\n",
       "  (63): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (64): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (65): ReLU(inplace=True)\n",
       "  (66): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (67): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (68): ReLU(inplace=True)\n",
       "  (69): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (70): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (71): ReLU(inplace=True)\n",
       "  (72): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (73): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (74): ReLU(inplace=True)\n",
       "  (75): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (76): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (77): ReLU(inplace=True)\n",
       "  (78): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (79): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (80): ReLU(inplace=True)\n",
       "  (81): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (82): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (83): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (84): Conv2d(40, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (85): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (86): ReLU(inplace=True)\n",
       "  (87): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (88): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (89): ReLU(inplace=True)\n",
       "  (90): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (91): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (92): ReLU(inplace=True)\n",
       "  (93): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (94): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (95): ReLU(inplace=True)\n",
       "  (96): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (97): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (98): ReLU(inplace=True)\n",
       "  (99): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (100): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (101): ReLU(inplace=True)\n",
       "  (102): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (103): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (104): ReLU(inplace=True)\n",
       "  (105): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (106): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (107): ReLU(inplace=True)\n",
       "  (108): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (109): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (110): ReLU(inplace=True)\n",
       "  (111): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (112): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (113): ReLU(inplace=True)\n",
       "  (114): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (115): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (116): ReLU(inplace=True)\n",
       "  (117): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (118): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (119): ReLU(inplace=True)\n",
       "  (120): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (121): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (122): ReLU(inplace=True)\n",
       "  (123): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (124): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (125): ReLU(inplace=True)\n",
       "  (126): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (127): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (128): ReLU(inplace=True)\n",
       "  (129): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (130): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (131): ReLU(inplace=True)\n",
       "  (132): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (133): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (134): ReLU(inplace=True)\n",
       "  (135): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (136): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (137): ReLU(inplace=True)\n",
       "  (138): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (139): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (140): ReLU(inplace=True)\n",
       "  (141): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (142): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (143): ReLU(inplace=True)\n",
       "  (144): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (145): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (146): ReLU(inplace=True)\n",
       "  (147): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (148): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (149): ReLU(inplace=True)\n",
       "  (150): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (151): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (152): ReLU(inplace=True)\n",
       "  (153): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (154): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (155): ReLU(inplace=True)\n",
       "  (156): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (157): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (158): ReLU(inplace=True)\n",
       "  (159): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (160): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (161): ReLU(inplace=True)\n",
       "  (162): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (163): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (164): ReLU(inplace=True)\n",
       "  (165): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (166): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (167): ReLU(inplace=True)\n",
       "  (168): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (169): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (170): ReLU(inplace=True)\n",
       "  (171): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (172): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (173): ReLU(inplace=True)\n",
       "  (174): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (175): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (176): ReLU(inplace=True)\n",
       "  (177): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (178): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (179): ReLU(inplace=True)\n",
       "  (180): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (181): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (182): ReLU(inplace=True)\n",
       "  (183): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (184): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (185): ReLU(inplace=True)\n",
       "  (186): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (187): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (188): ReLU(inplace=True)\n",
       "  (189): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (190): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (191): ReLU(inplace=True)\n",
       "  (192): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (193): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (194): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (195): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (196): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (197): ReLU(inplace=True)\n",
       "  (198): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (199): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (200): ReLU(inplace=True)\n",
       "  (201): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (202): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (203): ReLU(inplace=True)\n",
       "  (204): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (205): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (206): ReLU(inplace=True)\n",
       "  (207): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (208): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (209): ReLU(inplace=True)\n",
       "  (210): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (211): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (212): ReLU(inplace=True)\n",
       "  (213): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (214): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (215): ReLU(inplace=True)\n",
       "  (216): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (217): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (218): ReLU(inplace=True)\n",
       "  (219): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (220): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (221): ReLU(inplace=True)\n",
       "  (222): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (223): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (224): ReLU(inplace=True)\n",
       "  (225): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (226): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (227): ReLU(inplace=True)\n",
       "  (228): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (229): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (230): ReLU(inplace=True)\n",
       "  (231): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (232): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (233): ReLU(inplace=True)\n",
       "  (234): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (235): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (236): ReLU(inplace=True)\n",
       "  (237): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (238): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (239): ReLU(inplace=True)\n",
       "  (240): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (241): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (242): ReLU(inplace=True)\n",
       "  (243): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (244): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (245): ReLU(inplace=True)\n",
       "  (246): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (247): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (248): ReLU(inplace=True)\n",
       "  (249): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (250): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (251): ReLU(inplace=True)\n",
       "  (252): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (253): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (254): ReLU(inplace=True)\n",
       "  (255): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (256): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (257): ReLU(inplace=True)\n",
       "  (258): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (259): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (260): ReLU(inplace=True)\n",
       "  (261): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (262): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (263): ReLU(inplace=True)\n",
       "  (264): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (265): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (266): ReLU(inplace=True)\n",
       "  (267): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (268): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (269): ReLU(inplace=True)\n",
       "  (270): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (271): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (272): ReLU(inplace=True)\n",
       "  (273): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (274): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (275): ReLU(inplace=True)\n",
       "  (276): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (277): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (278): AdaptiveAvgPool2d(output_size=1)\n",
       "  (279): Flatten()\n",
       "  (280): Linear(in_features=160, out_features=10, bias=True)\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#opt_func=optim.Adam,\n",
    "model_dir = '/home/cliff/ResearchProjects/geppy_nn/comp_graphs/new'\n",
    "learn = Learner(data, net, bn_wd=False, metrics=[error_rate, accuracy], model_dir=model_dir)#.mixup() \n",
    "learn.to_fp16()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "lck7gklvOxIm"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='90' class='' max='468', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      19.23% [90/468 00:17<01:14 4.5728]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find(wd=4e-4,end_lr=100)\n",
    "learn.recorder.plot()\n",
    "#SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qdLZHQXUOxIr",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='193' class='' max='1500', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      12.87% [193/1500 3:47:17<25:39:15]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.554363</td>\n",
       "      <td>1.557953</td>\n",
       "      <td>0.556200</td>\n",
       "      <td>0.443800</td>\n",
       "      <td>01:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.368053</td>\n",
       "      <td>1.311176</td>\n",
       "      <td>0.471600</td>\n",
       "      <td>0.528400</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.244355</td>\n",
       "      <td>1.201602</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.141673</td>\n",
       "      <td>1.066611</td>\n",
       "      <td>0.372600</td>\n",
       "      <td>0.627400</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.063274</td>\n",
       "      <td>1.015867</td>\n",
       "      <td>0.348000</td>\n",
       "      <td>0.652000</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.981949</td>\n",
       "      <td>0.891954</td>\n",
       "      <td>0.307600</td>\n",
       "      <td>0.692400</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.968433</td>\n",
       "      <td>0.910652</td>\n",
       "      <td>0.310800</td>\n",
       "      <td>0.689200</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.896409</td>\n",
       "      <td>0.863249</td>\n",
       "      <td>0.299200</td>\n",
       "      <td>0.700800</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.840841</td>\n",
       "      <td>0.774608</td>\n",
       "      <td>0.271200</td>\n",
       "      <td>0.728800</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.796669</td>\n",
       "      <td>0.744206</td>\n",
       "      <td>0.258200</td>\n",
       "      <td>0.741800</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.759942</td>\n",
       "      <td>0.736515</td>\n",
       "      <td>0.251400</td>\n",
       "      <td>0.748600</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.726406</td>\n",
       "      <td>0.725256</td>\n",
       "      <td>0.242400</td>\n",
       "      <td>0.757600</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.688216</td>\n",
       "      <td>0.645536</td>\n",
       "      <td>0.224800</td>\n",
       "      <td>0.775200</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.649982</td>\n",
       "      <td>0.633853</td>\n",
       "      <td>0.217200</td>\n",
       "      <td>0.782800</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.619391</td>\n",
       "      <td>0.561710</td>\n",
       "      <td>0.193800</td>\n",
       "      <td>0.806200</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.608070</td>\n",
       "      <td>0.579261</td>\n",
       "      <td>0.201600</td>\n",
       "      <td>0.798400</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.585772</td>\n",
       "      <td>0.534095</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.812000</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.560641</td>\n",
       "      <td>0.542581</td>\n",
       "      <td>0.189600</td>\n",
       "      <td>0.810400</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.547738</td>\n",
       "      <td>0.527990</td>\n",
       "      <td>0.181000</td>\n",
       "      <td>0.819000</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.528277</td>\n",
       "      <td>0.507225</td>\n",
       "      <td>0.172600</td>\n",
       "      <td>0.827400</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.522642</td>\n",
       "      <td>0.539528</td>\n",
       "      <td>0.183400</td>\n",
       "      <td>0.816600</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.491865</td>\n",
       "      <td>0.509947</td>\n",
       "      <td>0.170600</td>\n",
       "      <td>0.829400</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.490871</td>\n",
       "      <td>0.475264</td>\n",
       "      <td>0.161000</td>\n",
       "      <td>0.839000</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.470540</td>\n",
       "      <td>0.493650</td>\n",
       "      <td>0.168200</td>\n",
       "      <td>0.831800</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.469340</td>\n",
       "      <td>0.505448</td>\n",
       "      <td>0.171400</td>\n",
       "      <td>0.828600</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.444154</td>\n",
       "      <td>0.426356</td>\n",
       "      <td>0.143400</td>\n",
       "      <td>0.856600</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.428956</td>\n",
       "      <td>0.451779</td>\n",
       "      <td>0.148200</td>\n",
       "      <td>0.851800</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.432372</td>\n",
       "      <td>0.445050</td>\n",
       "      <td>0.149800</td>\n",
       "      <td>0.850200</td>\n",
       "      <td>01:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.414271</td>\n",
       "      <td>0.431968</td>\n",
       "      <td>0.151000</td>\n",
       "      <td>0.849000</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.415986</td>\n",
       "      <td>0.388557</td>\n",
       "      <td>0.129600</td>\n",
       "      <td>0.870400</td>\n",
       "      <td>01:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.379670</td>\n",
       "      <td>0.433879</td>\n",
       "      <td>0.149800</td>\n",
       "      <td>0.850200</td>\n",
       "      <td>01:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.376720</td>\n",
       "      <td>0.429229</td>\n",
       "      <td>0.141200</td>\n",
       "      <td>0.858800</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.380326</td>\n",
       "      <td>0.416196</td>\n",
       "      <td>0.136400</td>\n",
       "      <td>0.863600</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.360768</td>\n",
       "      <td>0.420699</td>\n",
       "      <td>0.137800</td>\n",
       "      <td>0.862200</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.363541</td>\n",
       "      <td>0.398159</td>\n",
       "      <td>0.136200</td>\n",
       "      <td>0.863800</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.349228</td>\n",
       "      <td>0.398278</td>\n",
       "      <td>0.131000</td>\n",
       "      <td>0.869000</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.347819</td>\n",
       "      <td>0.443298</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.852800</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.345948</td>\n",
       "      <td>0.382686</td>\n",
       "      <td>0.132600</td>\n",
       "      <td>0.867400</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.328316</td>\n",
       "      <td>0.423840</td>\n",
       "      <td>0.135800</td>\n",
       "      <td>0.864200</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.331721</td>\n",
       "      <td>0.392665</td>\n",
       "      <td>0.130800</td>\n",
       "      <td>0.869200</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.324408</td>\n",
       "      <td>0.448376</td>\n",
       "      <td>0.147000</td>\n",
       "      <td>0.853000</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.317689</td>\n",
       "      <td>0.364254</td>\n",
       "      <td>0.123000</td>\n",
       "      <td>0.877000</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.309256</td>\n",
       "      <td>0.383117</td>\n",
       "      <td>0.127000</td>\n",
       "      <td>0.873000</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.303436</td>\n",
       "      <td>0.365322</td>\n",
       "      <td>0.121000</td>\n",
       "      <td>0.879000</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.304968</td>\n",
       "      <td>0.389185</td>\n",
       "      <td>0.129800</td>\n",
       "      <td>0.870200</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.291019</td>\n",
       "      <td>0.381300</td>\n",
       "      <td>0.122400</td>\n",
       "      <td>0.877600</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.287948</td>\n",
       "      <td>0.334709</td>\n",
       "      <td>0.109800</td>\n",
       "      <td>0.890200</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.266542</td>\n",
       "      <td>0.365964</td>\n",
       "      <td>0.117400</td>\n",
       "      <td>0.882600</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.278313</td>\n",
       "      <td>0.380661</td>\n",
       "      <td>0.120800</td>\n",
       "      <td>0.879200</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.273454</td>\n",
       "      <td>0.411423</td>\n",
       "      <td>0.134600</td>\n",
       "      <td>0.865400</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.260469</td>\n",
       "      <td>0.380332</td>\n",
       "      <td>0.117000</td>\n",
       "      <td>0.883000</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.256955</td>\n",
       "      <td>0.345781</td>\n",
       "      <td>0.116200</td>\n",
       "      <td>0.883800</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.268985</td>\n",
       "      <td>0.348924</td>\n",
       "      <td>0.112800</td>\n",
       "      <td>0.887200</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.268690</td>\n",
       "      <td>0.352144</td>\n",
       "      <td>0.116200</td>\n",
       "      <td>0.883800</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.253072</td>\n",
       "      <td>0.399733</td>\n",
       "      <td>0.122000</td>\n",
       "      <td>0.878000</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.251805</td>\n",
       "      <td>0.395296</td>\n",
       "      <td>0.124400</td>\n",
       "      <td>0.875600</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.251487</td>\n",
       "      <td>0.397263</td>\n",
       "      <td>0.124200</td>\n",
       "      <td>0.875800</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.245580</td>\n",
       "      <td>0.425587</td>\n",
       "      <td>0.125600</td>\n",
       "      <td>0.874400</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.243651</td>\n",
       "      <td>0.357509</td>\n",
       "      <td>0.111600</td>\n",
       "      <td>0.888400</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.240153</td>\n",
       "      <td>0.363575</td>\n",
       "      <td>0.112800</td>\n",
       "      <td>0.887200</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.237239</td>\n",
       "      <td>0.335159</td>\n",
       "      <td>0.107400</td>\n",
       "      <td>0.892600</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.243560</td>\n",
       "      <td>0.331940</td>\n",
       "      <td>0.109200</td>\n",
       "      <td>0.890800</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.231999</td>\n",
       "      <td>0.316429</td>\n",
       "      <td>0.100800</td>\n",
       "      <td>0.899200</td>\n",
       "      <td>01:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.224045</td>\n",
       "      <td>0.339102</td>\n",
       "      <td>0.107400</td>\n",
       "      <td>0.892600</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.215889</td>\n",
       "      <td>0.326728</td>\n",
       "      <td>0.104000</td>\n",
       "      <td>0.896000</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.219192</td>\n",
       "      <td>0.308075</td>\n",
       "      <td>0.103000</td>\n",
       "      <td>0.897000</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.228836</td>\n",
       "      <td>0.343797</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>0.891000</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.213406</td>\n",
       "      <td>0.333185</td>\n",
       "      <td>0.102200</td>\n",
       "      <td>0.897800</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.219005</td>\n",
       "      <td>0.357604</td>\n",
       "      <td>0.103800</td>\n",
       "      <td>0.896200</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.207929</td>\n",
       "      <td>0.336726</td>\n",
       "      <td>0.100800</td>\n",
       "      <td>0.899200</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.214518</td>\n",
       "      <td>0.309845</td>\n",
       "      <td>0.093000</td>\n",
       "      <td>0.907000</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.214921</td>\n",
       "      <td>0.340119</td>\n",
       "      <td>0.102000</td>\n",
       "      <td>0.898000</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.213649</td>\n",
       "      <td>0.313401</td>\n",
       "      <td>0.101400</td>\n",
       "      <td>0.898600</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.211179</td>\n",
       "      <td>0.345874</td>\n",
       "      <td>0.110600</td>\n",
       "      <td>0.889400</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.202356</td>\n",
       "      <td>0.335848</td>\n",
       "      <td>0.103400</td>\n",
       "      <td>0.896600</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.199124</td>\n",
       "      <td>0.348949</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.899600</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.201516</td>\n",
       "      <td>0.340876</td>\n",
       "      <td>0.111000</td>\n",
       "      <td>0.889000</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.201307</td>\n",
       "      <td>0.329514</td>\n",
       "      <td>0.105800</td>\n",
       "      <td>0.894200</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.192905</td>\n",
       "      <td>0.361195</td>\n",
       "      <td>0.105400</td>\n",
       "      <td>0.894600</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.203816</td>\n",
       "      <td>0.324807</td>\n",
       "      <td>0.099400</td>\n",
       "      <td>0.900600</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.189513</td>\n",
       "      <td>0.339853</td>\n",
       "      <td>0.103600</td>\n",
       "      <td>0.896400</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.202053</td>\n",
       "      <td>0.341363</td>\n",
       "      <td>0.101000</td>\n",
       "      <td>0.899000</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.200977</td>\n",
       "      <td>0.373195</td>\n",
       "      <td>0.115600</td>\n",
       "      <td>0.884400</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.183578</td>\n",
       "      <td>0.322967</td>\n",
       "      <td>0.098400</td>\n",
       "      <td>0.901600</td>\n",
       "      <td>01:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.193389</td>\n",
       "      <td>0.379939</td>\n",
       "      <td>0.111800</td>\n",
       "      <td>0.888200</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.199684</td>\n",
       "      <td>0.340157</td>\n",
       "      <td>0.101400</td>\n",
       "      <td>0.898600</td>\n",
       "      <td>01:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.188148</td>\n",
       "      <td>0.330797</td>\n",
       "      <td>0.097800</td>\n",
       "      <td>0.902200</td>\n",
       "      <td>01:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.192425</td>\n",
       "      <td>0.342948</td>\n",
       "      <td>0.097000</td>\n",
       "      <td>0.903000</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.190099</td>\n",
       "      <td>0.320585</td>\n",
       "      <td>0.096800</td>\n",
       "      <td>0.903200</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.197148</td>\n",
       "      <td>0.379513</td>\n",
       "      <td>0.104600</td>\n",
       "      <td>0.895400</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.176711</td>\n",
       "      <td>0.385309</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.894000</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.188102</td>\n",
       "      <td>0.327082</td>\n",
       "      <td>0.099000</td>\n",
       "      <td>0.901000</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.184217</td>\n",
       "      <td>0.333353</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.899600</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.176925</td>\n",
       "      <td>0.333491</td>\n",
       "      <td>0.100200</td>\n",
       "      <td>0.899800</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.182758</td>\n",
       "      <td>0.352890</td>\n",
       "      <td>0.102600</td>\n",
       "      <td>0.897400</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.173768</td>\n",
       "      <td>0.325760</td>\n",
       "      <td>0.091800</td>\n",
       "      <td>0.908200</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.180460</td>\n",
       "      <td>0.338623</td>\n",
       "      <td>0.099400</td>\n",
       "      <td>0.900600</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.168843</td>\n",
       "      <td>0.319654</td>\n",
       "      <td>0.088400</td>\n",
       "      <td>0.911600</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.172479</td>\n",
       "      <td>0.310805</td>\n",
       "      <td>0.088800</td>\n",
       "      <td>0.911200</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.180907</td>\n",
       "      <td>0.314063</td>\n",
       "      <td>0.088800</td>\n",
       "      <td>0.911200</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.164307</td>\n",
       "      <td>0.315924</td>\n",
       "      <td>0.092400</td>\n",
       "      <td>0.907600</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.166815</td>\n",
       "      <td>0.380643</td>\n",
       "      <td>0.113400</td>\n",
       "      <td>0.886600</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.175171</td>\n",
       "      <td>0.332250</td>\n",
       "      <td>0.096600</td>\n",
       "      <td>0.903400</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.172147</td>\n",
       "      <td>0.311049</td>\n",
       "      <td>0.095400</td>\n",
       "      <td>0.904600</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.178678</td>\n",
       "      <td>0.331713</td>\n",
       "      <td>0.096800</td>\n",
       "      <td>0.903200</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.171604</td>\n",
       "      <td>0.291113</td>\n",
       "      <td>0.088600</td>\n",
       "      <td>0.911400</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.179552</td>\n",
       "      <td>0.360093</td>\n",
       "      <td>0.108000</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.163398</td>\n",
       "      <td>0.301287</td>\n",
       "      <td>0.086800</td>\n",
       "      <td>0.913200</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.164042</td>\n",
       "      <td>0.380241</td>\n",
       "      <td>0.102400</td>\n",
       "      <td>0.897600</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.176312</td>\n",
       "      <td>0.307727</td>\n",
       "      <td>0.086800</td>\n",
       "      <td>0.913200</td>\n",
       "      <td>01:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.165108</td>\n",
       "      <td>0.325667</td>\n",
       "      <td>0.091800</td>\n",
       "      <td>0.908200</td>\n",
       "      <td>01:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.159186</td>\n",
       "      <td>0.325871</td>\n",
       "      <td>0.096400</td>\n",
       "      <td>0.903600</td>\n",
       "      <td>01:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.161422</td>\n",
       "      <td>0.312548</td>\n",
       "      <td>0.093400</td>\n",
       "      <td>0.906600</td>\n",
       "      <td>01:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.159745</td>\n",
       "      <td>0.322365</td>\n",
       "      <td>0.090600</td>\n",
       "      <td>0.909400</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.169271</td>\n",
       "      <td>0.353364</td>\n",
       "      <td>0.100200</td>\n",
       "      <td>0.899800</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.162049</td>\n",
       "      <td>0.317357</td>\n",
       "      <td>0.094800</td>\n",
       "      <td>0.905200</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.160044</td>\n",
       "      <td>0.343446</td>\n",
       "      <td>0.100600</td>\n",
       "      <td>0.899400</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.156824</td>\n",
       "      <td>0.307336</td>\n",
       "      <td>0.081800</td>\n",
       "      <td>0.918200</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.151166</td>\n",
       "      <td>0.365168</td>\n",
       "      <td>0.102200</td>\n",
       "      <td>0.897800</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.154416</td>\n",
       "      <td>0.346192</td>\n",
       "      <td>0.093800</td>\n",
       "      <td>0.906200</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.150009</td>\n",
       "      <td>0.328099</td>\n",
       "      <td>0.096400</td>\n",
       "      <td>0.903600</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.152433</td>\n",
       "      <td>0.303962</td>\n",
       "      <td>0.091000</td>\n",
       "      <td>0.909000</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.155281</td>\n",
       "      <td>0.337166</td>\n",
       "      <td>0.096400</td>\n",
       "      <td>0.903600</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.157314</td>\n",
       "      <td>0.312841</td>\n",
       "      <td>0.090800</td>\n",
       "      <td>0.909200</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.157507</td>\n",
       "      <td>0.324527</td>\n",
       "      <td>0.093600</td>\n",
       "      <td>0.906400</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.158232</td>\n",
       "      <td>0.322137</td>\n",
       "      <td>0.092400</td>\n",
       "      <td>0.907600</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.153476</td>\n",
       "      <td>0.351575</td>\n",
       "      <td>0.099000</td>\n",
       "      <td>0.901000</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.148280</td>\n",
       "      <td>0.328924</td>\n",
       "      <td>0.095200</td>\n",
       "      <td>0.904800</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.152905</td>\n",
       "      <td>0.342465</td>\n",
       "      <td>0.097400</td>\n",
       "      <td>0.902600</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.145472</td>\n",
       "      <td>0.320012</td>\n",
       "      <td>0.092400</td>\n",
       "      <td>0.907600</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.141800</td>\n",
       "      <td>0.317920</td>\n",
       "      <td>0.089600</td>\n",
       "      <td>0.910400</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.143924</td>\n",
       "      <td>0.318808</td>\n",
       "      <td>0.090600</td>\n",
       "      <td>0.909400</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.144265</td>\n",
       "      <td>0.335154</td>\n",
       "      <td>0.097600</td>\n",
       "      <td>0.902400</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.143427</td>\n",
       "      <td>0.293141</td>\n",
       "      <td>0.082600</td>\n",
       "      <td>0.917400</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.153263</td>\n",
       "      <td>0.304190</td>\n",
       "      <td>0.086200</td>\n",
       "      <td>0.913800</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.148581</td>\n",
       "      <td>0.334571</td>\n",
       "      <td>0.092400</td>\n",
       "      <td>0.907600</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.155265</td>\n",
       "      <td>0.339549</td>\n",
       "      <td>0.094800</td>\n",
       "      <td>0.905200</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.158388</td>\n",
       "      <td>0.322502</td>\n",
       "      <td>0.087800</td>\n",
       "      <td>0.912200</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.139617</td>\n",
       "      <td>0.301966</td>\n",
       "      <td>0.088000</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.144941</td>\n",
       "      <td>0.318567</td>\n",
       "      <td>0.088400</td>\n",
       "      <td>0.911600</td>\n",
       "      <td>01:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.139724</td>\n",
       "      <td>0.285986</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>02:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.141384</td>\n",
       "      <td>0.280760</td>\n",
       "      <td>0.078600</td>\n",
       "      <td>0.921400</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.144334</td>\n",
       "      <td>0.311666</td>\n",
       "      <td>0.086000</td>\n",
       "      <td>0.914000</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.137805</td>\n",
       "      <td>0.292122</td>\n",
       "      <td>0.084600</td>\n",
       "      <td>0.915400</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.133122</td>\n",
       "      <td>0.331776</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.131488</td>\n",
       "      <td>0.319847</td>\n",
       "      <td>0.087600</td>\n",
       "      <td>0.912400</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.137190</td>\n",
       "      <td>0.304032</td>\n",
       "      <td>0.084800</td>\n",
       "      <td>0.915200</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.131062</td>\n",
       "      <td>0.320896</td>\n",
       "      <td>0.088600</td>\n",
       "      <td>0.911400</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.142716</td>\n",
       "      <td>0.322816</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>01:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.134610</td>\n",
       "      <td>0.277259</td>\n",
       "      <td>0.081000</td>\n",
       "      <td>0.919000</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.142122</td>\n",
       "      <td>0.399279</td>\n",
       "      <td>0.111200</td>\n",
       "      <td>0.888800</td>\n",
       "      <td>01:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.140533</td>\n",
       "      <td>0.329839</td>\n",
       "      <td>0.093600</td>\n",
       "      <td>0.906400</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.130334</td>\n",
       "      <td>0.331174</td>\n",
       "      <td>0.092000</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.132534</td>\n",
       "      <td>0.311738</td>\n",
       "      <td>0.088600</td>\n",
       "      <td>0.911400</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.127391</td>\n",
       "      <td>0.267939</td>\n",
       "      <td>0.080600</td>\n",
       "      <td>0.919400</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.126209</td>\n",
       "      <td>0.324243</td>\n",
       "      <td>0.088400</td>\n",
       "      <td>0.911600</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.132285</td>\n",
       "      <td>0.274869</td>\n",
       "      <td>0.077000</td>\n",
       "      <td>0.923000</td>\n",
       "      <td>01:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.133337</td>\n",
       "      <td>0.330139</td>\n",
       "      <td>0.091800</td>\n",
       "      <td>0.908200</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.129759</td>\n",
       "      <td>0.355141</td>\n",
       "      <td>0.101400</td>\n",
       "      <td>0.898600</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.137395</td>\n",
       "      <td>0.320728</td>\n",
       "      <td>0.087000</td>\n",
       "      <td>0.913000</td>\n",
       "      <td>01:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.130738</td>\n",
       "      <td>0.317228</td>\n",
       "      <td>0.085800</td>\n",
       "      <td>0.914200</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.129204</td>\n",
       "      <td>0.363501</td>\n",
       "      <td>0.098800</td>\n",
       "      <td>0.901200</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.123516</td>\n",
       "      <td>0.265583</td>\n",
       "      <td>0.076400</td>\n",
       "      <td>0.923600</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.122630</td>\n",
       "      <td>0.277065</td>\n",
       "      <td>0.082200</td>\n",
       "      <td>0.917800</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.124522</td>\n",
       "      <td>0.249087</td>\n",
       "      <td>0.079800</td>\n",
       "      <td>0.920200</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.113383</td>\n",
       "      <td>0.263091</td>\n",
       "      <td>0.079400</td>\n",
       "      <td>0.920600</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.123115</td>\n",
       "      <td>0.300449</td>\n",
       "      <td>0.084600</td>\n",
       "      <td>0.915400</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.126869</td>\n",
       "      <td>0.280496</td>\n",
       "      <td>0.077800</td>\n",
       "      <td>0.922200</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.117641</td>\n",
       "      <td>0.279019</td>\n",
       "      <td>0.079400</td>\n",
       "      <td>0.920600</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.124244</td>\n",
       "      <td>0.328279</td>\n",
       "      <td>0.091600</td>\n",
       "      <td>0.908400</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.130162</td>\n",
       "      <td>0.327804</td>\n",
       "      <td>0.087800</td>\n",
       "      <td>0.912200</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.130430</td>\n",
       "      <td>0.265787</td>\n",
       "      <td>0.076200</td>\n",
       "      <td>0.923800</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.118089</td>\n",
       "      <td>0.291220</td>\n",
       "      <td>0.077600</td>\n",
       "      <td>0.922400</td>\n",
       "      <td>01:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.127687</td>\n",
       "      <td>0.316685</td>\n",
       "      <td>0.086200</td>\n",
       "      <td>0.913800</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.117181</td>\n",
       "      <td>0.302339</td>\n",
       "      <td>0.086800</td>\n",
       "      <td>0.913200</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.113161</td>\n",
       "      <td>0.274210</td>\n",
       "      <td>0.078800</td>\n",
       "      <td>0.921200</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.113610</td>\n",
       "      <td>0.310903</td>\n",
       "      <td>0.087800</td>\n",
       "      <td>0.912200</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.123490</td>\n",
       "      <td>0.286290</td>\n",
       "      <td>0.080600</td>\n",
       "      <td>0.919400</td>\n",
       "      <td>02:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.113322</td>\n",
       "      <td>0.285565</td>\n",
       "      <td>0.081200</td>\n",
       "      <td>0.918800</td>\n",
       "      <td>01:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.113067</td>\n",
       "      <td>0.301026</td>\n",
       "      <td>0.080400</td>\n",
       "      <td>0.919600</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.111966</td>\n",
       "      <td>0.285027</td>\n",
       "      <td>0.075800</td>\n",
       "      <td>0.924200</td>\n",
       "      <td>01:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.121147</td>\n",
       "      <td>0.279110</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>02:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.119100</td>\n",
       "      <td>0.260541</td>\n",
       "      <td>0.076800</td>\n",
       "      <td>0.923200</td>\n",
       "      <td>02:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.119153</td>\n",
       "      <td>0.283291</td>\n",
       "      <td>0.079200</td>\n",
       "      <td>0.920800</td>\n",
       "      <td>01:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.106615</td>\n",
       "      <td>0.299996</td>\n",
       "      <td>0.081800</td>\n",
       "      <td>0.918200</td>\n",
       "      <td>01:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.114260</td>\n",
       "      <td>0.288283</td>\n",
       "      <td>0.077400</td>\n",
       "      <td>0.922600</td>\n",
       "      <td>01:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.118587</td>\n",
       "      <td>0.278394</td>\n",
       "      <td>0.075200</td>\n",
       "      <td>0.924800</td>\n",
       "      <td>01:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.121979</td>\n",
       "      <td>0.291298</td>\n",
       "      <td>0.079400</td>\n",
       "      <td>0.920600</td>\n",
       "      <td>01:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.106139</td>\n",
       "      <td>0.261788</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.926000</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.121165</td>\n",
       "      <td>0.306210</td>\n",
       "      <td>0.080600</td>\n",
       "      <td>0.919400</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.112532</td>\n",
       "      <td>0.277754</td>\n",
       "      <td>0.077600</td>\n",
       "      <td>0.922400</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.101227</td>\n",
       "      <td>0.298647</td>\n",
       "      <td>0.080800</td>\n",
       "      <td>0.919200</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.115600</td>\n",
       "      <td>0.269154</td>\n",
       "      <td>0.075600</td>\n",
       "      <td>0.924400</td>\n",
       "      <td>01:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='113' class='' max='351', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      32.19% [113/351 00:22<00:46 0.0959]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with accuracy value: 0.4438000023365021.\n",
      "Better model found at epoch 1 with accuracy value: 0.5284000039100647.\n",
      "Better model found at epoch 2 with accuracy value: 0.5799999833106995.\n",
      "Better model found at epoch 3 with accuracy value: 0.6273999810218811.\n",
      "Better model found at epoch 4 with accuracy value: 0.6520000100135803.\n",
      "Better model found at epoch 5 with accuracy value: 0.6923999786376953.\n",
      "Better model found at epoch 7 with accuracy value: 0.7008000016212463.\n",
      "Better model found at epoch 8 with accuracy value: 0.7287999987602234.\n",
      "Better model found at epoch 9 with accuracy value: 0.7418000102043152.\n",
      "Better model found at epoch 10 with accuracy value: 0.7486000061035156.\n",
      "Better model found at epoch 11 with accuracy value: 0.7576000094413757.\n",
      "Better model found at epoch 12 with accuracy value: 0.7752000093460083.\n",
      "Better model found at epoch 13 with accuracy value: 0.782800018787384.\n",
      "Better model found at epoch 14 with accuracy value: 0.8062000274658203.\n",
      "Better model found at epoch 16 with accuracy value: 0.8119999766349792.\n",
      "Better model found at epoch 18 with accuracy value: 0.8190000057220459.\n",
      "Better model found at epoch 19 with accuracy value: 0.8274000287055969.\n",
      "Better model found at epoch 21 with accuracy value: 0.8294000029563904.\n",
      "Better model found at epoch 22 with accuracy value: 0.8389999866485596.\n",
      "Better model found at epoch 25 with accuracy value: 0.8565999865531921.\n",
      "Better model found at epoch 29 with accuracy value: 0.8704000115394592.\n",
      "Better model found at epoch 41 with accuracy value: 0.8769999742507935.\n",
      "Better model found at epoch 43 with accuracy value: 0.8790000081062317.\n",
      "Better model found at epoch 46 with accuracy value: 0.8902000188827515.\n",
      "Better model found at epoch 60 with accuracy value: 0.8925999999046326.\n",
      "Better model found at epoch 62 with accuracy value: 0.8992000222206116.\n",
      "Better model found at epoch 70 with accuracy value: 0.9070000052452087.\n",
      "Better model found at epoch 95 with accuracy value: 0.9082000255584717.\n",
      "Better model found at epoch 97 with accuracy value: 0.9115999937057495.\n",
      "Better model found at epoch 107 with accuracy value: 0.9132000207901001.\n",
      "Better model found at epoch 117 with accuracy value: 0.9182000160217285.\n",
      "Better model found at epoch 140 with accuracy value: 0.9200000166893005.\n",
      "Better model found at epoch 141 with accuracy value: 0.9214000105857849.\n",
      "Better model found at epoch 156 with accuracy value: 0.9229999780654907.\n",
      "Better model found at epoch 162 with accuracy value: 0.9236000180244446.\n",
      "Better model found at epoch 171 with accuracy value: 0.923799991607666.\n",
      "Better model found at epoch 180 with accuracy value: 0.9241999983787537.\n",
      "Better model found at epoch 186 with accuracy value: 0.9247999787330627.\n",
      "Better model found at epoch 188 with accuracy value: 0.9259999990463257.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-622ec7567605>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveModelCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'improvement'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model_2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0004\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/fastai/train.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[0;34m(learn, cyc_len, max_lr, moms, div_factor, pct_start, final_div, wd, callbacks, tot_epochs, start_epoch)\u001b[0m\n\u001b[1;32m     21\u001b[0m     callbacks.append(OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor, pct_start=pct_start,\n\u001b[1;32m     22\u001b[0m                                        final_div=final_div, tot_epochs=tot_epochs, start_epoch=start_epoch))\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m def fit_fc(learn:Learner, tot_epochs:int=1, lr:float=defaults.lr,  moms:Tuple[float,float]=(0.95,0.85), start_pct:float=0.72,\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_bwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backward_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mskip_bwd\u001b[0m\u001b[0;34m:\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backward_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/fastai/callback.py\u001b[0m in \u001b[0;36mon_backward_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_backward_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;34m\"Handle end of gradient calculation.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'backward_end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall_mets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'skip_step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/fastai/callback.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, cb_name, call_mets, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcall_mets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_and_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_and_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/fastai/callback.py\u001b[0m in \u001b[0;36m_call_and_update\u001b[0;34m(self, cb, cb_name, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_and_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;34m\"Call `cb_name` on `cb` and update the inner state.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mifnone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'on_{cb_name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/fastai/callbacks/fp16.py\u001b[0m in \u001b[0;36mon_backward_end\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_backward_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;34m\"Convert the gradients back to FP32 and divide them by the scale.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgrad_overflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_scale\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_scale\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/fastai/callbacks/fp16.py\u001b[0m in \u001b[0;36mgrad_overflow\u001b[0;34m(param_group)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-inf'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cb = SaveModelCallback(learn, every='improvement', monitor='accuracy', name='model_2')\n",
    "learn.fit_one_cycle(1500, 3e-3, wd=0.0004, callbacks=[cb]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pv1Ys1fJOxI0"
   },
   "outputs": [],
   "source": [
    "####################### Testing ###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zSTp3jiyOxI7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph_dir = 'comp_graphs/new/*.dot' #'/content/mnt/My Drive/Colab Notebooks/ResearchProject/comp_graphs/experiment_1/best/indv_1/*.dot'\n",
    "graph = [AGraph(g) for g in glob.glob(graph_dir)]\n",
    "_, comp_graph = cell_graph.generate_comp_graph(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 860,
     "status": "ok",
     "timestamp": 1578951687810,
     "user": {
      "displayName": "cliff akosa",
      "photoUrl": "",
      "userId": "08793119188384687776"
     },
     "user_tz": -540
    },
    "id": "3XCZ37WkOxI_",
    "outputId": "9dfb177e-eb0b-445d-caad-69aa23d2ed6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.78781"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = arch_config(comp_graph=comp_graph,\n",
    "                   #depth_coeff=1.0,\n",
    "                   #width_coeff=1.0,\n",
    "                   channels=40,\n",
    "                   repeat_list=[3, 4, 3],\n",
    "                   classes=10)\n",
    "\n",
    "net = get_net(conf)\n",
    "count_parameters(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GQM1HF35OxJE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfms = get_transforms(do_flip=False)\n",
    "path = untar_data(URLs.CIFAR)\n",
    "bs = 256\n",
    "\n",
    "data = (ImageList.from_folder(path)\n",
    "        .split_by_folder(train='train', valid='test')\n",
    "        .label_from_folder()\n",
    "        .transform(tfms, size=32)\n",
    "        .databunch(bs=bs, num_workers=num_cpus())\n",
    "        .normalize(cifar_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nS-jVbg1OxJI"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0.1600816, tensor(0.9640), tensor(0.0360)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = '/home/cliff/ResearchProjects/geppy_nn/comp_graphs/new' #'/content/mnt/My Drive/Colab Notebooks/ResearchProject/models/'\n",
    "model = Learner(data, net, metrics=[accuracy, error_rate]).load(model_dir+'/model_1')\n",
    "model.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7482,
     "status": "ok",
     "timestamp": 1578951705123,
     "user": {
      "displayName": "cliff akosa",
      "photoUrl": "",
      "userId": "08793119188384687776"
     },
     "user_tz": -540
    },
    "id": "l7NjwVsQOxJO",
    "outputId": "0d96025f-bd2c-4c03-d33a-cf2cb17ac896"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kAfjfhBVOxJU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds,y,losses = model.get_preds(ds_type=DatasetType.Valid, with_loss=True)\n",
    "preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1MvzY-ESOxJY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I8paJKf7OxJb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aqiu-ZVZOxJg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cJUyPLoMOxJk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GBmkvi7-OxJp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kQuWTD4mOxJu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "evolution_search.ipynb",
   "provenance": [
    {
     "file_id": "1sFVrI5kmKHCFVSbnea50sJnWOiTrRt3a",
     "timestamp": 1577364210955
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
