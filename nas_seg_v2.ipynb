{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic segmentation of aerial images with deep networks\n",
    "\n",
    "This notebook presents a straightforward PyTorch implementation of a Fully Convolutional Network for semantic segmentation of aerial images. More specifically, we aim to automatically perform scene interpretation of images taken from a plane or a satellite by classifying every pixel into several land cover classes.\n",
    "\n",
    "As a demonstration, we are going to use the [SegNet architecture](http://mi.eng.cam.ac.uk/projects/segnet/) to segment aerial images over the cities of Vaihingen and Potsdam. The images are from the [ISPRS 2D Semantic Labeling dataset](http://www2.isprs.org/commissions/comm3/wg4/results.html). We will train a network to segment roads, buildings, vegetation and cars.\n",
    "\n",
    "This work is a PyTorch implementation of the baseline presented in [\"Beyond RGB: Very High Resolution Urban Remote Sensing With Multimodal Deep Networks \"](https://hal.archives-ouvertes.fr/hal-01636145), *Nicolas Audebert*, *Bertrand Le Saux* and *Sébastien Lefèvre*, ISPRS Journal, 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "This notebook requires a few useful libraries, e.g. `torch`, `scikit-image`, `numpy` and `matplotlib`. You can install everything using `pip install -r requirements.txt`.\n",
    "\n",
    "This is expected to run on GPU, and therefore you should use `torch` in combination with CUDA/cuDNN. This can probably be made to run on CPU but be warned that:\n",
    "  * you have to remove all calls to `torch.Tensor.cuda()` throughout this notebook,\n",
    "  * this will be very slow.\n",
    "  \n",
    "A \"small\" GPU should be enough, e.g. this runs fine on a 4.7GB Tesla K20m. It uses quite a lot of RAM as the dataset is stored in-memory (about 5GB for Vaihingen). You can spare some memory by disabling the caching below. 4GB should be more than enough without caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gepcore.utils import convolution\n",
    "from gepcore.utils import cell_graph\n",
    "from gepcore.entity import Gene, Chromosome, KExpressionGraph\n",
    "from gepcore.symbol import PrimitiveSet\n",
    "#from gepcore.operators import *\n",
    "from nas_seg.seg_model import get_net, arch_config\n",
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "# import fastai deep learning tools\n",
    "from fastai.vision.all import *\n",
    "\n",
    "\n",
    "# imports and stuff\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "import itertools\n",
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler\n",
    "import torch.nn.init\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "There are several parameters than can be tuned to use this notebook with different datasets. The default parameters are suitable for the ISPRS dataset, but you can change them to work with your data.\n",
    "\n",
    "### Examples\n",
    "\n",
    "  * Binary classification: `N_CLASSES = 2`\n",
    "  * Multi-spectral data (e.g. IRRGB): `IN_CHANNELS = 4`\n",
    "  * New folder naming convention : `DATA_FOLDER = MAIN_FOLDER + 'sentinel2/sentinel2_img_{}.tif'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "WINDOW_SIZE = (128, 128) # Patch size\n",
    "STRIDE = 32 # Stride for testing\n",
    "IN_CHANNELS = 3 # Number of input channels (e.g. RGB)\n",
    "FOLDER = \"/home/cliff/rs_imagery/ISPRS-DATASETS/\" # Replace with your \"/path/to/the/ISPRS/dataset/folder/\"\n",
    "BATCH_SIZE = 10 # Number of samples in a mini-batch\n",
    "\n",
    "LABELS = [\"roads\", \"buildings\", \"low veg.\", \"trees\", \"cars\", \"clutter\"] # Label names\n",
    "N_CLASSES = len(LABELS) # Number of classes\n",
    "WEIGHTS = torch.ones(N_CLASSES) # Weights for class balancing\n",
    "CACHE = True # Store the dataset in-memory\n",
    "\n",
    "DATASET = 'Vaihingen'\n",
    "\n",
    "if DATASET == 'Potsdam':\n",
    "    MAIN_FOLDER = FOLDER + 'Potsdam/'\n",
    "    # Uncomment the next line for IRRG data\n",
    "    # DATA_FOLDER = MAIN_FOLDER + '3_Ortho_IRRG/top_potsdam_{}_IRRG.tif'\n",
    "    # For RGB data\n",
    "    DATA_FOLDER = MAIN_FOLDER + '2_Ortho_RGB/top_potsdam_{}_RGB.tif'\n",
    "    LABEL_FOLDER = MAIN_FOLDER + '5_Labels_for_participants/top_potsdam_{}_label.tif'\n",
    "    ERODED_FOLDER = MAIN_FOLDER + '5_Labels_for_participants_no_Boundary/top_potsdam_{}_label_noBoundary.tif'    \n",
    "elif DATASET == 'Vaihingen':\n",
    "    MAIN_FOLDER = FOLDER + 'Vaihingen/'\n",
    "    DATA_FOLDER = MAIN_FOLDER + 'top/top_mosaic_09cm_area{}.tif'\n",
    "    LABEL_FOLDER = MAIN_FOLDER + 'gts_for_participants/top_mosaic_09cm_area{}.tif'\n",
    "    ERODED_FOLDER = MAIN_FOLDER + 'gts_eroded_for_participants/top_mosaic_09cm_area{}_noBoundary.tif'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the dataset\n",
    "\n",
    "First, let's check that we are able to access the dataset and see what's going on. We use ```scikit-image``` for image manipulation.\n",
    "\n",
    "As the ISPRS dataset is stored with a ground truth in the RGB format, we need to define the color palette that can map the label id to its RGB color. We define two helper functions to convert from numeric to colors and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISPRS color palette\n",
    "# Let's define the standard ISPRS color palette\n",
    "palette = {0 : (255, 255, 255), # Impervious surfaces (white)\n",
    "           1 : (0, 0, 255),     # Buildings (blue)\n",
    "           2 : (0, 255, 255),   # Low vegetation (cyan)\n",
    "           3 : (0, 255, 0),     # Trees (green)\n",
    "           4 : (255, 255, 0),   # Cars (yellow)\n",
    "           5 : (255, 0, 0),     # Clutter (red)\n",
    "           6 : (0, 0, 0)}       # Undefined (black)\n",
    "\n",
    "invert_palette = {v: k for k, v in palette.items()}\n",
    "\n",
    "def convert_to_color(arr_2d, palette=palette):\n",
    "    \"\"\" Numeric labels to RGB-color encoding \"\"\"\n",
    "    arr_3d = np.zeros((arr_2d.shape[0], arr_2d.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "    for c, i in palette.items():\n",
    "        m = arr_2d == c\n",
    "        arr_3d[m] = i\n",
    "\n",
    "    return arr_3d\n",
    "\n",
    "def convert_from_color(arr_3d, palette=invert_palette):\n",
    "    \"\"\" RGB-color encoding to grayscale labels \"\"\"\n",
    "    arr_2d = np.zeros((arr_3d.shape[0], arr_3d.shape[1]), dtype=np.uint8)\n",
    "\n",
    "    for c, i in palette.items():\n",
    "        m = np.all(arr_3d == np.array(c).reshape(1, 1, 3), axis=2)\n",
    "        arr_2d[m] = i\n",
    "\n",
    "    return arr_2d\n",
    "\n",
    "# # We load one tile from the dataset and we display it\n",
    "# img = io.imread('/home/cliff/rs_imagery/ISPRS-DATASETS/Vaihingen/top/top_mosaic_09cm_area11.tif')\n",
    "# fig = plt.figure()\n",
    "# fig.add_subplot(121)\n",
    "# plt.imshow(img)\n",
    "\n",
    "# # We load the ground truth\n",
    "# gt = io.imread('/home/cliff/rs_imagery/ISPRS-DATASETS/Vaihingen/gts_for_participants/top_mosaic_09cm_area11.tif')\n",
    "# fig.add_subplot(122)\n",
    "# plt.imshow(gt)\n",
    "# plt.show()\n",
    "\n",
    "# # We also check that we can convert the ground truth into an array format\n",
    "# array_gt = convert_from_color(gt)\n",
    "# print(\"Ground truth in numerical format has shape ({},{}) : \\n\".format(*array_gt.shape[:2]), array_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define a bunch of utils functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "\n",
    "def get_random_pos(img, window_shape):\n",
    "    \"\"\" Extract of 2D random patch of shape window_shape in the image \"\"\"\n",
    "    w, h = window_shape\n",
    "    W, H = img.shape[:-1] #img.shape[-2:]\n",
    "    x1 = random.randint(0, W - w - 1)\n",
    "    x2 = x1 + w\n",
    "    y1 = random.randint(0, H - h - 1)\n",
    "    y2 = y1 + h\n",
    "    return x1, x2, y1, y2\n",
    "\n",
    "\n",
    "# def CrossEntropy2d(input, target, weight=None):#, size_average=True):\n",
    "#     \"\"\" 2D version of the cross entropy loss \"\"\"\n",
    "#     dim = input.dim()\n",
    "#     if dim == 2:\n",
    "#         return F.cross_entropy(input, target, weight)\n",
    "#     elif dim == 4:\n",
    "#         output = input.view(input.size(0),input.size(1), -1)\n",
    "#         output = torch.transpose(output,1,2).contiguous()\n",
    "#         output = output.view(-1,output.size(2))\n",
    "#         target = target.view(-1)\n",
    "#         return F.cross_entropy(output, target, weight)\n",
    "#     else:\n",
    "#         raise ValueError('Expected 2 or 4 dimensions (got {})'.format(dim))\n",
    "\n",
    "def accuracy(input, target):\n",
    "    return 100 * float(np.count_nonzero(input == target)) / target.size\n",
    "\n",
    "def sliding_window(top, step=10, window_size=(20,20)):\n",
    "    \"\"\" Slide a window_shape window across the image with a stride of step \"\"\"\n",
    "    for x in range(0, top.shape[0], step):\n",
    "        if x + window_size[0] > top.shape[0]:\n",
    "            x = top.shape[0] - window_size[0]\n",
    "        for y in range(0, top.shape[1], step):\n",
    "            if y + window_size[1] > top.shape[1]:\n",
    "                y = top.shape[1] - window_size[1]\n",
    "            yield x, y, window_size[0], window_size[1]\n",
    "            \n",
    "def count_sliding_window(top, step=10, window_size=(20,20)):\n",
    "    \"\"\" Count the number of windows in an image \"\"\"\n",
    "    c = 0\n",
    "    for x in range(0, top.shape[0], step):\n",
    "        if x + window_size[0] > top.shape[0]:\n",
    "            x = top.shape[0] - window_size[0]\n",
    "        for y in range(0, top.shape[1], step):\n",
    "            if y + window_size[1] > top.shape[1]:\n",
    "                y = top.shape[1] - window_size[1]\n",
    "            c += 1\n",
    "    return c\n",
    "\n",
    "def grouper(n, iterable):\n",
    "    \"\"\" Browse an iterator by chunk of n elements \"\"\"\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "        chunk = tuple(itertools.islice(it, n))\n",
    "        if not chunk:\n",
    "            return\n",
    "        yield chunk\n",
    "\n",
    "def metrics(predictions, gts, label_values=LABELS):\n",
    "    cm = confusion_matrix(\n",
    "            gts,\n",
    "            predictions,\n",
    "            range(len(label_values)))\n",
    "    \n",
    "    print(\"Confusion matrix :\")\n",
    "    print(cm)\n",
    "    \n",
    "    print(\"---\")\n",
    "    \n",
    "    # Compute global accuracy\n",
    "    total = sum(sum(cm))\n",
    "    accuracy = sum([cm[x][x] for x in range(len(cm))])\n",
    "    accuracy *= 100 / float(total)\n",
    "    print(\"{} pixels processed\".format(total))\n",
    "    print(\"Total accuracy : {}%\".format(accuracy))\n",
    "    \n",
    "    print(\"---\")\n",
    "    \n",
    "    # Compute F1 score\n",
    "    F1Score = np.zeros(len(label_values))\n",
    "    for i in range(len(label_values)):\n",
    "        try:\n",
    "            F1Score[i] = 2. * cm[i,i] / (np.sum(cm[i,:]) + np.sum(cm[:,i]))\n",
    "        except:\n",
    "            # Ignore exception if there is no element in class i for test set\n",
    "            pass\n",
    "    print(\"F1Score :\")\n",
    "    for l_id, score in enumerate(F1Score):\n",
    "        print(\"{}: {}\".format(label_values[l_id], score))\n",
    "\n",
    "    print(\"---\")\n",
    "        \n",
    "    # Compute kappa coefficient\n",
    "    total = np.sum(cm)\n",
    "    pa = np.trace(cm) / float(total)\n",
    "    pe = np.sum(np.sum(cm, axis=0) * np.sum(cm, axis=1)) / float(total*total)\n",
    "    kappa = (pa - pe) / (1 - pe);\n",
    "    print(\"Kappa: \" + str(kappa))\n",
    "    return accuracy\n",
    "\n",
    "# IoU = TP / (TP + FP + FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "\n",
    "We define a PyTorch dataset (```torch.utils.data.Dataset```) that loads all the tiles in memory and performs random sampling. Tiles are stored in memory on the fly.\n",
    "\n",
    "The dataset also performs random data augmentation (horizontal and vertical flips) and normalizes the data in [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "\n",
    "# class ISPRS_dataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, ids, data_files=DATA_FOLDER, label_files=LABEL_FOLDER,\n",
    "#                             cache=False, augmentation=True):\n",
    "#         super(ISPRS_dataset, self).__init__()\n",
    "        \n",
    "#         self.augmentation = augmentation\n",
    "#         self.cache = cache\n",
    "        \n",
    "#         # List of files\n",
    "#         self.data_files = [DATA_FOLDER.format(id) for id in ids]\n",
    "#         self.label_files = [LABEL_FOLDER.format(id) for id in ids]\n",
    "\n",
    "#         # Sanity check : raise an error if some files do not exist\n",
    "#         for f in self.data_files + self.label_files:\n",
    "#             if not os.path.isfile(f):\n",
    "#                 raise KeyError('{} is not a file !'.format(f))\n",
    "        \n",
    "#         # Initialize cache dicts\n",
    "#         self.data_cache_ = {}\n",
    "#         self.label_cache_ = {}\n",
    "            \n",
    "    \n",
    "#     def __len__(self):\n",
    "#         # Default epoch size is 10 000 samples\n",
    "#         return 8000\n",
    "    \n",
    "#     @classmethod\n",
    "#     def data_augmentation(cls, *arrays, flip=True, mirror=True):\n",
    "#         will_flip, will_mirror = False, False\n",
    "#         if flip and random.random() < 0.5:\n",
    "#             will_flip = True\n",
    "#         if mirror and random.random() < 0.5:\n",
    "#             will_mirror = True\n",
    "        \n",
    "#         results = []\n",
    "#         for array in arrays:\n",
    "#             if will_flip:\n",
    "#                 if len(array.shape) == 2:\n",
    "#                     array = array[::-1, :]\n",
    "#                 else:\n",
    "#                     array = array[:, ::-1, :]\n",
    "#             if will_mirror:\n",
    "#                 if len(array.shape) == 2:\n",
    "#                     array = array[:, ::-1]\n",
    "#                 else:\n",
    "#                     array = array[:, :, ::-1]\n",
    "#             results.append(np.copy(array))\n",
    "            \n",
    "#         return tuple(results)\n",
    "    \n",
    "#     def __getitem__(self, i):\n",
    "#         # Pick a random image\n",
    "#         random_idx = random.randint(0, len(self.data_files) - 1)\n",
    "        \n",
    "#         # If the tile hasn't been loaded yet, put in cache\n",
    "#         if random_idx in self.data_cache_.keys():\n",
    "#             data = self.data_cache_[random_idx]\n",
    "#         else:\n",
    "#             # Data is normalized in [0, 1]\n",
    "#             data = 1/255 * np.asarray(io.imread(self.data_files[random_idx]).transpose((2,0,1)), dtype='float32')\n",
    "#             if self.cache:\n",
    "#                 self.data_cache_[random_idx] = data\n",
    "            \n",
    "#         if random_idx in self.label_cache_.keys():\n",
    "#             label = self.label_cache_[random_idx]\n",
    "#         else: \n",
    "#             # Labels are converted from RGB to their numeric values\n",
    "#             label = np.asarray(convert_from_color(io.imread(self.label_files[random_idx])), dtype='int64')\n",
    "#             if self.cache:\n",
    "#                 self.label_cache_[random_idx] = label\n",
    "\n",
    "#         # Get a random patch\n",
    "#         x1, x2, y1, y2 = get_random_pos(data, WINDOW_SIZE)\n",
    "#         data_p = data[:, x1:x2,y1:y2]\n",
    "#         label_p = label[x1:x2,y1:y2]\n",
    "        \n",
    "#         # Data augmentation\n",
    "#         data_p, label_p = self.data_augmentation(data_p, label_p)\n",
    "\n",
    "#         # Return the torch.Tensor values\n",
    "#         return (torch.from_numpy(data_p),\n",
    "#                 torch.from_numpy(label_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyDataset(Dataset):\n",
    "#     def __init__(self, image_paths, target_paths, train=True):\n",
    "#         self.image_paths = image_paths\n",
    "#         self.target_paths = target_paths\n",
    "\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         image = Image.open(self.image_paths[index])\n",
    "#         mask = Image.open(self.target_paths[index])\n",
    "#         x, y = self.transform(image, mask)\n",
    "#         return x, y\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ISPRSDataset(VisionDataset):\n",
    "#     def __init__(self, img_dir, msk_dir, transform=None, target_transform=None, transforms=None):\n",
    "#         super(ISPRSDataset, self).__init__(transform, target_transform, transforms)\n",
    "#         self.img_dir = img_dir\n",
    "#         self.msk_dir = msk_dir\n",
    "#         self.img_files = get_files(self.img_dir)\n",
    "#         self.get_mask = lambda x: self.msk_dir/f'{x.stem}{x.suffix}'\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.img_files)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img_file = self.img_files[idx]\n",
    "#         mask_file = self.get_mask(img_file)\n",
    "#         img = io.imread(img_file)\n",
    "#         mask = np.asarray(io.imread(mask_file))\n",
    "#         if self.transforms is not None:\n",
    "#             img, mask = self.transforms(img, mask)\n",
    "#         return img, mask\n",
    "    \n",
    "data_path = Path('/home/cliff/rs_imagery/ISPRS-DATASETS/Vaihingen/vaihingen_256')\n",
    "img_path = data_path/'images/train'\n",
    "msk_path = data_path/'masks/train'\n",
    "\n",
    "mask_labels = [\"roads\", \"buildings\", \"low veg.\", \"trees\", \"cars\", \"clutter\"] \n",
    "num_classes = len(mask_labels) \n",
    "print(img_path, '\\n', msk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "#from torch.utils.data import Dataset\n",
    "import torchvision.transforms.functional as tf\n",
    "\n",
    "class ISPRS_dataset(VisionDataset):\n",
    "    def __init__(self, data_dir, label_dir, transform=None, target_transform=None, transforms=None): \n",
    "        super(ISPRS_dataset, self).__init__(transform, target_transform, transforms)\n",
    "        self.data_dir = data_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.data_files = get_files(self.data_dir)\n",
    "        self.label_files = lambda x: self.label_dir/f'{x.stem}{x.suffix}'\n",
    "        \n",
    "#         #List of files\n",
    "#         self.data_files = [DATA_FOLDER.format(id) for id in ids]\n",
    "#         self.label_files = [LABEL_FOLDER.format(id) for id in ids]\n",
    "\n",
    "#         #Sanity check : raise an error if some files do not exist\n",
    "#         for f in self.data_files + self.label_files:\n",
    "#             if not os.path.isfile(f):\n",
    "#                 raise KeyError('{} is not a file !'.format(f))\n",
    "        \n",
    "#         #Initialize cache dicts\n",
    "#         self.data_cache_ = {}\n",
    "#         self.label_cache_ = {}\n",
    "#         for i in range(len(self.data_files)):\n",
    "#             data = np.asarray(io.imread(self.data_files[i]))\n",
    "#             label = np.asarray(convert_from_color(io.imread(self.label_files[i])))\n",
    "#             self.data_cache_[i] = data\n",
    "#             self.label_cache_[i] = label\n",
    "\n",
    "            \n",
    "    def flip_img(self, image, mask):\n",
    "        # to PIL image\n",
    "        image = tf.to_pil_image(image)\n",
    "        mask = tf.to_pil_image(mask)\n",
    "        \n",
    "        # Random horizontal flipping\n",
    "        if random.random() > 0.5:\n",
    "            image = tf.hflip(image)\n",
    "            mask = tf.hflip(mask)\n",
    "\n",
    "        # Random vertical flipping\n",
    "        if random.random() > 0.5:\n",
    "            image = tf.vflip(image)\n",
    "            mask = tf.vflip(mask)\n",
    "\n",
    "        # image to np array\n",
    "        image = np.array(image)\n",
    "        mask = np.array(mask)\n",
    "        return image, mask\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "#         total = 0\n",
    "#         for i in self.data_cache_:\n",
    "#             total += count_sliding_window(self.data_cache_[i], step=128, window_size=(256, 256)) \n",
    "        return len(self.data_files)\n",
    "    \n",
    "             \n",
    "    def __getitem__(self, idx):\n",
    "        data_file = self.data_files[idx]\n",
    "        label_file = self.label_files(data_file)\n",
    "        data_p = np.asarray(io.imread(data_file))\n",
    "        label_p = np.asarray(convert_from_color(io.imread(label_file)))\n",
    "\n",
    "        # Data augmentation\n",
    "        if self.transforms is not None:\n",
    "            data_p, label_p = self.flip_img(data_p, label_p)\n",
    "            data_p, label_p = self.transforms(data_p, label_p)\n",
    "\n",
    "        return (data_p, label_p)\n",
    "        \n",
    "#     for idx in self.data_cache_:\n",
    "#              #for i, coords in enumerate(grouper(1, sliding_window(img, step=128, window_size=(256,256)))):\n",
    "#             #for j, coords in enumerate(sliding_window(self.data_cache_[idx], step=128, window_size=(256,256))):\n",
    "#             coords = sliding_window(self.data_cache_[idx], step=128, window_size=(256,256))\n",
    "#             print(coords)\n",
    "#             x, y, w, h = coords\n",
    "#             data_p = self.data_cache_[j][x:x+w, y:y+h, :]\n",
    "#             label_p = self.label_cache_[j][x:x+w, y:y+h]               \n",
    "# #         # Pick a random image\n",
    "#         random_idx = random.randint(0, len(self.data_files) - 1)\n",
    "        \n",
    "#         # If the tile hasn't been loaded yet, put in cache\n",
    "#         if random_idx in self.data_cache_.keys():\n",
    "#             data = self.data_cache_[random_idx]\n",
    "#         else:\n",
    "#             data = np.asarray(io.imread(self.data_files[random_idx]))\n",
    "#             if self.cache:\n",
    "#                 self.data_cache_[random_idx] = data\n",
    "            \n",
    "#         if random_idx in self.label_cache_.keys():\n",
    "#             label = self.label_cache_[random_idx]\n",
    "#         else: \n",
    "#             # Labels are converted from RGB to their numeric values\n",
    "#             label = np.asarray(convert_from_color(io.imread(self.label_files[random_idx])))\n",
    "#             if self.cache:\n",
    "#                 self.label_cache_[random_idx] = label\n",
    "\n",
    "#         # Get a random patch\n",
    "#         x1, x2, y1, y2 = get_random_pos(data, WINDOW_SIZE)\n",
    "#         data_p = data[x1:x2,y1:y2,:]\n",
    "#         label_p = label[x1:x2,y1:y2]\n",
    "        \n",
    "#         # Data augmentation\n",
    "#         if self.transforms is not None:\n",
    "#             data_p, label_p = self.flip_img(data_p, label_p)\n",
    "#             data_p, label_p = self.transforms(data_p, label_p)\n",
    "        \n",
    "#         return (data_p, label_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network definition\n",
    "\n",
    "We can now define the Fully Convolutional network based on the SegNet architecture. We could use any other network as drop-in replacement, provided that the output has dimensions `(N_CLASSES, W, H)` where `W` and `H` are the sliding window dimensions (i.e. the network should preserve the spatial dimensions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygraphviz import AGraph\n",
    "#import glob\n",
    "\n",
    "graph = [AGraph(g) for g in glob('../graphs/*.dot')]\n",
    "_, comp_graphs = cell_graph.generate_comp_graph(graph)\n",
    "\n",
    "conf = arch_config(comp_graphs=comp_graphs, channels=64, classes=N_CLASSES)\n",
    "net = get_net(conf)\n",
    "net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now instantiate the network using the specified parameters. By default, the weights will be initialized using the [He policy](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## # instantiate the network\n",
    "# # net = SegNet()\n",
    "# # define primitive set\n",
    "# pset = PrimitiveSet('cnn')\n",
    "# # add cellular encoding program symbols\n",
    "# # pset.add_program_symbol(cell_graph.end)\n",
    "# pset.add_program_symbol(cell_graph.seq)\n",
    "# pset.add_program_symbol(cell_graph.cpo)\n",
    "# pset.add_program_symbol(cell_graph.cpi)\n",
    "\n",
    "# # add convolutional operations symbols\n",
    "# conv_symbol = convolution.get_symbol()\n",
    "# # pset.add_program_symbol(conv_symbol.conv1x1)\n",
    "# # pset.add_program_symbol(conv_symbol.conv3x3)\n",
    "# # pset.add_cell_symbol(conv_symbol.dwconv3x3)\n",
    "# # pset.add_cell_symbol(conv_symbol.sepconv3x3)\n",
    "# # pset.add_cell_symbol(conv_symbol.sepconv5x5)\n",
    "# pset.add_cell_symbol(conv_symbol.dilconv3x3)\n",
    "# pset.add_cell_symbol(conv_symbol.dilconv5x5)\n",
    "# pset.add_cell_symbol(conv_symbol.conv3x3)\n",
    "# # pset.add_cell_symbol(conv_symbol.conv1x1)\n",
    "# # pset.add_cell_symbol(conv_symbol.dwconv3x3)\n",
    "# # pset.add_cell_symbol(conv_symbol.maxpool3x3)\n",
    "# # pset.add_cell_symbol(conv_symbol.avgpool3x3)\n",
    "\n",
    "# def gene_gen():\n",
    "#     return Gene(pset, 1)\n",
    "\n",
    "# ch = Chromosome(gene_gen, 3)\n",
    "# graph, comp_graphs = cell_graph.generate_comp_graph(ch)\n",
    "\n",
    "# cell_graph.save_graph(graph, '../graphs/')\n",
    "# cell_graph.draw_graph(graph, '../graphs/')\n",
    "\n",
    "# conf = arch_config(comp_graphs=comp_graphs,\n",
    "#                    channels=64,\n",
    "#                    classes=N_CLASSES)\n",
    "\n",
    "# net = get_net(conf)\n",
    "# net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "We now create a train/test split. If you want to use another dataset, you have to adjust the method to collect all filenames. In our case, we specify a fixed train/test split for the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "tfms = transforms.Compose([transforms.ToTensor(),\n",
    "                           transforms.Normalize([0.4776, 0.3226, 0.3189], [0.1816, 0.1224, 0.1185])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "if DATASET == 'Potsdam':\n",
    "    all_files = sorted(glob(LABEL_FOLDER.replace('{}', '*')))\n",
    "    all_ids = [\"\".join(f.split('')[5:7]) for f in all_files]\n",
    "elif DATASET == 'Vaihingen':\n",
    "    #all_ids = \n",
    "    all_files = sorted(glob(LABEL_FOLDER.replace('{}', '*')))\n",
    "    all_ids = [f.split('area')[-1].split('.')[0] for f in all_files]\n",
    "# Random tile numbers for train/test split\n",
    "train_ids = random.sample(all_ids, 2 * len(all_ids) // 3 + 1)\n",
    "test_ids = list(set(all_ids) - set(train_ids))\n",
    "\n",
    "# Exemple of a train/test split on Vaihingen :\n",
    "train_ids = ['1', '3', '11', '13', '17', '26', '28', '32', '34', '37']\n",
    "valid_ids = ['21', '15'] \n",
    "test_ids = ['5', '7', '23', '30']\n",
    "print(\"Tiles for training : \", train_ids)\n",
    "print(\"Tiles for testing : \", test_ids)\n",
    "print(\"Tiles for validation : \", valid_ids)\n",
    "\n",
    "train_set = ISPRS_dataset(data_dir=img_path, label_dir=msk_path, transforms=tfms)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img, lb = iter(train_loader).next() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plt.imshow(convert_to_color(lb[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#im = transforms.ToPILImage()(img[0])\n",
    "#plt.imshow((img[0].numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing the optimizer\n",
    "\n",
    "We use the standard Stochastic Gradient Descent algorithm to optimize the network's weights.\n",
    "\n",
    "The encoder is trained at half the learning rate of the decoder, as we rely on the pre-trained VGG-16 weights. We use the ``torch.optim.lr_scheduler`` to reduce the learning rate by 10 after 25, 35 and 45 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 1e-2\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=base_lr, momentum=0.9, weight_decay=0.0005)\n",
    "optimizer = optim.Adam(net.parameters(), weight_decay=1e-2)\n",
    "# We define the scheduler\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr, steps_per_epoch=len(train_loader), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(net.parameters(), lr=1e-2, weight_decay=0.004)\n",
    "# # We define the scheduler\n",
    "# #scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [10, 25, 35], gamma=0.1)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torchvision.transforms.functional as tf\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, test_ids, all=False, stride=WINDOW_SIZE[0], batch_size=BATCH_SIZE, window_size=WINDOW_SIZE):\n",
    "    # Use the network on the test set\n",
    "    test_images = (np.asarray(io.imread(DATA_FOLDER.format(id))) for id in test_ids)\n",
    "    test_labels = (np.asarray(io.imread(LABEL_FOLDER.format(id)), dtype='uint8') for id in test_ids)\n",
    "    eroded_labels = (convert_from_color(io.imread(ERODED_FOLDER.format(id))) for id in test_ids)\n",
    "    all_preds = []\n",
    "    all_gts = []\n",
    "    \n",
    "    # Switch the network to inference mode\n",
    "    net.eval()\n",
    "\n",
    "    for img, gt, gt_e in tqdm(zip(test_images, test_labels, eroded_labels), total=len(test_ids), leave=False):\n",
    "        pred = np.zeros(img.shape[:2] + (N_CLASSES,))\n",
    "\n",
    "        total = count_sliding_window(img, step=stride, window_size=window_size) // batch_size\n",
    "        for i, coords in enumerate(tqdm(grouper(batch_size, \n",
    "                                                sliding_window(img, step=stride, window_size=window_size)), \n",
    "                                                total=total, leave=False)):\n",
    "            # Display in progress results\n",
    "            if i > 0 and total > 10 and i % int(10 * total / 100) == 0:\n",
    "                    _pred = np.argmax(pred, axis=-1)\n",
    "                    fig = plt.figure()\n",
    "                    fig.add_subplot(1,3,1)\n",
    "                    plt.imshow(np.asarray(img, dtype='uint8'))\n",
    "                    fig.add_subplot(1,3,2)\n",
    "                    plt.imshow(convert_to_color(_pred))\n",
    "                    fig.add_subplot(1,3,3)\n",
    "                    plt.imshow(gt)\n",
    "                    clear_output()\n",
    "                    plt.show()\n",
    "                    \n",
    "            # Build the tensor\n",
    "            #image_patches = [np.copy(img[x:x+w, y:y+h]).transpose((2,0,1)) for x,y,w,h in coords]\n",
    "            #image_patches = np.asarray(image_patches)\n",
    "            #image_patches = Variable(torch.from_numpy(image_patches).cuda(), volatile=True)\n",
    "\n",
    "            image_patches = [torch.clone(tf.normalize(tf.to_tensor(img[x:x+w, y:y+h]), \n",
    "                                        [0.4776, 0.3226, 0.3189], [0.1816, 0.1224, 0.1185]))\n",
    "                             for x,y,w,h in coords]\n",
    "            image_patches = torch.stack(image_patches).cuda()\n",
    "            #print(image_patches)\n",
    "            \n",
    "            #image_patches = Variable(torch.from_numpy(image_patches).cuda(), volatile=True)\n",
    "            \n",
    "            # Do the inference\n",
    "            outs = net(image_patches)\n",
    "            outs = outs.data.cpu().numpy()\n",
    "            \n",
    "            # Fill in the results array\n",
    "            for out, (x, y, w, h) in zip(outs, coords):\n",
    "                out = out.transpose((1,2,0))\n",
    "                pred[x:x+w, y:y+h] += out\n",
    "            del(outs)\n",
    "\n",
    "        pred = np.argmax(pred, axis=-1)\n",
    "\n",
    "        # Display the result\n",
    "        clear_output()\n",
    "        fig = plt.figure()\n",
    "        fig.add_subplot(1,3,1)\n",
    "        plt.imshow(np.asarray(img, dtype='uint8'))\n",
    "        fig.add_subplot(1,3,2)\n",
    "        plt.imshow(convert_to_color(pred))\n",
    "        fig.add_subplot(1,3,3)\n",
    "        plt.imshow(gt)\n",
    "        plt.show()\n",
    "\n",
    "        all_preds.append(pred)\n",
    "        all_gts.append(gt_e)\n",
    "\n",
    "        clear_output()\n",
    "        # Compute some metrics\n",
    "        metrics(pred.ravel(), gt_e.ravel())\n",
    "        accuracy = metrics(np.concatenate([p.ravel() for p in all_preds]), \n",
    "                           np.concatenate([p.ravel() for p in all_gts]).ravel())\n",
    "    if all:\n",
    "        return accuracy, all_preds, all_gts\n",
    "    else:\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, optimizer, epochs, scheduler=None, weights=WEIGHTS, save_epoch = 1):\n",
    "    losses = np.zeros(1000000)\n",
    "    mean_losses = np.zeros(100000000)\n",
    "    #weights = weights.cuda()\n",
    "\n",
    "    #criterion = nn.NLLLoss2d(weight=weights)\n",
    "    iter_ = 0\n",
    "    \n",
    "    for e in range(1, epochs + 1):\n",
    "        #if scheduler is not None:\n",
    "           # scheduler.step()\n",
    "        net.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            #data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "            data = data.to(device='cuda', dtype=torch.float32)\n",
    "            target = target.to(device='cuda', dtype=torch.long).squeeze()\n",
    "            optimizer.zero_grad()\n",
    "            output = net(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #scheduler.step()\n",
    "            losses[iter_] = loss.item()\n",
    "            mean_losses[iter_] = np.mean(losses[max(0,iter_-100):iter_])\n",
    "            \n",
    "            if iter_ % 100 == 0:\n",
    "                clear_output()\n",
    "                #rgb = np.asarray(255 * np.transpose(data.data.cpu().numpy()[0],(1,2,0)), dtype='uint8')\n",
    "                pred = np.argmax(output.data.cpu().numpy()[0], axis=0)\n",
    "                gt = target.data.cpu().numpy()[0]\n",
    "                print('Train (epoch {}/{}) [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAccuracy: {}'.format(\n",
    "                    e, epochs, batch_idx, len(train_loader),\n",
    "                    100. * batch_idx / len(train_loader), loss.item(), accuracy(pred, gt)))\n",
    "                plt.plot(mean_losses[:iter_]) and plt.show()\n",
    "                fig = plt.figure()\n",
    "                #fig.add_subplot(131)\n",
    "                #plt.imshow(rgb)\n",
    "                #plt.title('RGB')\n",
    "                fig.add_subplot(131)\n",
    "                plt.imshow(convert_to_color(gt))\n",
    "                plt.title('Ground truth')\n",
    "                fig.add_subplot(132)\n",
    "                plt.title('Prediction')\n",
    "                plt.imshow(convert_to_color(pred))\n",
    "                plt.show()\n",
    "            iter_ += 1\n",
    "            \n",
    "            del(data, target, loss)\n",
    "            \n",
    "        if e % save_epoch == 0:\n",
    "            # We validate with the largest possible stride for faster computing\n",
    "            acc = test(net, valid_ids, all=False)\n",
    "            scheduler.step(acc)\n",
    "            torch.save(net.state_dict(), './segnet256_epoch{}_{}'.format(e, acc))\n",
    "    torch.save(net.state_dict(), './segnet_final')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network\n",
    "\n",
    "Let's train the network for 50 epochs. The `matplotlib` graph is periodically udpated with the loss plot and a sample inference. Depending on your GPU, this might take from a few hours (Titan Pascal) to a full day (old K20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(net, optimizer, 50, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the network\n",
    "\n",
    "Now that the training has ended, we can load the final weights and test the network using a reasonable stride, e.g. half or a quarter of the window size. Inference time depends on the chosen stride, e.g. a step size of 32 (75% overlap) will take ~15 minutes, but no overlap will take only one minute or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load('./model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, all_preds, all_gts = test(net, test_ids, all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Confusion matrix :\n",
    "[[4493643  157861  114900   50671    6974    4918]\n",
    " [ 189511 5306269   30744   10540      69     458]\n",
    " [ 102023   89794 2316540  463292     717      27]\n",
    " [  23284    9544  121207 3921389     124     101]\n",
    " [  41181    3322     455     557  133306    7710]\n",
    " [   4067     763     567      15       0     980]]\n",
    "---\n",
    "17607523 pixels processed\n",
    "Total accuracy : 91.8478255004977%\n",
    "---\n",
    "F1Score :\n",
    "roads: 0.9281820438895197\n",
    "buildings: 0.9556416377851561\n",
    "low veg.: 0.8337667357831099\n",
    "trees: 0.9202856146122447\n",
    "cars: 0.8135334629151015\n",
    "clutter: 0.09521033712231614\n",
    "---\n",
    "Kappa: 0.8900782505413787"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the results\n",
    "\n",
    "We can visualize and save the resulting tiles for qualitative assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p, id_ in zip(all_preds, test_ids):\n",
    "    img = convert_to_color(p)\n",
    "    plt.imshow(img) and plt.show()\n",
    "    io.imsave('./inference_tile_{}.png'.format(id_), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
