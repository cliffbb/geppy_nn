{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! Good to go!\n"
     ]
    }
   ],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.distributed import *\n",
    "from fastai.metrics import error_rate\n",
    "from fastai.callbacks import SaveModelCallback\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  print(\"Great! Good to go!\")\n",
    "else:\n",
    "  print('CUDA is not up!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gepcore.utils import cell_graph, convolution\n",
    "from gepcore.entity import Gene, Chromosome\n",
    "from gepcore.symbol import PrimitiveSet\n",
    "from gepnet.model import get_gepnet, arch_config\n",
    "from gepnet.utils import count_parameters\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygraphviz import AGraph\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get chromosme from file\n",
    "graph = [AGraph(g) for g in glob.glob('nb_graphs/rs/run_3/*.dot')]\n",
    "_, comp_graph = cell_graph.generate_comp_graph(graph)\n",
    "cell_graph.draw_graph(graph, 'nb_graphs/rs/run_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# generate new chromosome\n",
    "# define primitive set\n",
    "pset = PrimitiveSet('cnn')\n",
    "\n",
    "# add cellular encoding program symbols\n",
    "pset.add_program_symbol(cell_graph.end)\n",
    "pset.add_program_symbol(cell_graph.seq)\n",
    "pset.add_program_symbol(cell_graph.cpo)\n",
    "pset.add_program_symbol(cell_graph.cpi)\n",
    "\n",
    "# add convolutional operations symbols\n",
    "conv_symbol = convolution.get_symbol()\n",
    "pset.add_cell_symbol(conv_symbol.conv1x1)\n",
    "pset.add_cell_symbol(conv_symbol.conv3x3)\n",
    "pset.add_cell_symbol(conv_symbol.dwconv3x3)\n",
    "#pset.add_cell_symbol(conv_symbol.conv1x3)\n",
    "#pset.add_cell_symbol(conv_symbol.conv3x1)\n",
    "#pset.add_cell_symbol(conv_symbol.maxpool3x3)\n",
    "\n",
    "def gene_gen():\n",
    "    return Gene(pset, 3)\n",
    "\n",
    "ch = Chromosome(gene_gen, 4)\n",
    "graph, comp_graph = cell_graph.generate_comp_graph(ch)\n",
    "\n",
    "cell_graph.save_graph(graph, 'nb_graphs/rs/run_4')\n",
    "cell_graph.draw_graph(graph, 'nb_graphs/rs/run_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# seed = 221\n",
    "# random.seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# enable torch backends\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "#torch.backends.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.604421"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = arch_config(comp_graph=comp_graph,\n",
    "                   depth_coeff=1.0,\n",
    "                   width_coeff=1.0,\n",
    "                   channels=24,\n",
    "                   repeat_list=[2, 3, 3, 2],\n",
    "                   classes=45)\n",
    "\n",
    "net = get_gepnet(conf)\n",
    "count_parameters(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = Path(\"/home/cliff/NWPU-RESISC45\")\n",
    "tfms = get_transforms(flip_vert=True, max_lighting=0.1, max_zoom=1.05, max_warp=0.)\n",
    "\n",
    "bs = 128\n",
    "data = (ImageList.from_folder(path/'train')\n",
    "        .split_by_rand_pct(valid_pct=0.2) \n",
    "        .label_from_folder()\n",
    "        .transform(tfms, size=224)\n",
    "        .databunch(bs=bs, num_workers=num_cpus())\n",
    "        .normalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=ImageDataBunch;\n",
       "\n",
       "Train: LabelList (20160 items)\n",
       "x: ImageList\n",
       "Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\n",
       "y: CategoryList\n",
       "forest,forest,forest,forest,forest\n",
       "Path: /home/cliff/NWPU-RESISC45/train;\n",
       "\n",
       "Valid: LabelList (5040 items)\n",
       "x: ImageList\n",
       "Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\n",
       "y: CategoryList\n",
       "lake,sea_ice,parking_lot,intersection,basketball_court\n",
       "Path: /home/cliff/NWPU-RESISC45/train;\n",
       "\n",
       "Test: None, model=GepNet(\n",
       "  (stem): Sequential(\n",
       "    (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (4): Conv2d(24, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (5): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (blocks): Sequential(\n",
       "    (0): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_2): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_2): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_3): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_3): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_2): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_2): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_3): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_3): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      (1): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_2): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_2): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_3): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_3): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_2): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_2): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_3): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_3): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_2): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_2): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_3): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_3): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      (1): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (7): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_2): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_2): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_3): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_3): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_2): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_2): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_3): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_3): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_2): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_2): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_3): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_3): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      (1): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (11): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_2): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_2): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_3): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_3): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_2): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_2): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_3): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_3): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (0): AdaptiveAvgPool2d(output_size=1)\n",
       "    (1): Flatten()\n",
       "    (2): Linear(in_features=384, out_features=45, bias=True)\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function error_rate at 0x7f8e406691e0>, <function accuracy at 0x7f8e4066df28>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/home/cliff/NWPU-RESISC45/train'), model_dir='/home/cliff/ResearchProjects/models/random_search/', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False), functools.partial(<class 'fastai.callbacks.mixup.MixUpCallback'>, alpha=0.4, stack_x=False, stack_y=True)], callbacks=[MixedPrecision\n",
       "learn: Learner(data=ImageDataBunch;\n",
       "\n",
       "Train: LabelList (20160 items)\n",
       "x: ImageList\n",
       "Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\n",
       "y: CategoryList\n",
       "forest,forest,forest,forest,forest\n",
       "Path: /home/cliff/NWPU-RESISC45/train;\n",
       "\n",
       "Valid: LabelList (5040 items)\n",
       "x: ImageList\n",
       "Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\n",
       "y: CategoryList\n",
       "lake,sea_ice,parking_lot,intersection,basketball_court\n",
       "Path: /home/cliff/NWPU-RESISC45/train;\n",
       "\n",
       "Test: None, model=GepNet(\n",
       "  (stem): Sequential(\n",
       "    (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (4): Conv2d(24, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (5): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (blocks): Sequential(\n",
       "    (0): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_2): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_2): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_3): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_3): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_2): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_2): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_3): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_3): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      (1): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_2): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_2): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_3): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_3): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_2): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_2): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_3): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_3): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_2): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_2): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_3): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_3): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      (1): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (7): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_2): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_2): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_3): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_3): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_2): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_2): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_3): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_3): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_2): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_2): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_3): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_3): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      (1): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (11): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_2): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_2): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_3): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_3): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_2): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_2): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_3): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv1x1_1): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_3): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (0): AdaptiveAvgPool2d(output_size=1)\n",
       "    (1): Flatten()\n",
       "    (2): Linear(in_features=384, out_features=45, bias=True)\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function error_rate at 0x7f8e406691e0>, <function accuracy at 0x7f8e4066df28>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/home/cliff/NWPU-RESISC45/train'), model_dir='/home/cliff/ResearchProjects/models/random_search/', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False), functools.partial(<class 'fastai.callbacks.mixup.MixUpCallback'>, alpha=0.4, stack_x=False, stack_y=True)], callbacks=[...], layer_groups=[Sequential(\n",
       "  (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (4): Conv2d(24, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (5): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (10): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "  (13): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): ReLU(inplace=True)\n",
       "  (15): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (16): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (17): ReLU(inplace=True)\n",
       "  (18): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (19): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (22): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (23): ReLU(inplace=True)\n",
       "  (24): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "  (25): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (26): ReLU(inplace=True)\n",
       "  (27): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (28): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "  (31): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (32): ReLU(inplace=True)\n",
       "  (33): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (34): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (35): ReLU(inplace=True)\n",
       "  (36): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "  (37): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (38): ReLU(inplace=True)\n",
       "  (39): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (40): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (41): ReLU(inplace=True)\n",
       "  (42): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (43): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (44): ReLU(inplace=True)\n",
       "  (45): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "  (46): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (47): ReLU(inplace=True)\n",
       "  (48): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (49): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (50): ReLU(inplace=True)\n",
       "  (51): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (52): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (53): ReLU(inplace=True)\n",
       "  (54): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (55): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (56): ReLU(inplace=True)\n",
       "  (57): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "  (58): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (59): ReLU(inplace=True)\n",
       "  (60): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (61): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (62): ReLU(inplace=True)\n",
       "  (63): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "  (64): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (65): ReLU(inplace=True)\n",
       "  (66): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (67): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (68): ReLU(inplace=True)\n",
       "  (69): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "  (70): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (71): ReLU(inplace=True)\n",
       "  (72): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (73): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (74): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (75): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (76): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (77): ReLU(inplace=True)\n",
       "  (78): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (79): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (80): ReLU(inplace=True)\n",
       "  (81): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "  (82): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (83): ReLU(inplace=True)\n",
       "  (84): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (85): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (86): ReLU(inplace=True)\n",
       "  (87): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (88): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (89): ReLU(inplace=True)\n",
       "  (90): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (91): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (92): ReLU(inplace=True)\n",
       "  (93): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "  (94): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (95): ReLU(inplace=True)\n",
       "  (96): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (97): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (98): ReLU(inplace=True)\n",
       "  (99): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "  (100): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (101): ReLU(inplace=True)\n",
       "  (102): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (103): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (104): ReLU(inplace=True)\n",
       "  (105): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "  (106): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (107): ReLU(inplace=True)\n",
       "  (108): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (109): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (110): ReLU(inplace=True)\n",
       "  (111): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (112): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (113): ReLU(inplace=True)\n",
       "  (114): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "  (115): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (116): ReLU(inplace=True)\n",
       "  (117): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (118): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (119): ReLU(inplace=True)\n",
       "  (120): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (121): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (122): ReLU(inplace=True)\n",
       "  (123): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (124): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (125): ReLU(inplace=True)\n",
       "  (126): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "  (127): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (128): ReLU(inplace=True)\n",
       "  (129): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (130): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (131): ReLU(inplace=True)\n",
       "  (132): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "  (133): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (134): ReLU(inplace=True)\n",
       "  (135): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (136): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (137): ReLU(inplace=True)\n",
       "  (138): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "  (139): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (140): ReLU(inplace=True)\n",
       "  (141): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (142): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (143): ReLU(inplace=True)\n",
       "  (144): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (145): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (146): ReLU(inplace=True)\n",
       "  (147): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "  (148): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (149): ReLU(inplace=True)\n",
       "  (150): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (151): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (152): ReLU(inplace=True)\n",
       "  (153): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (154): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (155): ReLU(inplace=True)\n",
       "  (156): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (157): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (158): ReLU(inplace=True)\n",
       "  (159): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "  (160): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (161): ReLU(inplace=True)\n",
       "  (162): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (163): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (164): ReLU(inplace=True)\n",
       "  (165): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "  (166): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (167): ReLU(inplace=True)\n",
       "  (168): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (169): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (170): ReLU(inplace=True)\n",
       "  (171): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "  (172): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (173): ReLU(inplace=True)\n",
       "  (174): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (175): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (176): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (177): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (178): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (179): ReLU(inplace=True)\n",
       "  (180): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (181): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (182): ReLU(inplace=True)\n",
       "  (183): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "  (184): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (185): ReLU(inplace=True)\n",
       "  (186): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (187): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (188): ReLU(inplace=True)\n",
       "  (189): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (190): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (191): ReLU(inplace=True)\n",
       "  (192): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (193): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (194): ReLU(inplace=True)\n",
       "  (195): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "  (196): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (197): ReLU(inplace=True)\n",
       "  (198): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (199): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (200): ReLU(inplace=True)\n",
       "  (201): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "  (202): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (203): ReLU(inplace=True)\n",
       "  (204): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (205): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (206): ReLU(inplace=True)\n",
       "  (207): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "  (208): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (209): ReLU(inplace=True)\n",
       "  (210): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (211): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (212): ReLU(inplace=True)\n",
       "  (213): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (214): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (215): ReLU(inplace=True)\n",
       "  (216): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "  (217): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (218): ReLU(inplace=True)\n",
       "  (219): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (220): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (221): ReLU(inplace=True)\n",
       "  (222): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (223): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (224): ReLU(inplace=True)\n",
       "  (225): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (226): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (227): ReLU(inplace=True)\n",
       "  (228): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "  (229): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (230): ReLU(inplace=True)\n",
       "  (231): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (232): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (233): ReLU(inplace=True)\n",
       "  (234): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "  (235): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (236): ReLU(inplace=True)\n",
       "  (237): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (238): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (239): ReLU(inplace=True)\n",
       "  (240): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "  (241): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (242): ReLU(inplace=True)\n",
       "  (243): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (244): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (245): ReLU(inplace=True)\n",
       "  (246): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (247): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (248): ReLU(inplace=True)\n",
       "  (249): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "  (250): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (251): ReLU(inplace=True)\n",
       "  (252): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (253): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (254): ReLU(inplace=True)\n",
       "  (255): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (256): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (257): ReLU(inplace=True)\n",
       "  (258): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (259): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (260): ReLU(inplace=True)\n",
       "  (261): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "  (262): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (263): ReLU(inplace=True)\n",
       "  (264): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (265): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (266): ReLU(inplace=True)\n",
       "  (267): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "  (268): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (269): ReLU(inplace=True)\n",
       "  (270): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (271): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (272): ReLU(inplace=True)\n",
       "  (273): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "  (274): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (275): ReLU(inplace=True)\n",
       "  (276): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (277): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (278): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (279): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (280): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (281): ReLU(inplace=True)\n",
       "  (282): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (283): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (284): ReLU(inplace=True)\n",
       "  (285): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "  (286): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (287): ReLU(inplace=True)\n",
       "  (288): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (289): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (290): ReLU(inplace=True)\n",
       "  (291): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (292): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (293): ReLU(inplace=True)\n",
       "  (294): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (295): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (296): ReLU(inplace=True)\n",
       "  (297): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "  (298): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (299): ReLU(inplace=True)\n",
       "  (300): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (301): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (302): ReLU(inplace=True)\n",
       "  (303): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "  (304): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (305): ReLU(inplace=True)\n",
       "  (306): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (307): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (308): ReLU(inplace=True)\n",
       "  (309): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "  (310): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (311): ReLU(inplace=True)\n",
       "  (312): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (313): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (314): ReLU(inplace=True)\n",
       "  (315): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (316): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (317): ReLU(inplace=True)\n",
       "  (318): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "  (319): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (320): ReLU(inplace=True)\n",
       "  (321): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (322): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (323): ReLU(inplace=True)\n",
       "  (324): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (325): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (326): ReLU(inplace=True)\n",
       "  (327): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (328): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (329): ReLU(inplace=True)\n",
       "  (330): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "  (331): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (332): ReLU(inplace=True)\n",
       "  (333): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (334): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (335): ReLU(inplace=True)\n",
       "  (336): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "  (337): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (338): ReLU(inplace=True)\n",
       "  (339): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (340): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (341): ReLU(inplace=True)\n",
       "  (342): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "  (343): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (344): ReLU(inplace=True)\n",
       "  (345): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (346): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (347): AdaptiveAvgPool2d(output_size=1)\n",
       "  (348): Flatten()\n",
       "  (349): Linear(in_features=384, out_features=45, bias=True)\n",
       ")], add_time=True, silent=False)\n",
       "loss_scale: 65536\n",
       "max_noskip: 1000\n",
       "dynamic: True\n",
       "clip: None\n",
       "flat_master: False\n",
       "max_scale: 16777216\n",
       "loss_fp32: True], layer_groups=[Sequential(\n",
       "  (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (4): Conv2d(24, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (5): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (10): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "  (13): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): ReLU(inplace=True)\n",
       "  (15): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (16): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (17): ReLU(inplace=True)\n",
       "  (18): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (19): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (22): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (23): ReLU(inplace=True)\n",
       "  (24): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "  (25): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (26): ReLU(inplace=True)\n",
       "  (27): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (28): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "  (31): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (32): ReLU(inplace=True)\n",
       "  (33): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (34): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (35): ReLU(inplace=True)\n",
       "  (36): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "  (37): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (38): ReLU(inplace=True)\n",
       "  (39): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (40): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (41): ReLU(inplace=True)\n",
       "  (42): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (43): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (44): ReLU(inplace=True)\n",
       "  (45): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "  (46): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (47): ReLU(inplace=True)\n",
       "  (48): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (49): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (50): ReLU(inplace=True)\n",
       "  (51): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (52): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (53): ReLU(inplace=True)\n",
       "  (54): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (55): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (56): ReLU(inplace=True)\n",
       "  (57): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "  (58): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (59): ReLU(inplace=True)\n",
       "  (60): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (61): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (62): ReLU(inplace=True)\n",
       "  (63): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "  (64): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (65): ReLU(inplace=True)\n",
       "  (66): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (67): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (68): ReLU(inplace=True)\n",
       "  (69): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "  (70): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (71): ReLU(inplace=True)\n",
       "  (72): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (73): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (74): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (75): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (76): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (77): ReLU(inplace=True)\n",
       "  (78): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (79): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (80): ReLU(inplace=True)\n",
       "  (81): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "  (82): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (83): ReLU(inplace=True)\n",
       "  (84): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (85): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (86): ReLU(inplace=True)\n",
       "  (87): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (88): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (89): ReLU(inplace=True)\n",
       "  (90): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (91): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (92): ReLU(inplace=True)\n",
       "  (93): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "  (94): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (95): ReLU(inplace=True)\n",
       "  (96): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (97): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (98): ReLU(inplace=True)\n",
       "  (99): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "  (100): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (101): ReLU(inplace=True)\n",
       "  (102): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (103): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (104): ReLU(inplace=True)\n",
       "  (105): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "  (106): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (107): ReLU(inplace=True)\n",
       "  (108): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (109): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (110): ReLU(inplace=True)\n",
       "  (111): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (112): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (113): ReLU(inplace=True)\n",
       "  (114): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "  (115): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (116): ReLU(inplace=True)\n",
       "  (117): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (118): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (119): ReLU(inplace=True)\n",
       "  (120): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (121): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (122): ReLU(inplace=True)\n",
       "  (123): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (124): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (125): ReLU(inplace=True)\n",
       "  (126): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "  (127): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (128): ReLU(inplace=True)\n",
       "  (129): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (130): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (131): ReLU(inplace=True)\n",
       "  (132): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "  (133): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (134): ReLU(inplace=True)\n",
       "  (135): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (136): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (137): ReLU(inplace=True)\n",
       "  (138): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "  (139): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (140): ReLU(inplace=True)\n",
       "  (141): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (142): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (143): ReLU(inplace=True)\n",
       "  (144): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (145): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (146): ReLU(inplace=True)\n",
       "  (147): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "  (148): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (149): ReLU(inplace=True)\n",
       "  (150): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (151): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (152): ReLU(inplace=True)\n",
       "  (153): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (154): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (155): ReLU(inplace=True)\n",
       "  (156): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (157): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (158): ReLU(inplace=True)\n",
       "  (159): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "  (160): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (161): ReLU(inplace=True)\n",
       "  (162): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (163): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (164): ReLU(inplace=True)\n",
       "  (165): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "  (166): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (167): ReLU(inplace=True)\n",
       "  (168): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (169): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (170): ReLU(inplace=True)\n",
       "  (171): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "  (172): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (173): ReLU(inplace=True)\n",
       "  (174): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (175): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (176): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (177): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (178): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (179): ReLU(inplace=True)\n",
       "  (180): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (181): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (182): ReLU(inplace=True)\n",
       "  (183): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "  (184): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (185): ReLU(inplace=True)\n",
       "  (186): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (187): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (188): ReLU(inplace=True)\n",
       "  (189): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (190): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (191): ReLU(inplace=True)\n",
       "  (192): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (193): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (194): ReLU(inplace=True)\n",
       "  (195): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "  (196): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (197): ReLU(inplace=True)\n",
       "  (198): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (199): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (200): ReLU(inplace=True)\n",
       "  (201): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "  (202): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (203): ReLU(inplace=True)\n",
       "  (204): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (205): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (206): ReLU(inplace=True)\n",
       "  (207): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "  (208): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (209): ReLU(inplace=True)\n",
       "  (210): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (211): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (212): ReLU(inplace=True)\n",
       "  (213): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (214): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (215): ReLU(inplace=True)\n",
       "  (216): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "  (217): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (218): ReLU(inplace=True)\n",
       "  (219): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (220): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (221): ReLU(inplace=True)\n",
       "  (222): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (223): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (224): ReLU(inplace=True)\n",
       "  (225): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (226): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (227): ReLU(inplace=True)\n",
       "  (228): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "  (229): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (230): ReLU(inplace=True)\n",
       "  (231): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (232): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (233): ReLU(inplace=True)\n",
       "  (234): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "  (235): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (236): ReLU(inplace=True)\n",
       "  (237): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (238): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (239): ReLU(inplace=True)\n",
       "  (240): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "  (241): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (242): ReLU(inplace=True)\n",
       "  (243): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (244): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (245): ReLU(inplace=True)\n",
       "  (246): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (247): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (248): ReLU(inplace=True)\n",
       "  (249): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "  (250): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (251): ReLU(inplace=True)\n",
       "  (252): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (253): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (254): ReLU(inplace=True)\n",
       "  (255): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (256): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (257): ReLU(inplace=True)\n",
       "  (258): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (259): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (260): ReLU(inplace=True)\n",
       "  (261): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "  (262): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (263): ReLU(inplace=True)\n",
       "  (264): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (265): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (266): ReLU(inplace=True)\n",
       "  (267): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "  (268): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (269): ReLU(inplace=True)\n",
       "  (270): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (271): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (272): ReLU(inplace=True)\n",
       "  (273): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "  (274): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (275): ReLU(inplace=True)\n",
       "  (276): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (277): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (278): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (279): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (280): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (281): ReLU(inplace=True)\n",
       "  (282): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (283): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (284): ReLU(inplace=True)\n",
       "  (285): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "  (286): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (287): ReLU(inplace=True)\n",
       "  (288): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (289): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (290): ReLU(inplace=True)\n",
       "  (291): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (292): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (293): ReLU(inplace=True)\n",
       "  (294): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (295): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (296): ReLU(inplace=True)\n",
       "  (297): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "  (298): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (299): ReLU(inplace=True)\n",
       "  (300): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (301): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (302): ReLU(inplace=True)\n",
       "  (303): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "  (304): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (305): ReLU(inplace=True)\n",
       "  (306): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (307): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (308): ReLU(inplace=True)\n",
       "  (309): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "  (310): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (311): ReLU(inplace=True)\n",
       "  (312): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (313): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (314): ReLU(inplace=True)\n",
       "  (315): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (316): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (317): ReLU(inplace=True)\n",
       "  (318): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "  (319): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (320): ReLU(inplace=True)\n",
       "  (321): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (322): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (323): ReLU(inplace=True)\n",
       "  (324): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (325): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (326): ReLU(inplace=True)\n",
       "  (327): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (328): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (329): ReLU(inplace=True)\n",
       "  (330): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "  (331): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (332): ReLU(inplace=True)\n",
       "  (333): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (334): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (335): ReLU(inplace=True)\n",
       "  (336): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "  (337): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (338): ReLU(inplace=True)\n",
       "  (339): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (340): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (341): ReLU(inplace=True)\n",
       "  (342): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "  (343): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (344): ReLU(inplace=True)\n",
       "  (345): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (346): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (347): AdaptiveAvgPool2d(output_size=1)\n",
       "  (348): Flatten()\n",
       "  (349): Linear(in_features=384, out_features=45, bias=True)\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = '/home/cliff/ResearchProjects/models/random_search/'\n",
    "learn = Learner(data, net, metrics=[error_rate, accuracy], model_dir=model_dir).mixup()\n",
    "learn.to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find(end_lr=100)\n",
    "\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='260' class='' max='600', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      43.33% [260/600 5:40:37<7:25:26]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.696464</td>\n",
       "      <td>2.094940</td>\n",
       "      <td>0.580754</td>\n",
       "      <td>0.419246</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.361426</td>\n",
       "      <td>1.684465</td>\n",
       "      <td>0.489286</td>\n",
       "      <td>0.510714</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.183659</td>\n",
       "      <td>1.520183</td>\n",
       "      <td>0.436706</td>\n",
       "      <td>0.563294</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.028662</td>\n",
       "      <td>1.294214</td>\n",
       "      <td>0.371230</td>\n",
       "      <td>0.628770</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.910251</td>\n",
       "      <td>1.190481</td>\n",
       "      <td>0.349802</td>\n",
       "      <td>0.650198</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.814554</td>\n",
       "      <td>1.052437</td>\n",
       "      <td>0.302183</td>\n",
       "      <td>0.697817</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.724163</td>\n",
       "      <td>0.951455</td>\n",
       "      <td>0.278968</td>\n",
       "      <td>0.721032</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.681807</td>\n",
       "      <td>0.924171</td>\n",
       "      <td>0.266468</td>\n",
       "      <td>0.733532</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.622994</td>\n",
       "      <td>0.846240</td>\n",
       "      <td>0.235516</td>\n",
       "      <td>0.764484</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.571323</td>\n",
       "      <td>0.791677</td>\n",
       "      <td>0.224802</td>\n",
       "      <td>0.775198</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.539456</td>\n",
       "      <td>0.819726</td>\n",
       "      <td>0.237698</td>\n",
       "      <td>0.762302</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.491623</td>\n",
       "      <td>0.691772</td>\n",
       "      <td>0.195437</td>\n",
       "      <td>0.804563</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.453353</td>\n",
       "      <td>0.708196</td>\n",
       "      <td>0.195635</td>\n",
       "      <td>0.804365</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.418295</td>\n",
       "      <td>0.636878</td>\n",
       "      <td>0.172222</td>\n",
       "      <td>0.827778</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.400248</td>\n",
       "      <td>0.600940</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.380882</td>\n",
       "      <td>0.606813</td>\n",
       "      <td>0.168254</td>\n",
       "      <td>0.831746</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.365887</td>\n",
       "      <td>0.604531</td>\n",
       "      <td>0.170238</td>\n",
       "      <td>0.829762</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.342516</td>\n",
       "      <td>0.590046</td>\n",
       "      <td>0.164881</td>\n",
       "      <td>0.835119</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.324267</td>\n",
       "      <td>0.652253</td>\n",
       "      <td>0.179960</td>\n",
       "      <td>0.820040</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.311756</td>\n",
       "      <td>0.536915</td>\n",
       "      <td>0.149008</td>\n",
       "      <td>0.850992</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.282888</td>\n",
       "      <td>0.570413</td>\n",
       "      <td>0.160119</td>\n",
       "      <td>0.839881</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.268807</td>\n",
       "      <td>0.561264</td>\n",
       "      <td>0.144246</td>\n",
       "      <td>0.855754</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.279838</td>\n",
       "      <td>0.541571</td>\n",
       "      <td>0.153968</td>\n",
       "      <td>0.846032</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.247269</td>\n",
       "      <td>0.519580</td>\n",
       "      <td>0.152778</td>\n",
       "      <td>0.847222</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.257590</td>\n",
       "      <td>0.534533</td>\n",
       "      <td>0.144444</td>\n",
       "      <td>0.855556</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.248114</td>\n",
       "      <td>0.482156</td>\n",
       "      <td>0.132738</td>\n",
       "      <td>0.867262</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.229002</td>\n",
       "      <td>0.510801</td>\n",
       "      <td>0.131151</td>\n",
       "      <td>0.868849</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.211520</td>\n",
       "      <td>0.528442</td>\n",
       "      <td>0.130952</td>\n",
       "      <td>0.869048</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.217727</td>\n",
       "      <td>0.483085</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.207996</td>\n",
       "      <td>0.457627</td>\n",
       "      <td>0.120040</td>\n",
       "      <td>0.879960</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.201626</td>\n",
       "      <td>0.511050</td>\n",
       "      <td>0.144048</td>\n",
       "      <td>0.855952</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.193540</td>\n",
       "      <td>0.476321</td>\n",
       "      <td>0.122817</td>\n",
       "      <td>0.877183</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.182397</td>\n",
       "      <td>0.433600</td>\n",
       "      <td>0.115873</td>\n",
       "      <td>0.884127</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.195938</td>\n",
       "      <td>0.459938</td>\n",
       "      <td>0.126786</td>\n",
       "      <td>0.873214</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.175368</td>\n",
       "      <td>0.509382</td>\n",
       "      <td>0.143651</td>\n",
       "      <td>0.856349</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.151058</td>\n",
       "      <td>0.429876</td>\n",
       "      <td>0.110913</td>\n",
       "      <td>0.889087</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.147461</td>\n",
       "      <td>0.441952</td>\n",
       "      <td>0.118254</td>\n",
       "      <td>0.881746</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.155981</td>\n",
       "      <td>0.475531</td>\n",
       "      <td>0.125397</td>\n",
       "      <td>0.874603</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.153135</td>\n",
       "      <td>0.476637</td>\n",
       "      <td>0.130357</td>\n",
       "      <td>0.869643</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.137845</td>\n",
       "      <td>0.453738</td>\n",
       "      <td>0.127381</td>\n",
       "      <td>0.872619</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.136976</td>\n",
       "      <td>0.446563</td>\n",
       "      <td>0.120635</td>\n",
       "      <td>0.879365</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.140013</td>\n",
       "      <td>0.406159</td>\n",
       "      <td>0.115873</td>\n",
       "      <td>0.884127</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.117956</td>\n",
       "      <td>0.404810</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.130412</td>\n",
       "      <td>0.456865</td>\n",
       "      <td>0.121032</td>\n",
       "      <td>0.878968</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.099185</td>\n",
       "      <td>0.454550</td>\n",
       "      <td>0.128373</td>\n",
       "      <td>0.871627</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.120293</td>\n",
       "      <td>0.441498</td>\n",
       "      <td>0.118651</td>\n",
       "      <td>0.881349</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.090750</td>\n",
       "      <td>0.381452</td>\n",
       "      <td>0.103373</td>\n",
       "      <td>0.896627</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.106965</td>\n",
       "      <td>0.448440</td>\n",
       "      <td>0.112302</td>\n",
       "      <td>0.887698</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.107297</td>\n",
       "      <td>0.464309</td>\n",
       "      <td>0.123413</td>\n",
       "      <td>0.876587</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.092806</td>\n",
       "      <td>0.499642</td>\n",
       "      <td>0.130754</td>\n",
       "      <td>0.869246</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.091763</td>\n",
       "      <td>0.373696</td>\n",
       "      <td>0.103770</td>\n",
       "      <td>0.896230</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.083949</td>\n",
       "      <td>0.401167</td>\n",
       "      <td>0.108135</td>\n",
       "      <td>0.891865</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.071042</td>\n",
       "      <td>0.371934</td>\n",
       "      <td>0.089087</td>\n",
       "      <td>0.910913</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.057342</td>\n",
       "      <td>0.389101</td>\n",
       "      <td>0.101389</td>\n",
       "      <td>0.898611</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.071521</td>\n",
       "      <td>0.395145</td>\n",
       "      <td>0.105556</td>\n",
       "      <td>0.894444</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.045443</td>\n",
       "      <td>0.447527</td>\n",
       "      <td>0.124802</td>\n",
       "      <td>0.875198</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.046153</td>\n",
       "      <td>0.448489</td>\n",
       "      <td>0.117659</td>\n",
       "      <td>0.882341</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.064332</td>\n",
       "      <td>0.411669</td>\n",
       "      <td>0.110516</td>\n",
       "      <td>0.889484</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.046488</td>\n",
       "      <td>0.386795</td>\n",
       "      <td>0.104563</td>\n",
       "      <td>0.895437</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.042583</td>\n",
       "      <td>0.379990</td>\n",
       "      <td>0.100595</td>\n",
       "      <td>0.899405</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.039968</td>\n",
       "      <td>0.418383</td>\n",
       "      <td>0.110119</td>\n",
       "      <td>0.889881</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.039063</td>\n",
       "      <td>0.360369</td>\n",
       "      <td>0.090476</td>\n",
       "      <td>0.909524</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.037274</td>\n",
       "      <td>0.369661</td>\n",
       "      <td>0.088492</td>\n",
       "      <td>0.911508</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.027060</td>\n",
       "      <td>0.364686</td>\n",
       "      <td>0.093651</td>\n",
       "      <td>0.906349</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.013907</td>\n",
       "      <td>0.332011</td>\n",
       "      <td>0.084524</td>\n",
       "      <td>0.915476</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.009365</td>\n",
       "      <td>0.353477</td>\n",
       "      <td>0.084921</td>\n",
       "      <td>0.915079</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.010803</td>\n",
       "      <td>0.378758</td>\n",
       "      <td>0.108532</td>\n",
       "      <td>0.891468</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.005201</td>\n",
       "      <td>0.377288</td>\n",
       "      <td>0.100992</td>\n",
       "      <td>0.899008</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.010824</td>\n",
       "      <td>0.412076</td>\n",
       "      <td>0.106349</td>\n",
       "      <td>0.893651</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.016660</td>\n",
       "      <td>0.368483</td>\n",
       "      <td>0.089087</td>\n",
       "      <td>0.910913</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.004302</td>\n",
       "      <td>0.353397</td>\n",
       "      <td>0.092857</td>\n",
       "      <td>0.907143</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.989419</td>\n",
       "      <td>0.383768</td>\n",
       "      <td>0.092460</td>\n",
       "      <td>0.907540</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.988318</td>\n",
       "      <td>0.371442</td>\n",
       "      <td>0.099603</td>\n",
       "      <td>0.900397</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.996233</td>\n",
       "      <td>0.327275</td>\n",
       "      <td>0.084127</td>\n",
       "      <td>0.915873</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.983155</td>\n",
       "      <td>0.424325</td>\n",
       "      <td>0.115278</td>\n",
       "      <td>0.884722</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.981540</td>\n",
       "      <td>0.370870</td>\n",
       "      <td>0.092460</td>\n",
       "      <td>0.907540</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.976435</td>\n",
       "      <td>0.351532</td>\n",
       "      <td>0.088690</td>\n",
       "      <td>0.911310</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.995931</td>\n",
       "      <td>0.421509</td>\n",
       "      <td>0.103571</td>\n",
       "      <td>0.896429</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.966148</td>\n",
       "      <td>0.333358</td>\n",
       "      <td>0.088690</td>\n",
       "      <td>0.911310</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.968675</td>\n",
       "      <td>0.334793</td>\n",
       "      <td>0.084127</td>\n",
       "      <td>0.915873</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.982424</td>\n",
       "      <td>0.320911</td>\n",
       "      <td>0.086508</td>\n",
       "      <td>0.913492</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.968042</td>\n",
       "      <td>0.316103</td>\n",
       "      <td>0.083929</td>\n",
       "      <td>0.916071</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.982693</td>\n",
       "      <td>0.319993</td>\n",
       "      <td>0.078770</td>\n",
       "      <td>0.921230</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.965858</td>\n",
       "      <td>0.333466</td>\n",
       "      <td>0.086706</td>\n",
       "      <td>0.913294</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.964929</td>\n",
       "      <td>0.370628</td>\n",
       "      <td>0.093254</td>\n",
       "      <td>0.906746</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.964402</td>\n",
       "      <td>0.321444</td>\n",
       "      <td>0.080159</td>\n",
       "      <td>0.919841</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.953451</td>\n",
       "      <td>0.347545</td>\n",
       "      <td>0.091865</td>\n",
       "      <td>0.908135</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.954067</td>\n",
       "      <td>0.381539</td>\n",
       "      <td>0.096429</td>\n",
       "      <td>0.903571</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.951128</td>\n",
       "      <td>0.300761</td>\n",
       "      <td>0.077381</td>\n",
       "      <td>0.922619</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.946011</td>\n",
       "      <td>0.325013</td>\n",
       "      <td>0.078571</td>\n",
       "      <td>0.921429</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.921470</td>\n",
       "      <td>0.371797</td>\n",
       "      <td>0.090675</td>\n",
       "      <td>0.909325</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.942020</td>\n",
       "      <td>0.355955</td>\n",
       "      <td>0.091071</td>\n",
       "      <td>0.908929</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.940147</td>\n",
       "      <td>0.326006</td>\n",
       "      <td>0.083730</td>\n",
       "      <td>0.916270</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.922193</td>\n",
       "      <td>0.328160</td>\n",
       "      <td>0.083730</td>\n",
       "      <td>0.916270</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.922560</td>\n",
       "      <td>0.334284</td>\n",
       "      <td>0.085119</td>\n",
       "      <td>0.914881</td>\n",
       "      <td>01:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.940747</td>\n",
       "      <td>0.350510</td>\n",
       "      <td>0.088492</td>\n",
       "      <td>0.911508</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.941576</td>\n",
       "      <td>0.314331</td>\n",
       "      <td>0.074802</td>\n",
       "      <td>0.925198</td>\n",
       "      <td>01:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.930973</td>\n",
       "      <td>0.343203</td>\n",
       "      <td>0.079762</td>\n",
       "      <td>0.920238</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.924700</td>\n",
       "      <td>0.315529</td>\n",
       "      <td>0.076587</td>\n",
       "      <td>0.923413</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.926430</td>\n",
       "      <td>0.348414</td>\n",
       "      <td>0.091071</td>\n",
       "      <td>0.908929</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.922370</td>\n",
       "      <td>0.342794</td>\n",
       "      <td>0.085119</td>\n",
       "      <td>0.914881</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.924900</td>\n",
       "      <td>0.314896</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.915665</td>\n",
       "      <td>0.328049</td>\n",
       "      <td>0.077579</td>\n",
       "      <td>0.922421</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.915443</td>\n",
       "      <td>0.339345</td>\n",
       "      <td>0.081746</td>\n",
       "      <td>0.918254</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.917320</td>\n",
       "      <td>0.293682</td>\n",
       "      <td>0.078175</td>\n",
       "      <td>0.921825</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.919483</td>\n",
       "      <td>0.394500</td>\n",
       "      <td>0.099008</td>\n",
       "      <td>0.900992</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.915762</td>\n",
       "      <td>0.302174</td>\n",
       "      <td>0.073810</td>\n",
       "      <td>0.926190</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.910081</td>\n",
       "      <td>0.317103</td>\n",
       "      <td>0.079762</td>\n",
       "      <td>0.920238</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.905487</td>\n",
       "      <td>0.338976</td>\n",
       "      <td>0.082341</td>\n",
       "      <td>0.917659</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.907409</td>\n",
       "      <td>0.276995</td>\n",
       "      <td>0.069246</td>\n",
       "      <td>0.930754</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.904858</td>\n",
       "      <td>0.308725</td>\n",
       "      <td>0.075595</td>\n",
       "      <td>0.924405</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.900743</td>\n",
       "      <td>0.282708</td>\n",
       "      <td>0.073611</td>\n",
       "      <td>0.926389</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.905516</td>\n",
       "      <td>0.302338</td>\n",
       "      <td>0.076984</td>\n",
       "      <td>0.923016</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.901966</td>\n",
       "      <td>0.309276</td>\n",
       "      <td>0.077778</td>\n",
       "      <td>0.922222</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.908991</td>\n",
       "      <td>0.278382</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.890239</td>\n",
       "      <td>0.333286</td>\n",
       "      <td>0.080556</td>\n",
       "      <td>0.919444</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.900090</td>\n",
       "      <td>0.332984</td>\n",
       "      <td>0.081151</td>\n",
       "      <td>0.918849</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.895232</td>\n",
       "      <td>0.341956</td>\n",
       "      <td>0.084325</td>\n",
       "      <td>0.915675</td>\n",
       "      <td>01:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.900891</td>\n",
       "      <td>0.301010</td>\n",
       "      <td>0.070238</td>\n",
       "      <td>0.929762</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.891935</td>\n",
       "      <td>0.267245</td>\n",
       "      <td>0.066270</td>\n",
       "      <td>0.933730</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.890140</td>\n",
       "      <td>0.315733</td>\n",
       "      <td>0.077183</td>\n",
       "      <td>0.922817</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.897213</td>\n",
       "      <td>0.289740</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.878480</td>\n",
       "      <td>0.320392</td>\n",
       "      <td>0.078175</td>\n",
       "      <td>0.921825</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.864857</td>\n",
       "      <td>0.345099</td>\n",
       "      <td>0.076786</td>\n",
       "      <td>0.923214</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.889590</td>\n",
       "      <td>0.331302</td>\n",
       "      <td>0.079167</td>\n",
       "      <td>0.920833</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.869646</td>\n",
       "      <td>0.311177</td>\n",
       "      <td>0.076389</td>\n",
       "      <td>0.923611</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.876628</td>\n",
       "      <td>0.278935</td>\n",
       "      <td>0.064683</td>\n",
       "      <td>0.935317</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.875843</td>\n",
       "      <td>0.331732</td>\n",
       "      <td>0.079167</td>\n",
       "      <td>0.920833</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.872652</td>\n",
       "      <td>0.387575</td>\n",
       "      <td>0.088690</td>\n",
       "      <td>0.911310</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.883514</td>\n",
       "      <td>0.283929</td>\n",
       "      <td>0.069048</td>\n",
       "      <td>0.930952</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.876341</td>\n",
       "      <td>0.295921</td>\n",
       "      <td>0.067857</td>\n",
       "      <td>0.932143</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.873553</td>\n",
       "      <td>0.343395</td>\n",
       "      <td>0.082143</td>\n",
       "      <td>0.917857</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.878419</td>\n",
       "      <td>0.290709</td>\n",
       "      <td>0.069643</td>\n",
       "      <td>0.930357</td>\n",
       "      <td>01:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.864666</td>\n",
       "      <td>0.306041</td>\n",
       "      <td>0.074802</td>\n",
       "      <td>0.925198</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.879846</td>\n",
       "      <td>0.286448</td>\n",
       "      <td>0.072222</td>\n",
       "      <td>0.927778</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.868111</td>\n",
       "      <td>0.305735</td>\n",
       "      <td>0.078373</td>\n",
       "      <td>0.921627</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.873194</td>\n",
       "      <td>0.313889</td>\n",
       "      <td>0.072421</td>\n",
       "      <td>0.927579</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.871569</td>\n",
       "      <td>0.357318</td>\n",
       "      <td>0.082540</td>\n",
       "      <td>0.917460</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.867288</td>\n",
       "      <td>0.281581</td>\n",
       "      <td>0.071032</td>\n",
       "      <td>0.928968</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.864203</td>\n",
       "      <td>0.280238</td>\n",
       "      <td>0.067460</td>\n",
       "      <td>0.932540</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.855557</td>\n",
       "      <td>0.296446</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.854894</td>\n",
       "      <td>0.285006</td>\n",
       "      <td>0.062103</td>\n",
       "      <td>0.937897</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.860226</td>\n",
       "      <td>0.290077</td>\n",
       "      <td>0.071230</td>\n",
       "      <td>0.928770</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.862529</td>\n",
       "      <td>0.303437</td>\n",
       "      <td>0.073214</td>\n",
       "      <td>0.926786</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.855004</td>\n",
       "      <td>0.254384</td>\n",
       "      <td>0.064286</td>\n",
       "      <td>0.935714</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.853960</td>\n",
       "      <td>0.282973</td>\n",
       "      <td>0.066071</td>\n",
       "      <td>0.933929</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.856188</td>\n",
       "      <td>0.266488</td>\n",
       "      <td>0.064881</td>\n",
       "      <td>0.935119</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.856740</td>\n",
       "      <td>0.375774</td>\n",
       "      <td>0.092659</td>\n",
       "      <td>0.907341</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.856362</td>\n",
       "      <td>0.286146</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.851427</td>\n",
       "      <td>0.290786</td>\n",
       "      <td>0.068254</td>\n",
       "      <td>0.931746</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.853459</td>\n",
       "      <td>0.280757</td>\n",
       "      <td>0.070635</td>\n",
       "      <td>0.929365</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.844275</td>\n",
       "      <td>0.285126</td>\n",
       "      <td>0.069048</td>\n",
       "      <td>0.930952</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.856861</td>\n",
       "      <td>0.280764</td>\n",
       "      <td>0.069643</td>\n",
       "      <td>0.930357</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.856172</td>\n",
       "      <td>0.287182</td>\n",
       "      <td>0.069841</td>\n",
       "      <td>0.930159</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.844492</td>\n",
       "      <td>0.263198</td>\n",
       "      <td>0.064087</td>\n",
       "      <td>0.935913</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.840466</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.070635</td>\n",
       "      <td>0.929365</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.835585</td>\n",
       "      <td>0.289406</td>\n",
       "      <td>0.066468</td>\n",
       "      <td>0.933532</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.842950</td>\n",
       "      <td>0.298983</td>\n",
       "      <td>0.074008</td>\n",
       "      <td>0.925992</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.837807</td>\n",
       "      <td>0.312889</td>\n",
       "      <td>0.072222</td>\n",
       "      <td>0.927778</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.837134</td>\n",
       "      <td>0.304909</td>\n",
       "      <td>0.072024</td>\n",
       "      <td>0.927976</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.847257</td>\n",
       "      <td>0.270310</td>\n",
       "      <td>0.064286</td>\n",
       "      <td>0.935714</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.824154</td>\n",
       "      <td>0.278176</td>\n",
       "      <td>0.064683</td>\n",
       "      <td>0.935317</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.835328</td>\n",
       "      <td>0.269850</td>\n",
       "      <td>0.059921</td>\n",
       "      <td>0.940079</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.842798</td>\n",
       "      <td>0.255781</td>\n",
       "      <td>0.060119</td>\n",
       "      <td>0.939881</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.835699</td>\n",
       "      <td>0.268312</td>\n",
       "      <td>0.058532</td>\n",
       "      <td>0.941468</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.837311</td>\n",
       "      <td>0.266068</td>\n",
       "      <td>0.065278</td>\n",
       "      <td>0.934722</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.829109</td>\n",
       "      <td>0.286042</td>\n",
       "      <td>0.066270</td>\n",
       "      <td>0.933730</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.838112</td>\n",
       "      <td>0.322895</td>\n",
       "      <td>0.079960</td>\n",
       "      <td>0.920040</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.842329</td>\n",
       "      <td>0.249294</td>\n",
       "      <td>0.059127</td>\n",
       "      <td>0.940873</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.834317</td>\n",
       "      <td>0.273013</td>\n",
       "      <td>0.062897</td>\n",
       "      <td>0.937103</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.821758</td>\n",
       "      <td>0.282992</td>\n",
       "      <td>0.069246</td>\n",
       "      <td>0.930754</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.817997</td>\n",
       "      <td>0.267661</td>\n",
       "      <td>0.066071</td>\n",
       "      <td>0.933929</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.828559</td>\n",
       "      <td>0.283497</td>\n",
       "      <td>0.063294</td>\n",
       "      <td>0.936706</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.820951</td>\n",
       "      <td>0.283705</td>\n",
       "      <td>0.065873</td>\n",
       "      <td>0.934127</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.814221</td>\n",
       "      <td>0.285921</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.823189</td>\n",
       "      <td>0.273466</td>\n",
       "      <td>0.063690</td>\n",
       "      <td>0.936310</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.821299</td>\n",
       "      <td>0.264632</td>\n",
       "      <td>0.064484</td>\n",
       "      <td>0.935516</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.829965</td>\n",
       "      <td>0.235047</td>\n",
       "      <td>0.054960</td>\n",
       "      <td>0.945040</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.810192</td>\n",
       "      <td>0.269073</td>\n",
       "      <td>0.060317</td>\n",
       "      <td>0.939683</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.819330</td>\n",
       "      <td>0.270051</td>\n",
       "      <td>0.062897</td>\n",
       "      <td>0.937103</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.807729</td>\n",
       "      <td>0.315814</td>\n",
       "      <td>0.073810</td>\n",
       "      <td>0.926190</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.821934</td>\n",
       "      <td>0.282058</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.807781</td>\n",
       "      <td>0.269795</td>\n",
       "      <td>0.061706</td>\n",
       "      <td>0.938294</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.808104</td>\n",
       "      <td>0.257617</td>\n",
       "      <td>0.060317</td>\n",
       "      <td>0.939683</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.805407</td>\n",
       "      <td>0.271188</td>\n",
       "      <td>0.065873</td>\n",
       "      <td>0.934127</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.807467</td>\n",
       "      <td>0.279709</td>\n",
       "      <td>0.062103</td>\n",
       "      <td>0.937897</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.811230</td>\n",
       "      <td>0.271122</td>\n",
       "      <td>0.064087</td>\n",
       "      <td>0.935913</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.795285</td>\n",
       "      <td>0.309240</td>\n",
       "      <td>0.063690</td>\n",
       "      <td>0.936310</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.804983</td>\n",
       "      <td>0.281465</td>\n",
       "      <td>0.065278</td>\n",
       "      <td>0.934722</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.810350</td>\n",
       "      <td>0.277413</td>\n",
       "      <td>0.065675</td>\n",
       "      <td>0.934325</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.801543</td>\n",
       "      <td>0.259438</td>\n",
       "      <td>0.059325</td>\n",
       "      <td>0.940675</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.799862</td>\n",
       "      <td>0.265135</td>\n",
       "      <td>0.062302</td>\n",
       "      <td>0.937698</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.794520</td>\n",
       "      <td>0.252369</td>\n",
       "      <td>0.053968</td>\n",
       "      <td>0.946032</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.804750</td>\n",
       "      <td>0.269054</td>\n",
       "      <td>0.061905</td>\n",
       "      <td>0.938095</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.790201</td>\n",
       "      <td>0.263897</td>\n",
       "      <td>0.058532</td>\n",
       "      <td>0.941468</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.805281</td>\n",
       "      <td>0.262114</td>\n",
       "      <td>0.060119</td>\n",
       "      <td>0.939881</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.788211</td>\n",
       "      <td>0.292498</td>\n",
       "      <td>0.064881</td>\n",
       "      <td>0.935119</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.789256</td>\n",
       "      <td>0.257264</td>\n",
       "      <td>0.062103</td>\n",
       "      <td>0.937897</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.795184</td>\n",
       "      <td>0.231825</td>\n",
       "      <td>0.054365</td>\n",
       "      <td>0.945635</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.790708</td>\n",
       "      <td>0.237447</td>\n",
       "      <td>0.054960</td>\n",
       "      <td>0.945040</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.785363</td>\n",
       "      <td>0.236897</td>\n",
       "      <td>0.053175</td>\n",
       "      <td>0.946825</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>0.779934</td>\n",
       "      <td>0.262084</td>\n",
       "      <td>0.061706</td>\n",
       "      <td>0.938294</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>0.795734</td>\n",
       "      <td>0.271665</td>\n",
       "      <td>0.062698</td>\n",
       "      <td>0.937302</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>0.785413</td>\n",
       "      <td>0.240614</td>\n",
       "      <td>0.054563</td>\n",
       "      <td>0.945437</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>0.792835</td>\n",
       "      <td>0.248404</td>\n",
       "      <td>0.054365</td>\n",
       "      <td>0.945635</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.791659</td>\n",
       "      <td>0.249536</td>\n",
       "      <td>0.057540</td>\n",
       "      <td>0.942460</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>0.786929</td>\n",
       "      <td>0.250949</td>\n",
       "      <td>0.057540</td>\n",
       "      <td>0.942460</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>0.786015</td>\n",
       "      <td>0.290335</td>\n",
       "      <td>0.063095</td>\n",
       "      <td>0.936905</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>0.780688</td>\n",
       "      <td>0.257905</td>\n",
       "      <td>0.060516</td>\n",
       "      <td>0.939484</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>0.779310</td>\n",
       "      <td>0.241305</td>\n",
       "      <td>0.055952</td>\n",
       "      <td>0.944048</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.780573</td>\n",
       "      <td>0.242156</td>\n",
       "      <td>0.050992</td>\n",
       "      <td>0.949008</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>0.778453</td>\n",
       "      <td>0.242235</td>\n",
       "      <td>0.058730</td>\n",
       "      <td>0.941270</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>0.773347</td>\n",
       "      <td>0.244799</td>\n",
       "      <td>0.050992</td>\n",
       "      <td>0.949008</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>0.780001</td>\n",
       "      <td>0.248454</td>\n",
       "      <td>0.056944</td>\n",
       "      <td>0.943056</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>0.788569</td>\n",
       "      <td>0.273223</td>\n",
       "      <td>0.055952</td>\n",
       "      <td>0.944048</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.777569</td>\n",
       "      <td>0.250096</td>\n",
       "      <td>0.060317</td>\n",
       "      <td>0.939683</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>0.796044</td>\n",
       "      <td>0.240188</td>\n",
       "      <td>0.057738</td>\n",
       "      <td>0.942262</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>0.771913</td>\n",
       "      <td>0.247151</td>\n",
       "      <td>0.059722</td>\n",
       "      <td>0.940278</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>0.774384</td>\n",
       "      <td>0.217867</td>\n",
       "      <td>0.052976</td>\n",
       "      <td>0.947024</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>0.767343</td>\n",
       "      <td>0.272601</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.770966</td>\n",
       "      <td>0.242194</td>\n",
       "      <td>0.053373</td>\n",
       "      <td>0.946627</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>0.779341</td>\n",
       "      <td>0.234445</td>\n",
       "      <td>0.054365</td>\n",
       "      <td>0.945635</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>0.755983</td>\n",
       "      <td>0.236237</td>\n",
       "      <td>0.057540</td>\n",
       "      <td>0.942460</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>0.774287</td>\n",
       "      <td>0.237203</td>\n",
       "      <td>0.049603</td>\n",
       "      <td>0.950397</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.781888</td>\n",
       "      <td>0.232678</td>\n",
       "      <td>0.052579</td>\n",
       "      <td>0.947421</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.774774</td>\n",
       "      <td>0.246339</td>\n",
       "      <td>0.054762</td>\n",
       "      <td>0.945238</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>0.772550</td>\n",
       "      <td>0.238405</td>\n",
       "      <td>0.056151</td>\n",
       "      <td>0.943849</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>0.764309</td>\n",
       "      <td>0.260579</td>\n",
       "      <td>0.058532</td>\n",
       "      <td>0.941468</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>0.759335</td>\n",
       "      <td>0.251102</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>0.766961</td>\n",
       "      <td>0.231197</td>\n",
       "      <td>0.051587</td>\n",
       "      <td>0.948413</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.770240</td>\n",
       "      <td>0.234973</td>\n",
       "      <td>0.051786</td>\n",
       "      <td>0.948214</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>0.762025</td>\n",
       "      <td>0.236019</td>\n",
       "      <td>0.052976</td>\n",
       "      <td>0.947024</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>0.763494</td>\n",
       "      <td>0.228274</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.770603</td>\n",
       "      <td>0.252777</td>\n",
       "      <td>0.055357</td>\n",
       "      <td>0.944643</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>0.765259</td>\n",
       "      <td>0.258316</td>\n",
       "      <td>0.057540</td>\n",
       "      <td>0.942460</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.778087</td>\n",
       "      <td>0.275038</td>\n",
       "      <td>0.058730</td>\n",
       "      <td>0.941270</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>0.761347</td>\n",
       "      <td>0.238760</td>\n",
       "      <td>0.053175</td>\n",
       "      <td>0.946825</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>0.759138</td>\n",
       "      <td>0.262715</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>0.761968</td>\n",
       "      <td>0.242321</td>\n",
       "      <td>0.054167</td>\n",
       "      <td>0.945833</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>0.767430</td>\n",
       "      <td>0.247207</td>\n",
       "      <td>0.056548</td>\n",
       "      <td>0.943452</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.757209</td>\n",
       "      <td>0.237014</td>\n",
       "      <td>0.053770</td>\n",
       "      <td>0.946230</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>0.753989</td>\n",
       "      <td>0.236413</td>\n",
       "      <td>0.051587</td>\n",
       "      <td>0.948413</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>0.764314</td>\n",
       "      <td>0.236320</td>\n",
       "      <td>0.051984</td>\n",
       "      <td>0.948016</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>0.747840</td>\n",
       "      <td>0.221576</td>\n",
       "      <td>0.051786</td>\n",
       "      <td>0.948214</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>0.751481</td>\n",
       "      <td>0.248841</td>\n",
       "      <td>0.056349</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.756162</td>\n",
       "      <td>0.242576</td>\n",
       "      <td>0.050397</td>\n",
       "      <td>0.949603</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>0.759203</td>\n",
       "      <td>0.248560</td>\n",
       "      <td>0.056548</td>\n",
       "      <td>0.943452</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>0.755202</td>\n",
       "      <td>0.230108</td>\n",
       "      <td>0.049008</td>\n",
       "      <td>0.950992</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>0.751644</td>\n",
       "      <td>0.242486</td>\n",
       "      <td>0.054563</td>\n",
       "      <td>0.945437</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>0.749724</td>\n",
       "      <td>0.245415</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.746515</td>\n",
       "      <td>0.246282</td>\n",
       "      <td>0.054365</td>\n",
       "      <td>0.945635</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>0.746299</td>\n",
       "      <td>0.230779</td>\n",
       "      <td>0.053373</td>\n",
       "      <td>0.946627</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>0.748066</td>\n",
       "      <td>0.248239</td>\n",
       "      <td>0.056349</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>0.752799</td>\n",
       "      <td>0.235846</td>\n",
       "      <td>0.054960</td>\n",
       "      <td>0.945040</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>0.741283</td>\n",
       "      <td>0.253032</td>\n",
       "      <td>0.055952</td>\n",
       "      <td>0.944048</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>0.751519</td>\n",
       "      <td>0.253375</td>\n",
       "      <td>0.055952</td>\n",
       "      <td>0.944048</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.753800</td>\n",
       "      <td>0.241923</td>\n",
       "      <td>0.054365</td>\n",
       "      <td>0.945635</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>0.744662</td>\n",
       "      <td>0.239701</td>\n",
       "      <td>0.055952</td>\n",
       "      <td>0.944048</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>0.739330</td>\n",
       "      <td>0.236335</td>\n",
       "      <td>0.051984</td>\n",
       "      <td>0.948016</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>0.749177</td>\n",
       "      <td>0.235790</td>\n",
       "      <td>0.054563</td>\n",
       "      <td>0.945437</td>\n",
       "      <td>01:20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='59' class='' max='157', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      37.58% [59/157 00:28<00:47 0.7423]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with accuracy value: 0.41924601793289185.\n",
      "Better model found at epoch 1 with accuracy value: 0.5107142925262451.\n",
      "Better model found at epoch 2 with accuracy value: 0.5632936358451843.\n",
      "Better model found at epoch 3 with accuracy value: 0.6287698149681091.\n",
      "Better model found at epoch 4 with accuracy value: 0.6501984000205994.\n",
      "Better model found at epoch 5 with accuracy value: 0.6978174448013306.\n",
      "Better model found at epoch 6 with accuracy value: 0.7210317254066467.\n",
      "Better model found at epoch 7 with accuracy value: 0.7335317730903625.\n",
      "Better model found at epoch 8 with accuracy value: 0.7644841074943542.\n",
      "Better model found at epoch 9 with accuracy value: 0.7751984000205994.\n",
      "Better model found at epoch 11 with accuracy value: 0.8045634627342224.\n",
      "Better model found at epoch 13 with accuracy value: 0.8277778029441833.\n",
      "Better model found at epoch 14 with accuracy value: 0.8285714387893677.\n",
      "Better model found at epoch 15 with accuracy value: 0.8317460417747498.\n",
      "Better model found at epoch 17 with accuracy value: 0.8351190686225891.\n",
      "Better model found at epoch 19 with accuracy value: 0.8509920835494995.\n",
      "Better model found at epoch 21 with accuracy value: 0.8557539582252502.\n",
      "Better model found at epoch 25 with accuracy value: 0.8672618865966797.\n",
      "Better model found at epoch 26 with accuracy value: 0.8688492178916931.\n",
      "Better model found at epoch 27 with accuracy value: 0.8690476417541504.\n",
      "Better model found at epoch 29 with accuracy value: 0.879960298538208.\n",
      "Better model found at epoch 32 with accuracy value: 0.8841269612312317.\n",
      "Better model found at epoch 35 with accuracy value: 0.8890873193740845.\n",
      "Better model found at epoch 46 with accuracy value: 0.8966270089149475.\n",
      "Better model found at epoch 52 with accuracy value: 0.9109126925468445.\n",
      "Better model found at epoch 62 with accuracy value: 0.9115079641342163.\n",
      "Better model found at epoch 64 with accuracy value: 0.9154762029647827.\n",
      "Better model found at epoch 73 with accuracy value: 0.9158729910850525.\n",
      "Better model found at epoch 81 with accuracy value: 0.9160714149475098.\n",
      "Better model found at epoch 82 with accuracy value: 0.921230137348175.\n",
      "Better model found at epoch 88 with accuracy value: 0.9226190447807312.\n",
      "Better model found at epoch 96 with accuracy value: 0.9251984357833862.\n",
      "Better model found at epoch 106 with accuracy value: 0.9261904954910278.\n",
      "Better model found at epoch 109 with accuracy value: 0.9307539463043213.\n",
      "Better model found at epoch 114 with accuracy value: 0.9333333373069763.\n",
      "Better model found at epoch 119 with accuracy value: 0.9337301850318909.\n",
      "Better model found at epoch 126 with accuracy value: 0.9353174567222595.\n",
      "Better model found at epoch 141 with accuracy value: 0.9378968477249146.\n",
      "Better model found at epoch 162 with accuracy value: 0.940079391002655.\n",
      "Better model found at epoch 164 with accuracy value: 0.9414682388305664.\n",
      "Better model found at epoch 177 with accuracy value: 0.945039689540863.\n",
      "Better model found at epoch 192 with accuracy value: 0.9460317492485046.\n",
      "Better model found at epoch 200 with accuracy value: 0.946825385093689.\n",
      "Better model found at epoch 210 with accuracy value: 0.9490079283714294.\n",
      "Better model found at epoch 223 with accuracy value: 0.9503968358039856.\n",
      "Better model found at epoch 247 with accuracy value: 0.9509920477867126.\n"
     ]
    }
   ],
   "source": [
    "cb = SaveModelCallback(learn, every='improvement', monitor='accuracy', name='run_4')\n",
    "learn.fit_one_cycle(600, 1e-2, wd=0.0004, callbacks=[cb]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################  Testing  ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = [AGraph(g) for g in glob.glob('nb_graphs/rs/run_3/*.dot')]\n",
    "_, comp_graph = cell_graph.generate_comp_graph(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = arch_config(comp_graph=comp_graph,\n",
    "                   depth_coeff=1.0,\n",
    "                   width_coeff=1.0,\n",
    "                   channels=16,\n",
    "                   repeat_list=[2, 2, 3, 1],\n",
    "                   classes=45)\n",
    "\n",
    "net = get_gepnet(conf)\n",
    "count_parameters(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = get_transforms(do_flip=False)\n",
    "path = Path(\"/home/cliff/NWPU-RESISC45\")\n",
    "\n",
    "bs = 128\n",
    "\n",
    "data = (ImageList.from_folder(path)\n",
    "        .split_by_folder(train='train', valid='test')\n",
    "        .label_from_folder()\n",
    "        .transform(tfms, size=224)\n",
    "        .databunch(bs=bs, num_workers=num_cpus())\n",
    "        .normalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_dir = '/home/cliff/ResearchProjects/models/random_search/'\n",
    "model = Learner(data, net, metrics=[accuracy, error_rate]).load(model_dir+'run_3')\n",
    "_, acc, err = model.validate()\n",
    "print('Accuracy: %.2f | Error: %.2f' %(acc.item()*100, err.item()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.validate(data.valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(model)\n",
    "losses,idxs = interp.top_losses()\n",
    "len(data.valid_ds)==len(losses)==len(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interp.plot_top_losses(9, figsize=(15,15), heatmap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_confusion_matrix(figsize=(15,16), dpi=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds,y,losses = model.get_preds(ds_type=DatasetType.Valid, with_loss=True)\n",
    "#preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('comp_graphs/experiment_1/best/stats.pkl', 'rb') as f:\n",
    "    stats = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
