{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! Good to go!\n"
     ]
    }
   ],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.distributed import *\n",
    "from fastai.metrics import error_rate\n",
    "from fastai.callbacks import SaveModelCallback\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  print(\"Great! Good to go!\")\n",
    "else:\n",
    "  print('CUDA is not up!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gepcore.utils import cell_graph, convolution\n",
    "from gepcore.entity import Gene, Chromosome\n",
    "from gepcore.symbol import PrimitiveSet\n",
    "from gepnet.model import get_gepnet, arch_config\n",
    "from gepnet.utils import count_parameters\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygraphviz import AGraph\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get chromosme from file\n",
    "graph = [AGraph(g) for g in glob.glob('nb_graphs/rs/*.dot')]\n",
    "_, comp_graph = cell_graph.generate_comp_graph(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# generate new chromosome\n",
    "# define primitive set\n",
    "pset = PrimitiveSet('cnn')\n",
    "\n",
    "# add cellular encoding program symbols\n",
    "pset.add_program_symbol(cell_graph.end)\n",
    "pset.add_program_symbol(cell_graph.seq)\n",
    "pset.add_program_symbol(cell_graph.cpo)\n",
    "pset.add_program_symbol(cell_graph.cpi)\n",
    "\n",
    "# add convolutional operations symbols\n",
    "conv_symbol = convolution.get_symbol()\n",
    "pset.add_cell_symbol(conv_symbol.conv1x1)\n",
    "pset.add_cell_symbol(conv_symbol.conv3x3)\n",
    "pset.add_cell_symbol(conv_symbol.dwconv3x3)\n",
    "pset.add_cell_symbol(conv_symbol.conv1x3)\n",
    "pset.add_cell_symbol(conv_symbol.conv3x1)\n",
    "#pset.add_cell_symbol(conv_symbol.maxpool3x3)\n",
    "\n",
    "def gene_gen():\n",
    "    return Gene(pset, 2)\n",
    "\n",
    "ch = Chromosome(gene_gen, 3)\n",
    "graph, comp_graph = cell_graph.generate_comp_graph(ch)\n",
    "\n",
    "cell_graph.save_graph(graph, 'nb_graphs/rs')\n",
    "cell_graph.draw_graph(graph, 'nb_graphs/rs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# seed = 221\n",
    "# random.seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# enable torch backends\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "#torch.backends.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.100877"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = arch_config(comp_graph=comp_graph,\n",
    "                   depth_coeff=1.0,\n",
    "                   width_coeff=1.0,\n",
    "                   channels=32,\n",
    "                   repeat_list=[3, 2, 1, 1],\n",
    "                   classes=45)\n",
    "\n",
    "net = get_gepnet(conf)\n",
    "count_parameters(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = Path(\"/home/cliff/NWPU-RESISC45\")\n",
    "tfms = get_transforms(flip_vert=True, max_lighting=0.1, max_zoom=1.05, max_warp=0.)\n",
    "\n",
    "bs = 64\n",
    "data = (ImageList.from_folder(path/'train')\n",
    "        .split_by_rand_pct(valid_pct=0.1) \n",
    "        .label_from_folder()\n",
    "        .transform(tfms, size=224)\n",
    "        .databunch(bs=bs, num_workers=num_cpus())\n",
    "        .normalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=ImageDataBunch;\n",
       "\n",
       "Train: LabelList (24300 items)\n",
       "x: ImageList\n",
       "Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\n",
       "y: CategoryList\n",
       "forest,forest,forest,forest,forest\n",
       "Path: /home/cliff/NWPU-RESISC45/train;\n",
       "\n",
       "Valid: LabelList (2700 items)\n",
       "x: ImageList\n",
       "Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\n",
       "y: CategoryList\n",
       "ground_track_field,wetland,industrial_area,island,wetland\n",
       "Path: /home/cliff/NWPU-RESISC45/train;\n",
       "\n",
       "Test: None, model=GepNet(\n",
       "  (stem): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): AvgPool2d(kernel_size=3, stride=2, padding=0)\n",
       "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): AvgPool2d(kernel_size=3, stride=2, padding=0)\n",
       "  )\n",
       "  (blocks): Sequential(\n",
       "    (0): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x3_0): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x1_2): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x1_1): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x3_0): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x1_2): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x1_1): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x3_0): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x1_2): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x1_1): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x3_0): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x1_2): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x1_1): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x3_0): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x1_2): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x1_1): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      (1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x3_0): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x1_2): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x1_1): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x3_0): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x1_2): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x1_1): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (0): AdaptiveAvgPool2d(output_size=1)\n",
       "    (1): Flatten()\n",
       "    (2): Linear(in_features=512, out_features=45, bias=True)\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function error_rate at 0x7f0f15cd71e0>, <function accuracy at 0x7f0f15cccf28>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/home/cliff/NWPU-RESISC45/train'), model_dir='/home/cliff/ResearchProjects/models/random_search/', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False), functools.partial(<class 'fastai.callbacks.mixup.MixUpCallback'>, alpha=0.4, stack_x=False, stack_y=True)], callbacks=[MixedPrecision\n",
       "learn: Learner(data=ImageDataBunch;\n",
       "\n",
       "Train: LabelList (24300 items)\n",
       "x: ImageList\n",
       "Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\n",
       "y: CategoryList\n",
       "forest,forest,forest,forest,forest\n",
       "Path: /home/cliff/NWPU-RESISC45/train;\n",
       "\n",
       "Valid: LabelList (2700 items)\n",
       "x: ImageList\n",
       "Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\n",
       "y: CategoryList\n",
       "ground_track_field,wetland,industrial_area,island,wetland\n",
       "Path: /home/cliff/NWPU-RESISC45/train;\n",
       "\n",
       "Test: None, model=GepNet(\n",
       "  (stem): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): AvgPool2d(kernel_size=3, stride=2, padding=0)\n",
       "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): AvgPool2d(kernel_size=3, stride=2, padding=0)\n",
       "  )\n",
       "  (blocks): Sequential(\n",
       "    (0): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x3_0): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x1_2): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x1_1): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x3_0): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x1_2): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x1_1): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x3_0): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x1_2): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x1_1): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x3_0): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x1_2): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x1_1): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x3_0): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x1_2): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x1_1): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      (1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x3_0): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x1_2): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x1_1): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): GepBlock(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (path_0): GepNetLayer(\n",
       "        (conv1x3_0): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (dwconv3x3_1): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x1_2): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_1): GepNetLayer(\n",
       "        (dwconv3x3_0): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (path_2): GepNetLayer(\n",
       "        (conv1x1_0): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3x1_1): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (convproj): Sequential(\n",
       "        (0): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (0): AdaptiveAvgPool2d(output_size=1)\n",
       "    (1): Flatten()\n",
       "    (2): Linear(in_features=512, out_features=45, bias=True)\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function error_rate at 0x7f0f15cd71e0>, <function accuracy at 0x7f0f15cccf28>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/home/cliff/NWPU-RESISC45/train'), model_dir='/home/cliff/ResearchProjects/models/random_search/', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False), functools.partial(<class 'fastai.callbacks.mixup.MixUpCallback'>, alpha=0.4, stack_x=False, stack_y=True)], callbacks=[...], layer_groups=[Sequential(\n",
       "  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): AvgPool2d(kernel_size=3, stride=2, padding=0)\n",
       "  (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): AvgPool2d(kernel_size=3, stride=2, padding=0)\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "  (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "  (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): ReLU(inplace=True)\n",
       "  (15): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "  (16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (17): ReLU(inplace=True)\n",
       "  (18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "  (19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (22): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (23): ReLU(inplace=True)\n",
       "  (24): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "  (25): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (26): ReLU(inplace=True)\n",
       "  (27): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (28): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "  (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (32): ReLU(inplace=True)\n",
       "  (33): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "  (34): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (35): ReLU(inplace=True)\n",
       "  (36): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "  (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (38): ReLU(inplace=True)\n",
       "  (39): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "  (40): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (41): ReLU(inplace=True)\n",
       "  (42): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (43): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (44): ReLU(inplace=True)\n",
       "  (45): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "  (46): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (47): ReLU(inplace=True)\n",
       "  (48): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (50): ReLU(inplace=True)\n",
       "  (51): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "  (52): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (53): ReLU(inplace=True)\n",
       "  (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "  (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (56): ReLU(inplace=True)\n",
       "  (57): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "  (58): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (59): ReLU(inplace=True)\n",
       "  (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "  (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (62): ReLU(inplace=True)\n",
       "  (63): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (64): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (65): ReLU(inplace=True)\n",
       "  (66): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "  (67): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (68): ReLU(inplace=True)\n",
       "  (69): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (70): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (71): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (72): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (73): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (74): ReLU(inplace=True)\n",
       "  (75): ReLU(inplace=True)\n",
       "  (76): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "  (77): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (78): ReLU(inplace=True)\n",
       "  (79): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "  (80): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (81): ReLU(inplace=True)\n",
       "  (82): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "  (83): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (84): ReLU(inplace=True)\n",
       "  (85): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "  (86): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (87): ReLU(inplace=True)\n",
       "  (88): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (89): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (90): ReLU(inplace=True)\n",
       "  (91): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "  (92): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (93): ReLU(inplace=True)\n",
       "  (94): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (95): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (96): ReLU(inplace=True)\n",
       "  (97): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "  (98): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (99): ReLU(inplace=True)\n",
       "  (100): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "  (101): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (102): ReLU(inplace=True)\n",
       "  (103): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "  (104): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (105): ReLU(inplace=True)\n",
       "  (106): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "  (107): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (108): ReLU(inplace=True)\n",
       "  (109): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (110): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (111): ReLU(inplace=True)\n",
       "  (112): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "  (113): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (114): ReLU(inplace=True)\n",
       "  (115): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (116): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (117): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (118): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (119): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (120): ReLU(inplace=True)\n",
       "  (121): ReLU(inplace=True)\n",
       "  (122): Conv2d(256, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "  (123): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (124): ReLU(inplace=True)\n",
       "  (125): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "  (126): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (127): ReLU(inplace=True)\n",
       "  (128): Conv2d(256, 256, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "  (129): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (130): ReLU(inplace=True)\n",
       "  (131): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "  (132): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (133): ReLU(inplace=True)\n",
       "  (134): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (135): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (136): ReLU(inplace=True)\n",
       "  (137): Conv2d(256, 256, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "  (138): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (139): ReLU(inplace=True)\n",
       "  (140): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (141): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (142): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (143): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (144): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (145): ReLU(inplace=True)\n",
       "  (146): ReLU(inplace=True)\n",
       "  (147): Conv2d(512, 512, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "  (148): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (149): ReLU(inplace=True)\n",
       "  (150): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "  (151): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (152): ReLU(inplace=True)\n",
       "  (153): Conv2d(512, 512, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "  (154): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (155): ReLU(inplace=True)\n",
       "  (156): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "  (157): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (158): ReLU(inplace=True)\n",
       "  (159): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (160): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (161): ReLU(inplace=True)\n",
       "  (162): Conv2d(512, 512, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "  (163): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (164): ReLU(inplace=True)\n",
       "  (165): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (166): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (167): AdaptiveAvgPool2d(output_size=1)\n",
       "  (168): Flatten()\n",
       "  (169): Linear(in_features=512, out_features=45, bias=True)\n",
       ")], add_time=True, silent=False)\n",
       "loss_scale: 65536\n",
       "max_noskip: 1000\n",
       "dynamic: True\n",
       "clip: None\n",
       "flat_master: False\n",
       "max_scale: 16777216\n",
       "loss_fp32: True], layer_groups=[Sequential(\n",
       "  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): AvgPool2d(kernel_size=3, stride=2, padding=0)\n",
       "  (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): AvgPool2d(kernel_size=3, stride=2, padding=0)\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "  (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "  (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): ReLU(inplace=True)\n",
       "  (15): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "  (16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (17): ReLU(inplace=True)\n",
       "  (18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "  (19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (22): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (23): ReLU(inplace=True)\n",
       "  (24): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "  (25): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (26): ReLU(inplace=True)\n",
       "  (27): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (28): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "  (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (32): ReLU(inplace=True)\n",
       "  (33): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "  (34): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (35): ReLU(inplace=True)\n",
       "  (36): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "  (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (38): ReLU(inplace=True)\n",
       "  (39): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "  (40): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (41): ReLU(inplace=True)\n",
       "  (42): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (43): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (44): ReLU(inplace=True)\n",
       "  (45): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "  (46): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (47): ReLU(inplace=True)\n",
       "  (48): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (50): ReLU(inplace=True)\n",
       "  (51): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "  (52): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (53): ReLU(inplace=True)\n",
       "  (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "  (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (56): ReLU(inplace=True)\n",
       "  (57): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "  (58): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (59): ReLU(inplace=True)\n",
       "  (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "  (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (62): ReLU(inplace=True)\n",
       "  (63): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (64): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (65): ReLU(inplace=True)\n",
       "  (66): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "  (67): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (68): ReLU(inplace=True)\n",
       "  (69): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (70): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (71): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (72): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (73): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (74): ReLU(inplace=True)\n",
       "  (75): ReLU(inplace=True)\n",
       "  (76): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "  (77): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (78): ReLU(inplace=True)\n",
       "  (79): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "  (80): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (81): ReLU(inplace=True)\n",
       "  (82): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "  (83): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (84): ReLU(inplace=True)\n",
       "  (85): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "  (86): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (87): ReLU(inplace=True)\n",
       "  (88): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (89): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (90): ReLU(inplace=True)\n",
       "  (91): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "  (92): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (93): ReLU(inplace=True)\n",
       "  (94): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (95): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (96): ReLU(inplace=True)\n",
       "  (97): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "  (98): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (99): ReLU(inplace=True)\n",
       "  (100): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "  (101): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (102): ReLU(inplace=True)\n",
       "  (103): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "  (104): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (105): ReLU(inplace=True)\n",
       "  (106): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "  (107): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (108): ReLU(inplace=True)\n",
       "  (109): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (110): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (111): ReLU(inplace=True)\n",
       "  (112): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "  (113): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (114): ReLU(inplace=True)\n",
       "  (115): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (116): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (117): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (118): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (119): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (120): ReLU(inplace=True)\n",
       "  (121): ReLU(inplace=True)\n",
       "  (122): Conv2d(256, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "  (123): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (124): ReLU(inplace=True)\n",
       "  (125): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "  (126): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (127): ReLU(inplace=True)\n",
       "  (128): Conv2d(256, 256, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "  (129): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (130): ReLU(inplace=True)\n",
       "  (131): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "  (132): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (133): ReLU(inplace=True)\n",
       "  (134): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (135): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (136): ReLU(inplace=True)\n",
       "  (137): Conv2d(256, 256, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "  (138): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (139): ReLU(inplace=True)\n",
       "  (140): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (141): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (142): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (143): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (144): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (145): ReLU(inplace=True)\n",
       "  (146): ReLU(inplace=True)\n",
       "  (147): Conv2d(512, 512, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "  (148): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (149): ReLU(inplace=True)\n",
       "  (150): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "  (151): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (152): ReLU(inplace=True)\n",
       "  (153): Conv2d(512, 512, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "  (154): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (155): ReLU(inplace=True)\n",
       "  (156): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "  (157): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (158): ReLU(inplace=True)\n",
       "  (159): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (160): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (161): ReLU(inplace=True)\n",
       "  (162): Conv2d(512, 512, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "  (163): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (164): ReLU(inplace=True)\n",
       "  (165): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (166): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (167): AdaptiveAvgPool2d(output_size=1)\n",
       "  (168): Flatten()\n",
       "  (169): Linear(in_features=512, out_features=45, bias=True)\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = '/home/cliff/ResearchProjects/models/random_search/'\n",
    "learn = Learner(data, net, metrics=[error_rate, accuracy], model_dir=model_dir).mixup()\n",
    "learn.to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='86' class='' max='379', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      22.69% [86/379 00:23<01:20 4.1790]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXxU5dn/8c+VHcgCgYQ9ECDsqwQEARWwaBFxV+zjiku1rVqttrX2Z1usdauCPn2qotaqWHetihubgCgRE/Z9CXsCCQTCmpBkrt8fM2jEBAKZM2eW6/16zYuZc+6Z+c4wk2vus9y3qCrGGGMiV5TbAYwxxrjLCoExxkQ4KwTGGBPhrBAYY0yEs0JgjDERLsbtACerWbNm2r59e7djGGNMSMnLy9ulqmk1rQu5QtC+fXtyc3PdjmGMMSFFRDbXts42DRljTISzQmCMMRHOCoExxkQ4KwTGGBPhrBAYY0yEs0JgjDERzgqBMcZEOCsEQWjRlj28m7eNw0eq3I5ijIkAIXdC2akqr6yiyqM0jAvel7y/rILHPlvDqzne8z7+9skqxg/N5JrB7UhOiHU5nTEmXEVMj+Ct3G0MffQL/jFrHaWHK9yO8yMzV+1k1MS5TPlmM+OHZDLlxtPp2TqFxz9fw5CHZ/HIp6vJ3VRCeaX1Eowx/iWhNkNZdna2nsoQE0u27mXSjLV8saaYpPgYrj2jHeOHZNI0Md6BlDVTVaYuLeTZORsor/QQGx1FXEwUVR4Py7fvo0vzJB65tBf9Mpp8d5/l20t5ZvYGPlleiCrEx0TRL6MxAzObcu3gdjQLYH5jTOgSkTxVza5xXaQUgqOWby/ln7PX8+nyHSTERHPnOVncNDSTmOj6d442FB9g+sqdVHmUEV3T6doiCREBYM2O/fzpw+Xk5JfQpXkSndITKa/0UFHlvQzp1Iybh3UgLqbmHCUHj/DtphIWbPReVhSU0qtNY97++eBa72OMMUdZIajB+qL9PPrZGqav3Emv1ik8dllvurVMPunHWb1jH1OXFPL5ih2sKzrwg3WtUhIY0S2daBGmfLOFxPgY7jm3Cz8bmEF0lNQr/6fLCrnttYWMH5LJAxd0r9djGWPCnxWCWqgqnyzbwQMfLKf0cAW/GN6JX5zdkYTY6BPet6LKw8Tpa3lmzgaiRBjYPpVzezRnVI8WxEQLX6wuYuaqIuat38XhiiquGpjBPaO6kNoozi/ZAf784Qr+/fUmnr26P+f1bOG3xzXGhB9XC4GIRAO5wHZVHVNLm8uAt4EBqnrcv/L+LARHlRw8woNTV/L+ou3ERUfRo3Uyp2U0oV9GY7LbpdIiJeEH7beWHOL21xexeOtersxuy+9+2rXWP/BlFVXsL6skLcn/2/LLK6u44tn55O86yMe3DyOjaUO/P4cxJjy4XQjuBrKB5JoKgYgkAR8DccCv3CgER83fsJsv1hSxaMselm4rpbzSA0BGakNOz0xlYGYqqvDg1JUg8PAlvRjTu5UjWepqa8khzn/6S9o1bcQ7tw0mPubEvRljTORxrRCISBvgZeAh4O5aCsEkYAZwD3CPm4WguiOVHlYV7iN38x6+yd/Ngk0l7D3kPez0tIzGPDWuH21Tg+MX+OcrdvDzV/O4vH8bHrm0d733Pxhjws/xCoHTZ1dNAn4LJNW0UkT6AW1VdaqI3FPbg4jILcAtABkZGU7k/JG4mCj6tG1Mn7aNuXFoJh6Psq7oANv2HOKszml+OcrIX87t0YI7Rmbx9Mx17Dl0hKfG9aNRfPCeOGeMCS6O/TUTkTFAkarm1bI+CpgI/OZEj6Wqk1U1W1Wz09JqnHLTcVFRQpcWSYzs1jyoisBRd/+kMw9e1JNZq4u44rn57CgtczuSMSZEOPkXbQgwVkQ2AW8AI0RkSrX1SUBPYLavzSDgQxGpsetiTuyaQe148foBbNp1kIv+7ytWFuxzO5IxJgQ4VghU9T5VbaOq7YFxwCxVvbra+lJVbaaq7X1tcoCxJ9pHYI5veJd03r71DETgkme+YtKMtTZ4nTHmuAK+jUNEJojI2EA/byTp3iqZD345hJHdmjNpxjpGPjGbDxZvJ9TOGTHGBEZEn1AWCRZsLGHC1BUs376P0zIac9XADM7uku7IeQ3GmOBlZxZHOI9HeWfhNiZNX0uBbydy7zYpDO+SzhUD2tK6cQOXExpjnGaFwADeITVWFOzji9VFzFpTxOKte2naKJ7Xbz6drOY1HuFrjAkTVghMjdbu3M//vPANHo8y5abTT2nQPWNMaDheIQi+A+JNwHRunsSbtwwiNjqKq57PYdm2UrcjGWNcYIUgwnVIS+Stnw+mUVwMP3shh7zNe9yOZIwJMCsEhoymDXnr1sGkNorjsme/5vqXFjBtxQ4qqzxuRzPGBIDtIzDf2X2gnJfnb+bNb7ewc185LZITuLBfK1IaxHL0YyICF/RuFTQD7hlj6sZ2FpuTUlnlYebqIl5fsIU5a4s59iPSPDme128eRIe0RHcCGmNOmhUCc8rKK6tQ9fYEBGFD8QGufuEboqOE128ZREcrBsaEBDtqyJyy+JhoEmKjiY+JJi4mim4tk3n9lkF4VBk3OYf1x8zTbIwJPVYIzEnr3DyJ128ehCqMm5zDup373Y5kjKkHKwTmlGQ1T+KNWwYhAldOzmHJ1r1uRzLGnCIrBOaUdUr3noPQMC6aq57P4ct1xW5HMsacAisEpl4ymzXivdvOICO1IeP//S0fLSlwO5Ix5iRZITD1lp6cwJs/H0y/tk24441FvDJ/k9uRjDEnwQqB8YuUBrG8cuNARnZtzgMfrGD5dhu3yJhQYYXA+E1CbDRPXtmH5IQYnpq5zu04xpg6skJg/Co5IZYbh3Zg+sqd1iswJkRYITB+d/2Q9iQnxPC09QqMCQlWCIzfpTSIZfzQTKat3MmKAusVGBPsrBAYR9wwJJMk6xUYExKsEBhHpDSIZfyQTD5fsZOVBfvcjmOMOQ4rBMYx44dar8CYUGCFwDgmpUEsNwzJ5LMVO+wIImOCmBUC46gbh2TStFEc9723jAqb+tKYoGSFwDgqpWEsD17Uk2XbS5k8N9/tOMaYGlghMI4b3asl5/dqyaQZa1mzw+YuMCbYWCEwAfGXC3uQlBDLve8sodI2ERkTVBwvBCISLSKLRGRqDetuFZFlIrJYROaJSHen8xh3NEuMZ8KFPVi6rZTJX9omImOCSSB6BHcCq2pZ9x9V7aWqfYHHgCcDkMe4ZEzvVozu1YJJ09ex1qa3NCZoOFoIRKQNcD7wQk3rVbX6mUaNAHUyj3HfhAt70ig+mnvftk1ExgQLp3sEk4DfArV+40XklyKyAW+P4I5a2twiIrkikltcbNMhhjLvJqKeLLFNRMYEDccKgYiMAYpUNe947VT1/1S1I/A74I+1tJmsqtmqmp2WluZAWhNIY3q35Kc9bRORMcHCyR7BEGCsiGwC3gBGiMiU47R/A7jIwTwmSIgID15km4iMCRaOFQJVvU9V26hqe2AcMEtVr67eRkSyqt08H7BBaSKEbSIyJngE/DwCEZkgImN9N38lIitEZDFwN3BdoPMY99gmImOCg6iG1oE62dnZmpub63YM4ye7DpQzauJc2qY25P3bziAqStyOZExYEpE8Vc2uaZ2dWWxc1Swxnj+M7saSrXv5ZHmh23GMiUhWCIzrLu7Xmi7Nk3hi2lobodQYF1ghMK6LjhLuPbcLG3cd5J28bW7HMSbiWCEwQWFkt3ROy2jMpBlrKauocjuOMRHFCoEJCiLC787rys595bz89Sa34xgTUawQmKBxeoemnN0ljX/O3kDp4Qq34xgTMawQmKBy77ldKD1cwfM2m5kxAWOFwASVHq1SGNunFS/O28jOfWVuxzEmIlghMEHnnlFd8Kjy5w9XuB3FmIhghcAEnYymDbnznCw+Xb6Dz1fscDuOMWHPCoEJSjcP60C3lsk88MFy9pXZjmNjnGSFwASl2OgoHrmkF8X7y3nss9VuxzEmrFkhMEGrT9vGXH9GJlNytpC7qcTtOMaELSsEJqj9ZlRnWjduwO/fW0Z5pZ1xbIwTrBCYoNYoPoaHLu7J+qID/PurTW7HMSYsWSEwQe/sLukMy2rGv77ayJFKG53UGH+zQmBCwk3DOrBzXzkfLytwO4oxYccKgQkJZ2Y1Iys9kRe+3EiozapnTLCzQmBCgohw07BMVhTsIyffjiAyxp+sEJiQcWHf1jRtFMcLX9qAdMb4kxUCEzISYqO5ZnA7Zq4uYkPxAbfjGBM2rBCYkHL1oHbExUTxr3kb3Y5iTNiwQmBCSrPEeC7p15p3F26j5OARt+MYExasEJiQM35oJmUVHl7L2ex2FGPCghUCE3I6N09ieJc0Js/NZ2vJIbfjGBPyrBCYkDThwp4A3PXmYiqr7GxjY+rDCoEJSW1TG/LXi3uSu3kP/5y9we04xoQ0KwQmZF3YtzUX9m3FUzPXsXDLHrfjGBOyHC8EIhItIotEZGoN6+4WkZUislREZopIO6fzmPAy4cKetEhO4K43F3OgvNLtOMaEpED0CO4EVtWybhGQraq9gXeAxwKQx4SRlAaxTLyyL1tLDvHH95dx0IqBMSfN0UIgIm2A84EXalqvql+o6tHDPnKANk7mMeFpYGYqvxqRxX8XF5D91xnc8foiZq7aSYXtRDamTmIcfvxJwG+BpDq0vRH4tKYVInILcAtARkaG38KZ8HHXOVkMy2rGfxdt5+NlhXy4pIDURnE8f21/+rdLdTueMUHNsR6BiIwBilQ1rw5trwaygcdrWq+qk1U1W1Wz09LS/JzUhAMRYUD7VB66uBcL/nAOL16XTYPYaH737jKbzMaYE3By09AQYKyIbALeAEaIyJRjG4nIOcD9wFhVLXcwj4kQcTFRjOzWnAcv6sH6ogO8aOMSGXNcjhUCVb1PVduoantgHDBLVa+u3kZE+gHP4S0CRU5lMZFpRNfmjOrenKdnrmP73sNuxzEmaAX8PAIRmSAiY303HwcSgbdFZLGIfBjoPCa8PXBBdwAmfLTC5STGBC+ndxYDoKqzgdm+6w9UW35OIJ7fRK42TRpyx8gsHv1sNbNW72RE1+ZuRzIm6NiZxSbs3Tg0k07pifzpwxWUVVS5HceYoGOFwIS9uJgoHrywJ1tLDjNpxjq34xgTdOpUCESko4jE+66fLSJ3iEhjZ6MZ4z+DOzZl3IC2PDtnAzNX7XQ7jjFBpa49gneBKhHpBLwIZAL/cSyVMQ7489ge9GiVzF1vLmbLbpvHwJij6loIPKpaCVwMTFLVu4CWzsUyxv8SYqN55n/6IyL8fEoeh4/Y/gJjoO6FoEJErgKuA46OIhrrTCRjnJPRtCGTxvVl9Y593P/fZaiq25GMcV1dC8ENwGDgIVXdKCKZwI/OEjYmFAzvks6dI7N4b+F2Xvtmi9txjHFdnc4jUNWVwB0AItIESFLVR5wMZoyT7hiRxeKte5kwdSWDOjSlU3qi25GMcU1djxqaLSLJIpIKLAFeEpEnnY1mjHOiooTHLutNg9hofv/uUjwe20RkIlddNw2lqOo+4BLgJVXtD9hZwSakpScl8MCY7uRu3sOrOZvdjmOMa+paCGJEpCVwBd/vLDYm5F1yWmvO6pzGo5+tZtseO6TURKa6FoIJwOfABlX9VkQ6AHaKpgl5IsJDF/dEgPves6OITGSqUyFQ1bdVtbeq3ua7na+qlzobzZjAaNOkIb/7aVe+XLeLdxdudzuOMQFX153FbUTkfREpEpGdIvKubz5iY8LC1ae3Y0D7Jkz4aAVF+8vcjmNMQNV109BLwIdAK6A18JFvmTFhISpKeOTS3pRVeHj4k9VuxzEmoOpaCNJU9SVVrfRd/g3Y5MEmrHRMS+TWszrw/qLtfL1hl9txjAmYuhaCXSJytYhE+y5XA7udDGaMG34xvBNtUxvw//673Ca9NxGjroVgPN5DR3cAhcBleIedMCasJMRGM2FsTzYUH+SFefluxzEmIOp61NAWVR2rqmmqmq6qF+E9ucyYsDO8azrn9vBOem/nFphIUJ8Zyu72WwpjgswDF/RAEP784Uq3oxjjuPoUAvFbCmOCTOvGDfj1OVnMWLWTGSttRjMT3uo0+mgt7BRME9bGD83kzW+38vjnaxjRNZ2oKPvtY35o0ZY9vDJ/M0X7y2jaKJ6miXE0S4ynU3oiP+nWPGQ+M8ctBCKyn5r/4AvQwJFExgSJ2Ogo7jwnizvfWMyny3dwfm+blM9AZZWHz1fs5MV5+Szcspek+Bg6NU9k25697D5whAPllQD0ap3C/ed3Y1CHpi4nPjEJtbFVsrOzNTc31+0YJkJUeZRzJ81FgM9+fSbRIfILz/jXnoNH+HrDbuatL2b2mmIKS8vISG3IDUPac3l2WxLjv/9NXVZRxWfLd/DYZ6spKC1jVPfm3De6G5nNGrn4CkBE8lQ1u8Z1VgiMOb6PlhRw++uLePqqfozt08rtOMZBh49UkZO/m4LSwxTsPUzh3jLWFR1geUEpqpAUH8Ogjk25rH8bzunW/Lg/DMoqqnhx3kb++cV6yis9/Oz0DG4fkUVaUnwAX9H3rBAYUw8ej3LeU3Op8ijT7jrLegVh7OoXvmHeeu9Z5dFRQovkBNqmNmBwh2YMzWpGnzYpxESf3DE2xfvLmTRjLW98u5X4mChuGtaBW87s8INeRCBYITCmnj5ZVsgvXlvIpCv7clG/1m7HMQ7I21zCpc/M544Rnbjq9AzSkxL8WvTziw/wxLS1fLyskKaN4rhiQFtGdk2nX0aTgPy4sEJgTD15PMrop7+kvNLD9LvOPOlfhSb43fjvb1m4ZQ9f/X4EDeOc+7W+ZOteJs5Yy7x1u6j0KE0axjK8Szpndk6jf7smtGnSAJGaC4Oq1rruRI5XCBzvm4hINJALbFfVMcesOxOYBPQGxqnqO07nMeZUREUJvz6nM7dOyeO/iwu4rL+Nwh5OVu/Yx8zVRdz9k86OFgGAPm0b8+8bBlJ6uIIv1xUzc1URs9YU8d4i71wYzZPjyW6XSufmSZQcLKegtIzCUu/+ivtGd3PksxeIjVR3AquA5BrWbQGuB+4JQA5j6uXcHs3p0SqZidPXcn6vljSIi3Y7kvGTZ2ZvoFFcNNcNbh+w50xpEMuY3q0Y07sVVR5lVeE+8jbvIXfzHvI2lfDxskKSEmJoldKAlo0T6NW6MRmpDR3J4mgh8E1ecz7wEDUMSaGqm3ztbJhHE/REhD+e352rns/hmdnruXtUF7cjGT/YvPsgHy0p4KZhHUhpGOtKhugooWfrFHq2TuG6M9oD3qOOEmID82PD6Q2dk4DfAvX6Qy8it4hIrojkFhcX+yeZMadgcMemXNCnFc/OzWfz7oNuxzF+8NzcfGKiorhpaKbbUX4gUEUAHCwEIjIGKFLVvPo+lqpOVtVsVc1OS7P5cIy77h/djdgo4S8f2YB0oa5oXxnv5G7jsuw2pCcnuB3HNU72CIYAY0VkE/AGMEJEpjj4fMYERIuUBO4YmcWs1UU2IF2Ie2HeRio9Hn5+Zge3o7jKsUKgqvepahtVbQ+MA2ap6tVOPZ8xgXTDkEw6pjXiL1NXUFZR5XYccwpKD1XwWs5mxvRuRbum7g7/4LaAHwwtIhNEZKzv+gAR2QZcDjwnIisCnceYUxEXE8WEC3uyteQwz87Z4HYccwpemb+Jg0eq+MXwjm5HcV1AznFW1dnAbN/1B6ot/xawA7JNSBrSqRnn927JM7M38LOBGRG9jTnUHD5SxUtfb2JE13S6tqjpyPbIYqdHGlMP947qwpEqDy/P3+R2FHMS3vh2CyUHj/CLs603AFYIjKmX9s0aMap7c6bkbOHQkUq345g6qKjy8PzcfAa2TyW7farbcYKCFQJj6umWMztQeriCt3O3uR3F1MEHiwsoKC3jNusNfMcKgTH11L9dKv0yGvPivI1UeUJrEMdI4/Eoz87ZQLeWyZzdxc5JOsoKgTF+cMuwDmwpOcS0FTvcjmKOY/qqnawvOsBtZ3c85VE8w5EVAmP8YFSPFmSkNmTyl/luRzG1UFX+OXsDGakNGd2zhdtxgooVAmP8IDpKuHFoJou27CVvc4nbcUwNvt20hyVb9/LzszrYfBLHsHfDGD+5PLsNKQ1imTzXegXB6M1vt5IYH8Ml/ezUpWNZITDGTxrGxXD1oAymrdxJfvEBt+OYavaXVfDJskIu6NPK5pGogRUCY/zoujPa0yguhrveXGxjEAWRj5cWcriiiisHtHU7SlCyQmCMH6UnJfD3y/uwZFupDVMdRN7K3Urn5on0aZPidpSgZIXAGD87r2cLbju7I68v2MJb3251O07EW1+0n4Vb9nJFdls7ZLQWVgiMccA9o7owtFMz/vjBcpZtK3U7TkR7O3cbMVHCRf1aux0laFkhMMYB0VHCU+P60qxRHLdOyWPPwSNuR4pIFVUe3l24nZHd0mmWGO92nKBlhcAYhzRNjOeZq/tTvL+cCVNtf4EbZq8pZteBcq7Itp3Ex2OFwBgH9WnbmKsHtWPq0gKK9pe5HSfivJW7lbSkeM7qbOMKHY8VAmMcds3gdlRUKW8ssB3HgVS0v4xZq4u49LQ2dibxCdi7Y4zDMps14szOabz2zWYqqjxux4kYHywqoMqjXJ5tZxKfiBUCYwLg2kHt2LmvnOkrd7odJWJMX7WT7i2T6ZiW6HaUoGeFwJgAGN41ndaNG/DK/E1uR4kI+8oqyNu8h+Fdbd9AXVghMCYAoqOEawa3Iye/hDU79rsdJ+x9tW4XVR7l7C7pbkcJCVYIjAmQK7LbEhcTxas5m9yOEva+WFNEckIM/do2djtKSLBCYEyApDaK44LerXhv4Xb2lVW4HSdsqSpz1hYzLCvNjhaqI3uXjAmg685ox6EjVbyXZxPdO2VV4X527iu3OYlPghUCYwKod5vG9GnbmFfmb7aJ7h3yxZoiAM6yQlBnVgiMCbCbh2WSv+sgU5cWuB0lLM1ZU0yPVsmkJyW4HSVkWCEwJsBG92xJ1xZJTJy+1k4w87PSwxXkbdnDcDta6KRYITAmwKKihN+M6sKm3Yd4b6HtK/Cned8dNmqbhU6G44VARKJFZJGITK1hXbyIvCki60XkGxFp73QeY4LBOd3S6dMmhadnrqe80qa09JfZvsNG+9phoyclED2CO4FVtay7Edijqp2AicCjAchjjOtEvL2C7XsP86bNYuYXqsrstcWc2dkOGz1Zjr5bItIGOB94oZYmFwIv+66/A4wUm0vORIhhWc0YmJnK/85az+Ej1iuorxUF+yjeX25nE58Cp8vmJOC3QG17xFoDWwFUtRIoBZoe20hEbhGRXBHJLS4udiqrMQElIvzmJ50p3l/OlJzNbscJeXPWev822NwDJ8+xQiAiY4AiVc07XrMalv3o4GpVnayq2aqanZZm/8kmfJzeoSnDsprxzJwNHCivdDtOyFJVPl+xg56tk0lLsikpT5aTPYIhwFgR2QS8AYwQkSnHtNkGtAUQkRggBShxMJMxQeeeUV0oOXiEyXM2uB0lZE1fuZOl20q5ckCG21FCkmOFQFXvU9U2qtoeGAfMUtWrj2n2IXCd7/plvjZ2uqWJKH3aNmZM75ZM/jKfHaU2neXJqqjy8Mhnq+mQ1ohxA2xu4lMR8F3rIjJBRMb6br4INBWR9cDdwO8DnceYYPC787ri8cCT09e4HSXkvLFgC/nFB7nvp92ItaOFTklMIJ5EVWcDs33XH6i2vAy4PBAZjAlmbVMbcu3gdrz41UZuGJJJt5bJbkcKCfvLKpg0Yx0DM1M5p5sdLXSqrHwaEyR+NaITyQmx/O2T2k67Mcd6bk4+uw8e4f7R3bAjz0+dFQJjgkTjhnHcPqITX67b9d2hkKZ2haWHef7LfMb2aUUfO5O4XqwQGBNErhncjozUhjz8ySobpvoEnpi2FlW499wubkcJeVYIjAki8THR/Pa8LqzesZ938mzoidpsLTnEuwu3cd0Z7Wib2tDtOCHPCoExQeb8Xi3pl9GYv09by0E7yaxGHy0tQBWuHdze7ShhwQqBMUFGRPjj+d0p3l/Oc3Pz3Y4TlKYuKaRP28bWG/ATKwTGBKH+7Zp4TzKbu4HC0sNuxwkq+cUHWFm4jwt6t3Q7StiwQmBMkDp6ktnjn9tJZtVNXVoIwOheVgj8xQqBMUGqbWpDbhjanvcWbmfZtlK34wSNj5cWkt2uCa0aN3A7StiwQmBMEPvl8E6kNorjrx+v5OgwXIWlh3luzgbue28ZZRWRNY/Bup37WbNzP2Nss5BfBWSICWPMqUlOiOWuc7L4fx+s4KGPV7GiYB85G3dzdGjGpIQY/jC6m7shA+ijpYWI2GYhf7MegTFB7qqBGXRKT+SFeRvZsa+MO0dm8cU9Z3PVwAxe+DKfvM173I4YEKrK1KUFnJ6ZSnpygttxwor1CIwJcjHRUUy58XR2HSinR6vk78bU+cPorsxdW8y97yzhkzuGkRAb7XJSZ60q3E9+8UHGD8l0O0rYsR6BMSGgRUoCPVun/GBgtaSEWB65tBf5xQeZOGOti+kCY+rSAqKjhJ/2bOF2lLBjhcCYEDYsK42rBrbl+bn5LNwSvpuIvJuFCjmjY1OaJtpUlP5mhcCYEPeH0d1okZzAvW8vCdujiJZtL2VLySE7WsghVgiMCXFJCbE8fGlvNhQf5N53luIJw1FLZ60uQgRGdbfNQk6wQmBMGDircxq/O68rHy0p4E8friDcpv7Oyd9Nj1bJNGkU53aUsGRHDRkTJm47uyN7Dx/huTn5NG4Yy29Ghcc4/WUVVSzcspdrB7VzO0rYskJgTBj5/XldKT1Uwf/OWk9Kg1huGtbhuO33l1WQlBAboHSnZtGWvRyp9DCoQ1O3o4Qt2zRkTBgRER66uBeje7Xgrx+v4h+z1nHoyI/nNNhacohbX82j15+n8eK8jS4krbuc/N1ECQzITHU7StiyHoExYSY6Sph4ZV+qPIv4+7S1vDhvI9efkcl1Z7QjITaaZ+ds4JnZGxCBvm0b8+DUlXg8ys1nHr/34Jb5+bvp0SqFlAbB3XMJZVYIjAlD8THRPHdNNnmb9/DM7PVMnLGWyXM3kNwglsLSMs7v3ZL7R3cjLbaAiT0AAAxjSURBVCmeX7+xmIc+WUWVKree1dHt6D9QVlHF4i17ue4M2z/gJCsExoSx/u2a8MJ1A1i9Yx/PzcmnsPQwT17Rl8Edv9/e/tS4vkRFCY98upoqj/LL4Z1cTPxDC7fs4UiV5wd5jf9ZITAmAnRtkczEK/vWuC4mOoqJV/QhWryT4MTHRJ1wJ3Og5Gzw7h/Ibm/7B5xkhcAYQ0x0FE9c0ZeyCg9/+2QVXVskMzSrmduxyMkvoWfrFJKD/MimUGdHDRljAO9O5ieu6EOn9ERuf30hW0sO1diuvDIww1gcPlLF4q177bDRALBCYIz5TqP4GJ67JptKj3LrlLwfjF20addBrn9pAX3/Mj0gA9x9t3/ACoHjHCsEIpIgIgtEZImIrBCRv9TQpp2IzBSRpSIyW0TaOJXHGFM3mc0a8dS4vqws3PfddJhPTlvDqIlzyd20h5QGsdz6ah4795U5muPo+QPZ7Zs4+jzG2R5BOTBCVfsAfYHzRGTQMW3+Dryiqr2BCcDDDuYxxtTRiK7Nueuczry/aDuDH57J07PW89NeLZj1m7N4efxADpRX8vNX8xwd7TQnfze9WqcE/ZnP4cCxQqBeB3w3Y32XY0fC6g7M9F3/ArjQqTzGmJPzq+GdGNunFS1TGvD6zYN4alw/0pMT6NIiiSev6MPirXu5//3ljgxw993+ATtsNCAcPWpIRKKBPKAT8H+q+s0xTZYAlwJPARcDSSLSVFV3H/M4twC3AGRkZDgZ2RjjExUlPH1VvxrXndezJXeOzOKpmevo0SqZ8UP9O31k3uY9VFSp7SgOEEd3Fqtqlar2BdoAA0Wk5zFN7gHOEpFFwFnAduBHA6Oo6mRVzVbV7LS0NCcjG2Pq6M6RWYzq3pyHPlnFpBlr2XPwSI3tqjzKgfIfj3dUE1Vla8kh3l24jegoYYCdPxAQATmPQFX3ishs4DxgebXlBcAlACKSCFyqqqWByGSMqZ+oKOHJK/ty95uLmTRjHc/NyefKAW25cWgmKQ1jmbu2mFmri5i9ppiSg0do4dus1LVFEh3TEhGBiiqlospDWUUVKwr2sWBjCTt8O6HP7pJGYryd6hQIjr3LIpIGVPiKQAPgHODRY9o0A0pU1QPcB/zLqTzGGP9LjI9h8rXZrN25n+fm5DMlZzOv5mxGgEqP0rhhLMO7pNMpPZH1RQdYvWM/8zfs5kiV50ePlZ4Uz8DMVE7PTGVgZlOy0hMD/4IilJPltiXwsm8/QRTwlqpOFZEJQK6qfgicDTwsIgrMBX7pYB5jjEM6N0/iiSv68JtRnXntm814FEZ2TadfRhOio+QHbSuqPBTuLUME4mKiiI2OIjZaSIyPQURqeQbjJAm1Ke2ys7M1NzfX7RjGGBNSRCRPVbNrWmdnFhtjTISzQmCMMRHOCoExxkQ4KwTGGBPhrBAYY0yEs0JgjDERzgqBMcZEOCsExhgT4ULuhDIRKQb2AseOSZRSbVnKMeuP3q6pTTNg1ylEOfY5TqZNbflqun2i607lr2v2mpadKHN9sx8v34nWnyh/uH92qi+zz87JrT+Zz45b39vjtclS1ZQa76GqIXcBJh9v2bHrj96uqQ3e4S78kqGubWrLd6LXEsj8dc1+Kvnrm93J/OH+2TlmmX12HPrsuPW9Pdn8Ry+humnooxMsO3b9R3Vo448MdW1TW76abtfl+qk40f3rmr2mZSfKXN/sdXmMU80f7p+dYH7vj70davmD4Xt7vDa13jfkNg35m4jkai3jb4SCUM4fytnB8rsplLND8OUP1R6BP012O0A9hXL+UM4Olt9NoZwdgix/xPcIjDEm0lmPwBhjIpwVAmOMiXBhVQhE5F8iUiQiy0/c+kf37S8iy0RkvYg8LdWmShKR20VkjYisEJHH/Jv6u+fwe3YR+bOIbBeRxb7LaP8n/y6DI++9b/09IqK+qU0d4dD7/6CILPW999NEpJX/kzuW/XERWe3L/76INPZ/8u8yOJH/ct/31SMift8pW5/MtTzedSKyzne5rtry4343/OZUj8UNxgtwJnAasPwU7rsAGAwI8CnwU9/y4cAMIN53Oz2Esv8ZuCdU33vfurbA58BmoFko5QeSq7W5A3g2hLKPAmJ81x8FHg2x974b0AWYDWQHS2ZfnvbHLEsF8n3/NvFdb3K81+fvS1j1CFR1LlBSfZmIdBSRz0QkT0S+FJGux95PRFri/dLOV++7/wpwkW/1bcAjqlrue46iEMoeMA7mnwj8FnD0qAYn8qvqvmpNG+HQa3Ao+zRVrfQ1zQHaOJHdwfyrVHVNsGWuxbnAdFUtUdU9wHTgvEB+t8OqENRiMnC7qvYH7gH+WUOb1sC2are3+ZYBdAaGicg3IjJHRAY4mvaH6psd4Fe+7v2/RKSJc1FrVK/8IjIW2K6qS5wOWot6v/8i8pCIbAX+B3jAwazH8sdn56jxeH+NBpI/8wdKXTLXpDWwtdrto68jYK8vxokHDRYikgicAbxdbdNafE1Na1h29NdbDN7u2iBgAPCWiHTwVWjH+Cn7M8CDvtsPAk/g/VI7rr75RaQhcD/eTRQB56f3H1W9H7hfRO4DfgX8yc9RfxzIT9l9j3U/UAm85s+Mx+PP/IFyvMwicgNwp29ZJ+ATETkCbFTVi6n9dQTs9YV1IcDb49mrqn2rLxSRaCDPd/NDvH8wq3d92wAFvuvbgPd8f/gXiIgH74BRxU4Gxw/ZVXVntfs9D0x1MvAx6pu/I5AJLPF9sdoAC0VkoKrucDg7+OezU91/gI8JQCHAT9l9Oy3HACOd/uFzDH+/94FQY2YAVX0JeAlARGYD16vqpmpNtgFnV7vdBu++hG0E6vU5sePBzQvQnmo7cICvgct91wXoU8v9vsX7q//oTpnRvuW3AhN81zvj7cJJiGRvWa3NXcAbofTeH9NmEw7uLHbo/c+q1uZ24J0Qyn4esBJIc/I9d/qzg0M7i081M7XvLN6Id8tDE9/11Lq8Pr+9lkD8JwfqArwOFAIVeKvpjXh/VX4GLPF9sB+o5b7ZwHJgA/APvj/rOg6Y4lu3EBgRQtlfBZYBS/H+gmrpRHan8h/TZhPOHjXkxPv/rm/5UrwDfrUOoezr8f7oWey7OHLEk4P5L/Y9VjmwE/g8GDJTQyHwLR/ve8/XAzeczHfDHxcbYsIYYyJcJBw1ZIwx5jisEBhjTISzQmCMMRHOCoExxkQ4KwTGGBPhrBCYkCciBwL8fC+ISHc/PVaVeEcnXS4iH51olE8RaSwiv/DHcxtzlB0+akKeiBxQ1UQ/Pl6Mfj/gmqOqZxeRl4G1qvrQcdq3B6aqas9A5DORwXoEJiyJSJqIvCsi3/ouQ3zLB4rI1yKyyPdvF9/y60XkbRH5CJgmImeLyGwReUe84/K/dnQseN/ybN/1A76B5ZaISI6INPct7+i7/a2ITKhjr2U+3w+4lygiM0VkoXjHo7/Q1+YRoKOvF/G4r+29vudZKiJ/8ePbaCKEFQITrp4CJqrqAOBS4AXf8tXAmaraD+9ooH+rdp/BwHWqOsJ3ux/wa6A70AEYUsPzNAJyVLUPMBe4udrzP+V7/hOOD+MbR2ck3jPAAcqAi1X1NLxzYjzhK0S/Bzaoal9VvVdERgFZwECgL9BfRM480fMZU124DzpnItc5QPdqI0Emi0gSkAK8LCJZeEdyjK12n+mqWn2M+QWqug1ARBbjHVtm3jHPc4TvB/PLA37iuz6Y78eO/w/w91pyNqj22Hl4x6IH79gyf/P9Uffg7Sk0r+H+o3yXRb7biXgLw9xans+YH7FCYMJVFDBYVQ9XXygi/wt8oaoX+7a3z662+uAxj1Fe7XoVNX9fKvT7HW21tTmew6raV0RS8BaUXwJP452/IA3or6oVIrIJSKjh/gI8rKrPneTzGvMd2zRkwtU0vOP/AyAiR4cHTgG2+65f7+Dz5+DdJAUw7kSNVbUU73SW94hILN6cRb4iMBxo52u6H0iqdtfPgfG+8fARkdYiku6n12AihBUCEw4aisi2ape78f5RzfbtQF2JdzhxgMeAh0XkKyDawUy/Bu4WkQVAS6D0RHdQ1UV4R64ch3cimGwRycXbO1jta7Mb+Mp3uOnjqjoN76an+SKyDHiHHxYKY07IDh81xgG+GdYOq6qKyDjgKlW98ET3M8YNto/AGGf0B/7hO9JnLwGaItSYU2E9AmOMiXC2j8AYYyKcFQJjjIlwVgiMMSbCWSEwxpgIZ4XAGGMi3P8HpUukbE0rp7gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find(end_lr=100)\n",
    "\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='285' class='' max='600', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      47.50% [285/600 7:35:42<8:23:40]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.439584</td>\n",
       "      <td>1.680075</td>\n",
       "      <td>0.474815</td>\n",
       "      <td>0.525185</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.107430</td>\n",
       "      <td>1.367144</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.985620</td>\n",
       "      <td>1.130047</td>\n",
       "      <td>0.321481</td>\n",
       "      <td>0.678519</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.794244</td>\n",
       "      <td>0.959966</td>\n",
       "      <td>0.278148</td>\n",
       "      <td>0.721852</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.744379</td>\n",
       "      <td>0.917023</td>\n",
       "      <td>0.272963</td>\n",
       "      <td>0.727037</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.623210</td>\n",
       "      <td>0.781989</td>\n",
       "      <td>0.227407</td>\n",
       "      <td>0.772593</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.591066</td>\n",
       "      <td>0.729900</td>\n",
       "      <td>0.213333</td>\n",
       "      <td>0.786667</td>\n",
       "      <td>01:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.534815</td>\n",
       "      <td>0.698907</td>\n",
       "      <td>0.198519</td>\n",
       "      <td>0.801481</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.487468</td>\n",
       "      <td>0.708731</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.437526</td>\n",
       "      <td>0.615955</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.392238</td>\n",
       "      <td>0.602285</td>\n",
       "      <td>0.174074</td>\n",
       "      <td>0.825926</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.405445</td>\n",
       "      <td>0.629937</td>\n",
       "      <td>0.169259</td>\n",
       "      <td>0.830741</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.357286</td>\n",
       "      <td>0.566579</td>\n",
       "      <td>0.160741</td>\n",
       "      <td>0.839259</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.346992</td>\n",
       "      <td>0.529988</td>\n",
       "      <td>0.146667</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.307061</td>\n",
       "      <td>0.496193</td>\n",
       "      <td>0.135556</td>\n",
       "      <td>0.864444</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.307546</td>\n",
       "      <td>0.503068</td>\n",
       "      <td>0.136296</td>\n",
       "      <td>0.863704</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.274701</td>\n",
       "      <td>0.472524</td>\n",
       "      <td>0.128889</td>\n",
       "      <td>0.871111</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.256042</td>\n",
       "      <td>0.507053</td>\n",
       "      <td>0.132963</td>\n",
       "      <td>0.867037</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.257564</td>\n",
       "      <td>0.479332</td>\n",
       "      <td>0.127037</td>\n",
       "      <td>0.872963</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.237195</td>\n",
       "      <td>0.503417</td>\n",
       "      <td>0.141111</td>\n",
       "      <td>0.858889</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.233343</td>\n",
       "      <td>0.462906</td>\n",
       "      <td>0.117407</td>\n",
       "      <td>0.882593</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.217951</td>\n",
       "      <td>0.457327</td>\n",
       "      <td>0.120370</td>\n",
       "      <td>0.879630</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.226257</td>\n",
       "      <td>0.441498</td>\n",
       "      <td>0.112222</td>\n",
       "      <td>0.887778</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.209432</td>\n",
       "      <td>0.545847</td>\n",
       "      <td>0.144074</td>\n",
       "      <td>0.855926</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.212506</td>\n",
       "      <td>0.437759</td>\n",
       "      <td>0.114074</td>\n",
       "      <td>0.885926</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.190347</td>\n",
       "      <td>0.440548</td>\n",
       "      <td>0.114815</td>\n",
       "      <td>0.885185</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.174392</td>\n",
       "      <td>0.405784</td>\n",
       "      <td>0.113704</td>\n",
       "      <td>0.886296</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.199785</td>\n",
       "      <td>0.420747</td>\n",
       "      <td>0.109630</td>\n",
       "      <td>0.890370</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.153092</td>\n",
       "      <td>0.403278</td>\n",
       "      <td>0.102963</td>\n",
       "      <td>0.897037</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.159887</td>\n",
       "      <td>0.403986</td>\n",
       "      <td>0.113333</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.170382</td>\n",
       "      <td>0.405314</td>\n",
       "      <td>0.102963</td>\n",
       "      <td>0.897037</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.170389</td>\n",
       "      <td>0.415593</td>\n",
       "      <td>0.109259</td>\n",
       "      <td>0.890741</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.160897</td>\n",
       "      <td>0.397750</td>\n",
       "      <td>0.107037</td>\n",
       "      <td>0.892963</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.146653</td>\n",
       "      <td>0.387972</td>\n",
       "      <td>0.105556</td>\n",
       "      <td>0.894444</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.106930</td>\n",
       "      <td>0.356007</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.131025</td>\n",
       "      <td>0.372255</td>\n",
       "      <td>0.095556</td>\n",
       "      <td>0.904444</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.105895</td>\n",
       "      <td>0.384392</td>\n",
       "      <td>0.094074</td>\n",
       "      <td>0.905926</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.133774</td>\n",
       "      <td>0.399376</td>\n",
       "      <td>0.097037</td>\n",
       "      <td>0.902963</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.095849</td>\n",
       "      <td>0.334785</td>\n",
       "      <td>0.084074</td>\n",
       "      <td>0.915926</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.111865</td>\n",
       "      <td>0.372632</td>\n",
       "      <td>0.105926</td>\n",
       "      <td>0.894074</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.092614</td>\n",
       "      <td>0.342164</td>\n",
       "      <td>0.087037</td>\n",
       "      <td>0.912963</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.083388</td>\n",
       "      <td>0.337450</td>\n",
       "      <td>0.085185</td>\n",
       "      <td>0.914815</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.097926</td>\n",
       "      <td>0.344943</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.086643</td>\n",
       "      <td>0.368310</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.080686</td>\n",
       "      <td>0.347009</td>\n",
       "      <td>0.085556</td>\n",
       "      <td>0.914444</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.079190</td>\n",
       "      <td>0.321903</td>\n",
       "      <td>0.080370</td>\n",
       "      <td>0.919630</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.069464</td>\n",
       "      <td>0.352645</td>\n",
       "      <td>0.095926</td>\n",
       "      <td>0.904074</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.057363</td>\n",
       "      <td>0.364259</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.062615</td>\n",
       "      <td>0.326832</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.060762</td>\n",
       "      <td>0.340256</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.066463</td>\n",
       "      <td>0.337158</td>\n",
       "      <td>0.082222</td>\n",
       "      <td>0.917778</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.041028</td>\n",
       "      <td>0.334063</td>\n",
       "      <td>0.091481</td>\n",
       "      <td>0.908518</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.052310</td>\n",
       "      <td>0.299922</td>\n",
       "      <td>0.078519</td>\n",
       "      <td>0.921481</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.032537</td>\n",
       "      <td>0.305563</td>\n",
       "      <td>0.076296</td>\n",
       "      <td>0.923704</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.042817</td>\n",
       "      <td>0.285498</td>\n",
       "      <td>0.070741</td>\n",
       "      <td>0.929259</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.015907</td>\n",
       "      <td>0.362901</td>\n",
       "      <td>0.091111</td>\n",
       "      <td>0.908889</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.024362</td>\n",
       "      <td>0.352316</td>\n",
       "      <td>0.082963</td>\n",
       "      <td>0.917037</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.046163</td>\n",
       "      <td>0.332564</td>\n",
       "      <td>0.079630</td>\n",
       "      <td>0.920370</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.024819</td>\n",
       "      <td>0.315196</td>\n",
       "      <td>0.076296</td>\n",
       "      <td>0.923704</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.015574</td>\n",
       "      <td>0.298531</td>\n",
       "      <td>0.074815</td>\n",
       "      <td>0.925185</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.992300</td>\n",
       "      <td>0.286506</td>\n",
       "      <td>0.067778</td>\n",
       "      <td>0.932222</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.016650</td>\n",
       "      <td>0.337510</td>\n",
       "      <td>0.079630</td>\n",
       "      <td>0.920370</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.008631</td>\n",
       "      <td>0.295677</td>\n",
       "      <td>0.070370</td>\n",
       "      <td>0.929630</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.011446</td>\n",
       "      <td>0.325483</td>\n",
       "      <td>0.075926</td>\n",
       "      <td>0.924074</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.002548</td>\n",
       "      <td>0.308164</td>\n",
       "      <td>0.072963</td>\n",
       "      <td>0.927037</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.003150</td>\n",
       "      <td>0.269845</td>\n",
       "      <td>0.066296</td>\n",
       "      <td>0.933704</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.001359</td>\n",
       "      <td>0.295193</td>\n",
       "      <td>0.071852</td>\n",
       "      <td>0.928148</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.997880</td>\n",
       "      <td>0.301277</td>\n",
       "      <td>0.074444</td>\n",
       "      <td>0.925556</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.981604</td>\n",
       "      <td>0.292994</td>\n",
       "      <td>0.072963</td>\n",
       "      <td>0.927037</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.984483</td>\n",
       "      <td>0.326774</td>\n",
       "      <td>0.079259</td>\n",
       "      <td>0.920741</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.998265</td>\n",
       "      <td>0.340831</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.006249</td>\n",
       "      <td>0.357762</td>\n",
       "      <td>0.086667</td>\n",
       "      <td>0.913333</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.979212</td>\n",
       "      <td>0.265331</td>\n",
       "      <td>0.062222</td>\n",
       "      <td>0.937778</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.971409</td>\n",
       "      <td>0.309019</td>\n",
       "      <td>0.074444</td>\n",
       "      <td>0.925556</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.958159</td>\n",
       "      <td>0.333806</td>\n",
       "      <td>0.092963</td>\n",
       "      <td>0.907037</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.993929</td>\n",
       "      <td>0.266831</td>\n",
       "      <td>0.064444</td>\n",
       "      <td>0.935556</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.968848</td>\n",
       "      <td>0.300552</td>\n",
       "      <td>0.070370</td>\n",
       "      <td>0.929630</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.978277</td>\n",
       "      <td>0.275854</td>\n",
       "      <td>0.064815</td>\n",
       "      <td>0.935185</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.972453</td>\n",
       "      <td>0.299296</td>\n",
       "      <td>0.074444</td>\n",
       "      <td>0.925556</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.961020</td>\n",
       "      <td>0.319189</td>\n",
       "      <td>0.087407</td>\n",
       "      <td>0.912593</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.941908</td>\n",
       "      <td>0.282755</td>\n",
       "      <td>0.071852</td>\n",
       "      <td>0.928148</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.958671</td>\n",
       "      <td>0.276985</td>\n",
       "      <td>0.070741</td>\n",
       "      <td>0.929259</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.964919</td>\n",
       "      <td>0.274636</td>\n",
       "      <td>0.064074</td>\n",
       "      <td>0.935926</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.951953</td>\n",
       "      <td>0.297657</td>\n",
       "      <td>0.081481</td>\n",
       "      <td>0.918519</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.949753</td>\n",
       "      <td>0.267741</td>\n",
       "      <td>0.061111</td>\n",
       "      <td>0.938889</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.942319</td>\n",
       "      <td>0.286194</td>\n",
       "      <td>0.069630</td>\n",
       "      <td>0.930370</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.949025</td>\n",
       "      <td>0.267461</td>\n",
       "      <td>0.063333</td>\n",
       "      <td>0.936667</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.941125</td>\n",
       "      <td>0.305209</td>\n",
       "      <td>0.074815</td>\n",
       "      <td>0.925185</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.946298</td>\n",
       "      <td>0.287646</td>\n",
       "      <td>0.067037</td>\n",
       "      <td>0.932963</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.957568</td>\n",
       "      <td>0.295605</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.980729</td>\n",
       "      <td>0.331835</td>\n",
       "      <td>0.075926</td>\n",
       "      <td>0.924074</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.952714</td>\n",
       "      <td>0.285868</td>\n",
       "      <td>0.071852</td>\n",
       "      <td>0.928148</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.944065</td>\n",
       "      <td>0.272346</td>\n",
       "      <td>0.065926</td>\n",
       "      <td>0.934074</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.927404</td>\n",
       "      <td>0.292230</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.943491</td>\n",
       "      <td>0.260803</td>\n",
       "      <td>0.064815</td>\n",
       "      <td>0.935185</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.944797</td>\n",
       "      <td>0.289353</td>\n",
       "      <td>0.071481</td>\n",
       "      <td>0.928519</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.947875</td>\n",
       "      <td>0.267182</td>\n",
       "      <td>0.065185</td>\n",
       "      <td>0.934815</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.925933</td>\n",
       "      <td>0.272477</td>\n",
       "      <td>0.063333</td>\n",
       "      <td>0.936667</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.926936</td>\n",
       "      <td>0.283646</td>\n",
       "      <td>0.071852</td>\n",
       "      <td>0.928148</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.917864</td>\n",
       "      <td>0.255301</td>\n",
       "      <td>0.056296</td>\n",
       "      <td>0.943704</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.936678</td>\n",
       "      <td>0.285409</td>\n",
       "      <td>0.067778</td>\n",
       "      <td>0.932222</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.928702</td>\n",
       "      <td>0.296323</td>\n",
       "      <td>0.076296</td>\n",
       "      <td>0.923704</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.947696</td>\n",
       "      <td>0.284816</td>\n",
       "      <td>0.067407</td>\n",
       "      <td>0.932593</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.902186</td>\n",
       "      <td>0.274106</td>\n",
       "      <td>0.064815</td>\n",
       "      <td>0.935185</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.928548</td>\n",
       "      <td>0.280356</td>\n",
       "      <td>0.063333</td>\n",
       "      <td>0.936667</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.928426</td>\n",
       "      <td>0.340852</td>\n",
       "      <td>0.085926</td>\n",
       "      <td>0.914074</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.931339</td>\n",
       "      <td>0.249241</td>\n",
       "      <td>0.059630</td>\n",
       "      <td>0.940370</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.922278</td>\n",
       "      <td>0.330308</td>\n",
       "      <td>0.078519</td>\n",
       "      <td>0.921481</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.917406</td>\n",
       "      <td>0.270733</td>\n",
       "      <td>0.064074</td>\n",
       "      <td>0.935926</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.918250</td>\n",
       "      <td>0.274699</td>\n",
       "      <td>0.070370</td>\n",
       "      <td>0.929630</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.899472</td>\n",
       "      <td>0.223338</td>\n",
       "      <td>0.050370</td>\n",
       "      <td>0.949630</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.903797</td>\n",
       "      <td>0.257553</td>\n",
       "      <td>0.062222</td>\n",
       "      <td>0.937778</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.912851</td>\n",
       "      <td>0.262581</td>\n",
       "      <td>0.061852</td>\n",
       "      <td>0.938148</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.914240</td>\n",
       "      <td>0.332477</td>\n",
       "      <td>0.080741</td>\n",
       "      <td>0.919259</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.900390</td>\n",
       "      <td>0.288169</td>\n",
       "      <td>0.069259</td>\n",
       "      <td>0.930741</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.905277</td>\n",
       "      <td>0.284367</td>\n",
       "      <td>0.071111</td>\n",
       "      <td>0.928889</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.885366</td>\n",
       "      <td>0.303705</td>\n",
       "      <td>0.068519</td>\n",
       "      <td>0.931481</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.907530</td>\n",
       "      <td>0.245985</td>\n",
       "      <td>0.055926</td>\n",
       "      <td>0.944074</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.921534</td>\n",
       "      <td>0.257586</td>\n",
       "      <td>0.060741</td>\n",
       "      <td>0.939259</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.921038</td>\n",
       "      <td>0.289583</td>\n",
       "      <td>0.065556</td>\n",
       "      <td>0.934444</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.901574</td>\n",
       "      <td>0.301727</td>\n",
       "      <td>0.068519</td>\n",
       "      <td>0.931481</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.896484</td>\n",
       "      <td>0.287389</td>\n",
       "      <td>0.074444</td>\n",
       "      <td>0.925556</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.902865</td>\n",
       "      <td>0.258515</td>\n",
       "      <td>0.063333</td>\n",
       "      <td>0.936667</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.899782</td>\n",
       "      <td>0.277237</td>\n",
       "      <td>0.071111</td>\n",
       "      <td>0.928889</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.890722</td>\n",
       "      <td>0.261508</td>\n",
       "      <td>0.058889</td>\n",
       "      <td>0.941111</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.904202</td>\n",
       "      <td>0.249429</td>\n",
       "      <td>0.062593</td>\n",
       "      <td>0.937407</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.890976</td>\n",
       "      <td>0.239214</td>\n",
       "      <td>0.062222</td>\n",
       "      <td>0.937778</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.908833</td>\n",
       "      <td>0.274280</td>\n",
       "      <td>0.063704</td>\n",
       "      <td>0.936296</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.913765</td>\n",
       "      <td>0.307081</td>\n",
       "      <td>0.070370</td>\n",
       "      <td>0.929630</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.890050</td>\n",
       "      <td>0.321652</td>\n",
       "      <td>0.072222</td>\n",
       "      <td>0.927778</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.881414</td>\n",
       "      <td>0.258916</td>\n",
       "      <td>0.065185</td>\n",
       "      <td>0.934815</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.897445</td>\n",
       "      <td>0.295608</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.894236</td>\n",
       "      <td>0.287068</td>\n",
       "      <td>0.062222</td>\n",
       "      <td>0.937778</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.887414</td>\n",
       "      <td>0.254226</td>\n",
       "      <td>0.057037</td>\n",
       "      <td>0.942963</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.888212</td>\n",
       "      <td>0.254146</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.888953</td>\n",
       "      <td>0.251796</td>\n",
       "      <td>0.062222</td>\n",
       "      <td>0.937778</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.872592</td>\n",
       "      <td>0.255315</td>\n",
       "      <td>0.055926</td>\n",
       "      <td>0.944074</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.872191</td>\n",
       "      <td>0.259402</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.875060</td>\n",
       "      <td>0.247562</td>\n",
       "      <td>0.057778</td>\n",
       "      <td>0.942222</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.874504</td>\n",
       "      <td>0.281579</td>\n",
       "      <td>0.066296</td>\n",
       "      <td>0.933704</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.886062</td>\n",
       "      <td>0.253225</td>\n",
       "      <td>0.058519</td>\n",
       "      <td>0.941481</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.887891</td>\n",
       "      <td>0.273507</td>\n",
       "      <td>0.065556</td>\n",
       "      <td>0.934444</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.877825</td>\n",
       "      <td>0.251755</td>\n",
       "      <td>0.054815</td>\n",
       "      <td>0.945185</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.875673</td>\n",
       "      <td>0.255925</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.871250</td>\n",
       "      <td>0.261086</td>\n",
       "      <td>0.055185</td>\n",
       "      <td>0.944815</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.884799</td>\n",
       "      <td>0.254984</td>\n",
       "      <td>0.055926</td>\n",
       "      <td>0.944074</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.881450</td>\n",
       "      <td>0.272156</td>\n",
       "      <td>0.058519</td>\n",
       "      <td>0.941481</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.873688</td>\n",
       "      <td>0.242903</td>\n",
       "      <td>0.054444</td>\n",
       "      <td>0.945556</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.885904</td>\n",
       "      <td>0.252986</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.896186</td>\n",
       "      <td>0.237739</td>\n",
       "      <td>0.058889</td>\n",
       "      <td>0.941111</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.874481</td>\n",
       "      <td>0.253561</td>\n",
       "      <td>0.052222</td>\n",
       "      <td>0.947778</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.871846</td>\n",
       "      <td>0.283087</td>\n",
       "      <td>0.065556</td>\n",
       "      <td>0.934444</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.865264</td>\n",
       "      <td>0.259502</td>\n",
       "      <td>0.060741</td>\n",
       "      <td>0.939259</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.859767</td>\n",
       "      <td>0.246009</td>\n",
       "      <td>0.057778</td>\n",
       "      <td>0.942222</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.859541</td>\n",
       "      <td>0.260478</td>\n",
       "      <td>0.058889</td>\n",
       "      <td>0.941111</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.865562</td>\n",
       "      <td>0.274692</td>\n",
       "      <td>0.062222</td>\n",
       "      <td>0.937778</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.877264</td>\n",
       "      <td>0.270593</td>\n",
       "      <td>0.068148</td>\n",
       "      <td>0.931852</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.879804</td>\n",
       "      <td>0.275490</td>\n",
       "      <td>0.062222</td>\n",
       "      <td>0.937778</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.863981</td>\n",
       "      <td>0.262051</td>\n",
       "      <td>0.058519</td>\n",
       "      <td>0.941481</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.843361</td>\n",
       "      <td>0.269631</td>\n",
       "      <td>0.059630</td>\n",
       "      <td>0.940370</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.853519</td>\n",
       "      <td>0.263092</td>\n",
       "      <td>0.059630</td>\n",
       "      <td>0.940370</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.860657</td>\n",
       "      <td>0.269993</td>\n",
       "      <td>0.063704</td>\n",
       "      <td>0.936296</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.842231</td>\n",
       "      <td>0.282639</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.860686</td>\n",
       "      <td>0.234426</td>\n",
       "      <td>0.045556</td>\n",
       "      <td>0.954444</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.868702</td>\n",
       "      <td>0.260922</td>\n",
       "      <td>0.052963</td>\n",
       "      <td>0.947037</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.836533</td>\n",
       "      <td>0.278575</td>\n",
       "      <td>0.062593</td>\n",
       "      <td>0.937407</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.849729</td>\n",
       "      <td>0.247395</td>\n",
       "      <td>0.060370</td>\n",
       "      <td>0.939630</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.853043</td>\n",
       "      <td>0.261299</td>\n",
       "      <td>0.061852</td>\n",
       "      <td>0.938148</td>\n",
       "      <td>01:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.850771</td>\n",
       "      <td>0.267845</td>\n",
       "      <td>0.058889</td>\n",
       "      <td>0.941111</td>\n",
       "      <td>01:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.854339</td>\n",
       "      <td>0.252556</td>\n",
       "      <td>0.053704</td>\n",
       "      <td>0.946296</td>\n",
       "      <td>01:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.837771</td>\n",
       "      <td>0.246840</td>\n",
       "      <td>0.058148</td>\n",
       "      <td>0.941852</td>\n",
       "      <td>01:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.855738</td>\n",
       "      <td>0.243921</td>\n",
       "      <td>0.051852</td>\n",
       "      <td>0.948148</td>\n",
       "      <td>01:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.830578</td>\n",
       "      <td>0.286888</td>\n",
       "      <td>0.068519</td>\n",
       "      <td>0.931481</td>\n",
       "      <td>01:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.852476</td>\n",
       "      <td>0.280668</td>\n",
       "      <td>0.061111</td>\n",
       "      <td>0.938889</td>\n",
       "      <td>01:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.832406</td>\n",
       "      <td>0.252439</td>\n",
       "      <td>0.055185</td>\n",
       "      <td>0.944815</td>\n",
       "      <td>01:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.833631</td>\n",
       "      <td>0.277544</td>\n",
       "      <td>0.064815</td>\n",
       "      <td>0.935185</td>\n",
       "      <td>01:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.860452</td>\n",
       "      <td>0.272723</td>\n",
       "      <td>0.060370</td>\n",
       "      <td>0.939630</td>\n",
       "      <td>01:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.832980</td>\n",
       "      <td>0.251389</td>\n",
       "      <td>0.059630</td>\n",
       "      <td>0.940370</td>\n",
       "      <td>01:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.829044</td>\n",
       "      <td>0.266150</td>\n",
       "      <td>0.057778</td>\n",
       "      <td>0.942222</td>\n",
       "      <td>01:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.862247</td>\n",
       "      <td>0.253798</td>\n",
       "      <td>0.059630</td>\n",
       "      <td>0.940370</td>\n",
       "      <td>01:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.830607</td>\n",
       "      <td>0.249871</td>\n",
       "      <td>0.052963</td>\n",
       "      <td>0.947037</td>\n",
       "      <td>01:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.833126</td>\n",
       "      <td>0.247885</td>\n",
       "      <td>0.051852</td>\n",
       "      <td>0.948148</td>\n",
       "      <td>01:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.839116</td>\n",
       "      <td>0.242769</td>\n",
       "      <td>0.052593</td>\n",
       "      <td>0.947407</td>\n",
       "      <td>01:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.841330</td>\n",
       "      <td>0.263207</td>\n",
       "      <td>0.053704</td>\n",
       "      <td>0.946296</td>\n",
       "      <td>01:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.849777</td>\n",
       "      <td>0.238889</td>\n",
       "      <td>0.056296</td>\n",
       "      <td>0.943704</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.829335</td>\n",
       "      <td>0.261908</td>\n",
       "      <td>0.057037</td>\n",
       "      <td>0.942963</td>\n",
       "      <td>01:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.831312</td>\n",
       "      <td>0.254687</td>\n",
       "      <td>0.048889</td>\n",
       "      <td>0.951111</td>\n",
       "      <td>01:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.831287</td>\n",
       "      <td>0.239585</td>\n",
       "      <td>0.054444</td>\n",
       "      <td>0.945556</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.823010</td>\n",
       "      <td>0.261080</td>\n",
       "      <td>0.057407</td>\n",
       "      <td>0.942593</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.835472</td>\n",
       "      <td>0.250979</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.820677</td>\n",
       "      <td>0.253925</td>\n",
       "      <td>0.058889</td>\n",
       "      <td>0.941111</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.826672</td>\n",
       "      <td>0.252669</td>\n",
       "      <td>0.054815</td>\n",
       "      <td>0.945185</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.827414</td>\n",
       "      <td>0.252993</td>\n",
       "      <td>0.054444</td>\n",
       "      <td>0.945556</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.821969</td>\n",
       "      <td>0.257378</td>\n",
       "      <td>0.050370</td>\n",
       "      <td>0.949630</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.816688</td>\n",
       "      <td>0.250532</td>\n",
       "      <td>0.054074</td>\n",
       "      <td>0.945926</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.811516</td>\n",
       "      <td>0.248842</td>\n",
       "      <td>0.048889</td>\n",
       "      <td>0.951111</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.832672</td>\n",
       "      <td>0.258541</td>\n",
       "      <td>0.057407</td>\n",
       "      <td>0.942593</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.830801</td>\n",
       "      <td>0.248543</td>\n",
       "      <td>0.054444</td>\n",
       "      <td>0.945556</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.814436</td>\n",
       "      <td>0.259359</td>\n",
       "      <td>0.048519</td>\n",
       "      <td>0.951481</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.815190</td>\n",
       "      <td>0.252228</td>\n",
       "      <td>0.050370</td>\n",
       "      <td>0.949630</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.829255</td>\n",
       "      <td>0.225750</td>\n",
       "      <td>0.049259</td>\n",
       "      <td>0.950741</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>0.802210</td>\n",
       "      <td>0.294564</td>\n",
       "      <td>0.067407</td>\n",
       "      <td>0.932593</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>0.828500</td>\n",
       "      <td>0.266807</td>\n",
       "      <td>0.054074</td>\n",
       "      <td>0.945926</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>0.817774</td>\n",
       "      <td>0.253489</td>\n",
       "      <td>0.058148</td>\n",
       "      <td>0.941852</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>0.794576</td>\n",
       "      <td>0.229336</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.826436</td>\n",
       "      <td>0.246948</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>0.798392</td>\n",
       "      <td>0.273967</td>\n",
       "      <td>0.056667</td>\n",
       "      <td>0.943333</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>0.812910</td>\n",
       "      <td>0.224506</td>\n",
       "      <td>0.051111</td>\n",
       "      <td>0.948889</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>0.808099</td>\n",
       "      <td>0.255794</td>\n",
       "      <td>0.054074</td>\n",
       "      <td>0.945926</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>0.818910</td>\n",
       "      <td>0.224038</td>\n",
       "      <td>0.048519</td>\n",
       "      <td>0.951481</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.799876</td>\n",
       "      <td>0.237629</td>\n",
       "      <td>0.050741</td>\n",
       "      <td>0.949259</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>0.806621</td>\n",
       "      <td>0.223459</td>\n",
       "      <td>0.051852</td>\n",
       "      <td>0.948148</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>0.794499</td>\n",
       "      <td>0.231892</td>\n",
       "      <td>0.052593</td>\n",
       "      <td>0.947407</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>0.795265</td>\n",
       "      <td>0.212152</td>\n",
       "      <td>0.052222</td>\n",
       "      <td>0.947778</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>0.796762</td>\n",
       "      <td>0.234861</td>\n",
       "      <td>0.056667</td>\n",
       "      <td>0.943333</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.821224</td>\n",
       "      <td>0.246394</td>\n",
       "      <td>0.055185</td>\n",
       "      <td>0.944815</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>0.814930</td>\n",
       "      <td>0.267898</td>\n",
       "      <td>0.052222</td>\n",
       "      <td>0.947778</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>0.804526</td>\n",
       "      <td>0.228617</td>\n",
       "      <td>0.048148</td>\n",
       "      <td>0.951852</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>0.805482</td>\n",
       "      <td>0.241895</td>\n",
       "      <td>0.052593</td>\n",
       "      <td>0.947407</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>0.807483</td>\n",
       "      <td>0.230656</td>\n",
       "      <td>0.047037</td>\n",
       "      <td>0.952963</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.803340</td>\n",
       "      <td>0.228814</td>\n",
       "      <td>0.047037</td>\n",
       "      <td>0.952963</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>0.814141</td>\n",
       "      <td>0.229015</td>\n",
       "      <td>0.049630</td>\n",
       "      <td>0.950370</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>0.803286</td>\n",
       "      <td>0.217909</td>\n",
       "      <td>0.047778</td>\n",
       "      <td>0.952222</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>0.791919</td>\n",
       "      <td>0.235450</td>\n",
       "      <td>0.048148</td>\n",
       "      <td>0.951852</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.807685</td>\n",
       "      <td>0.239919</td>\n",
       "      <td>0.052222</td>\n",
       "      <td>0.947778</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.784018</td>\n",
       "      <td>0.224468</td>\n",
       "      <td>0.050370</td>\n",
       "      <td>0.949630</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>0.793790</td>\n",
       "      <td>0.252157</td>\n",
       "      <td>0.053704</td>\n",
       "      <td>0.946296</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>0.792523</td>\n",
       "      <td>0.233889</td>\n",
       "      <td>0.048889</td>\n",
       "      <td>0.951111</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>0.807995</td>\n",
       "      <td>0.244437</td>\n",
       "      <td>0.051852</td>\n",
       "      <td>0.948148</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>0.795471</td>\n",
       "      <td>0.234640</td>\n",
       "      <td>0.044815</td>\n",
       "      <td>0.955185</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.799073</td>\n",
       "      <td>0.217341</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>0.792352</td>\n",
       "      <td>0.231292</td>\n",
       "      <td>0.047778</td>\n",
       "      <td>0.952222</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>0.787249</td>\n",
       "      <td>0.242079</td>\n",
       "      <td>0.051481</td>\n",
       "      <td>0.948519</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.795045</td>\n",
       "      <td>0.217170</td>\n",
       "      <td>0.049259</td>\n",
       "      <td>0.950741</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>0.790357</td>\n",
       "      <td>0.211453</td>\n",
       "      <td>0.044815</td>\n",
       "      <td>0.955185</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.790193</td>\n",
       "      <td>0.248335</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>0.775146</td>\n",
       "      <td>0.227694</td>\n",
       "      <td>0.048148</td>\n",
       "      <td>0.951852</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>0.803656</td>\n",
       "      <td>0.225572</td>\n",
       "      <td>0.045926</td>\n",
       "      <td>0.954074</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>0.789921</td>\n",
       "      <td>0.230350</td>\n",
       "      <td>0.048148</td>\n",
       "      <td>0.951852</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>0.792502</td>\n",
       "      <td>0.232645</td>\n",
       "      <td>0.048889</td>\n",
       "      <td>0.951111</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.770643</td>\n",
       "      <td>0.247776</td>\n",
       "      <td>0.053333</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>0.780956</td>\n",
       "      <td>0.255417</td>\n",
       "      <td>0.051481</td>\n",
       "      <td>0.948519</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>0.783656</td>\n",
       "      <td>0.235726</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>0.789359</td>\n",
       "      <td>0.222190</td>\n",
       "      <td>0.047778</td>\n",
       "      <td>0.952222</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>0.780867</td>\n",
       "      <td>0.235161</td>\n",
       "      <td>0.047778</td>\n",
       "      <td>0.952222</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.787495</td>\n",
       "      <td>0.238222</td>\n",
       "      <td>0.048148</td>\n",
       "      <td>0.951852</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>0.778063</td>\n",
       "      <td>0.207485</td>\n",
       "      <td>0.043333</td>\n",
       "      <td>0.956667</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>0.784415</td>\n",
       "      <td>0.217624</td>\n",
       "      <td>0.044815</td>\n",
       "      <td>0.955185</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>0.794757</td>\n",
       "      <td>0.211913</td>\n",
       "      <td>0.040741</td>\n",
       "      <td>0.959259</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>0.788172</td>\n",
       "      <td>0.216446</td>\n",
       "      <td>0.047037</td>\n",
       "      <td>0.952963</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.794432</td>\n",
       "      <td>0.230850</td>\n",
       "      <td>0.044815</td>\n",
       "      <td>0.955185</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>0.778473</td>\n",
       "      <td>0.235615</td>\n",
       "      <td>0.047407</td>\n",
       "      <td>0.952593</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>0.781677</td>\n",
       "      <td>0.219760</td>\n",
       "      <td>0.047407</td>\n",
       "      <td>0.952593</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>0.788405</td>\n",
       "      <td>0.223933</td>\n",
       "      <td>0.049259</td>\n",
       "      <td>0.950741</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>0.779631</td>\n",
       "      <td>0.213565</td>\n",
       "      <td>0.043333</td>\n",
       "      <td>0.956667</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>0.783696</td>\n",
       "      <td>0.233208</td>\n",
       "      <td>0.047407</td>\n",
       "      <td>0.952593</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.776476</td>\n",
       "      <td>0.217016</td>\n",
       "      <td>0.046667</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>0.766402</td>\n",
       "      <td>0.221927</td>\n",
       "      <td>0.044074</td>\n",
       "      <td>0.955926</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>0.764934</td>\n",
       "      <td>0.221629</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>0.776778</td>\n",
       "      <td>0.217498</td>\n",
       "      <td>0.041111</td>\n",
       "      <td>0.958889</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.774470</td>\n",
       "      <td>0.210367</td>\n",
       "      <td>0.041111</td>\n",
       "      <td>0.958889</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>0.792305</td>\n",
       "      <td>0.218320</td>\n",
       "      <td>0.045185</td>\n",
       "      <td>0.954815</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>0.785462</td>\n",
       "      <td>0.278518</td>\n",
       "      <td>0.054815</td>\n",
       "      <td>0.945185</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263</td>\n",
       "      <td>0.764064</td>\n",
       "      <td>0.220967</td>\n",
       "      <td>0.045556</td>\n",
       "      <td>0.954444</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>0.763738</td>\n",
       "      <td>0.216971</td>\n",
       "      <td>0.047407</td>\n",
       "      <td>0.952593</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>0.765326</td>\n",
       "      <td>0.204529</td>\n",
       "      <td>0.045185</td>\n",
       "      <td>0.954815</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>0.782432</td>\n",
       "      <td>0.204407</td>\n",
       "      <td>0.048519</td>\n",
       "      <td>0.951481</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>0.771192</td>\n",
       "      <td>0.211283</td>\n",
       "      <td>0.043333</td>\n",
       "      <td>0.956667</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>0.762172</td>\n",
       "      <td>0.219464</td>\n",
       "      <td>0.048148</td>\n",
       "      <td>0.951852</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269</td>\n",
       "      <td>0.762917</td>\n",
       "      <td>0.226612</td>\n",
       "      <td>0.049630</td>\n",
       "      <td>0.950370</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.761369</td>\n",
       "      <td>0.221596</td>\n",
       "      <td>0.046667</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271</td>\n",
       "      <td>0.776451</td>\n",
       "      <td>0.213712</td>\n",
       "      <td>0.046296</td>\n",
       "      <td>0.953704</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>0.766373</td>\n",
       "      <td>0.233953</td>\n",
       "      <td>0.049630</td>\n",
       "      <td>0.950370</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>0.764977</td>\n",
       "      <td>0.225762</td>\n",
       "      <td>0.044074</td>\n",
       "      <td>0.955926</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>0.760465</td>\n",
       "      <td>0.233254</td>\n",
       "      <td>0.047778</td>\n",
       "      <td>0.952222</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.763228</td>\n",
       "      <td>0.227021</td>\n",
       "      <td>0.050370</td>\n",
       "      <td>0.949630</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>0.749407</td>\n",
       "      <td>0.218264</td>\n",
       "      <td>0.049630</td>\n",
       "      <td>0.950370</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>0.772205</td>\n",
       "      <td>0.221027</td>\n",
       "      <td>0.048148</td>\n",
       "      <td>0.951852</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>0.753513</td>\n",
       "      <td>0.208678</td>\n",
       "      <td>0.040741</td>\n",
       "      <td>0.959259</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>0.767204</td>\n",
       "      <td>0.211058</td>\n",
       "      <td>0.042963</td>\n",
       "      <td>0.957037</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.753346</td>\n",
       "      <td>0.218067</td>\n",
       "      <td>0.042593</td>\n",
       "      <td>0.957407</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281</td>\n",
       "      <td>0.765586</td>\n",
       "      <td>0.237117</td>\n",
       "      <td>0.047407</td>\n",
       "      <td>0.952593</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>0.768614</td>\n",
       "      <td>0.221869</td>\n",
       "      <td>0.049630</td>\n",
       "      <td>0.950370</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>0.750256</td>\n",
       "      <td>0.255972</td>\n",
       "      <td>0.054815</td>\n",
       "      <td>0.945185</td>\n",
       "      <td>01:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>0.769570</td>\n",
       "      <td>0.211920</td>\n",
       "      <td>0.046296</td>\n",
       "      <td>0.953704</td>\n",
       "      <td>01:36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='8' class='' max='379', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      2.11% [8/379 00:03<02:25 0.7624]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with accuracy value: 0.5251851677894592.\n",
      "Better model found at epoch 1 with accuracy value: 0.6000000238418579.\n",
      "Better model found at epoch 2 with accuracy value: 0.678518533706665.\n",
      "Better model found at epoch 3 with accuracy value: 0.7218518257141113.\n",
      "Better model found at epoch 4 with accuracy value: 0.7270370125770569.\n",
      "Better model found at epoch 5 with accuracy value: 0.7725926041603088.\n",
      "Better model found at epoch 6 with accuracy value: 0.7866666913032532.\n",
      "Better model found at epoch 7 with accuracy value: 0.8014814853668213.\n",
      "Better model found at epoch 8 with accuracy value: 0.8055555820465088.\n",
      "Better model found at epoch 9 with accuracy value: 0.8299999833106995.\n",
      "Better model found at epoch 11 with accuracy value: 0.830740749835968.\n",
      "Better model found at epoch 12 with accuracy value: 0.8392592668533325.\n",
      "Better model found at epoch 13 with accuracy value: 0.8533333539962769.\n",
      "Better model found at epoch 14 with accuracy value: 0.8644444346427917.\n",
      "Better model found at epoch 16 with accuracy value: 0.8711110949516296.\n",
      "Better model found at epoch 18 with accuracy value: 0.8729629516601562.\n",
      "Better model found at epoch 20 with accuracy value: 0.8825926184654236.\n",
      "Better model found at epoch 22 with accuracy value: 0.8877778053283691.\n",
      "Better model found at epoch 27 with accuracy value: 0.8903703689575195.\n",
      "Better model found at epoch 28 with accuracy value: 0.8970370292663574.\n",
      "Better model found at epoch 34 with accuracy value: 0.9111111164093018.\n",
      "Better model found at epoch 38 with accuracy value: 0.915925920009613.\n",
      "Better model found at epoch 45 with accuracy value: 0.9196296334266663.\n",
      "Better model found at epoch 48 with accuracy value: 0.9200000166893005.\n",
      "Better model found at epoch 52 with accuracy value: 0.9214814901351929.\n",
      "Better model found at epoch 53 with accuracy value: 0.9237037301063538.\n",
      "Better model found at epoch 54 with accuracy value: 0.9292592406272888.\n",
      "Better model found at epoch 60 with accuracy value: 0.9322222471237183.\n",
      "Better model found at epoch 65 with accuracy value: 0.9337037205696106.\n",
      "Better model found at epoch 72 with accuracy value: 0.9377777576446533.\n",
      "Better model found at epoch 84 with accuracy value: 0.9388889074325562.\n",
      "Better model found at epoch 99 with accuracy value: 0.9437037110328674.\n",
      "Better model found at epoch 110 with accuracy value: 0.9496296048164368.\n",
      "Better model found at epoch 163 with accuracy value: 0.9544444680213928.\n",
      "Better model found at epoch 229 with accuracy value: 0.9551851749420166.\n",
      "Better model found at epoch 230 with accuracy value: 0.9555555582046509.\n",
      "Better model found at epoch 246 with accuracy value: 0.9566666483879089.\n",
      "Better model found at epoch 248 with accuracy value: 0.9592592716217041.\n"
     ]
    }
   ],
   "source": [
    "cb = SaveModelCallback(learn, every='improvement', monitor='accuracy', name='model_1')\n",
    "learn.fit_one_cycle(400, 1e-2, wd=0.0004, callbacks=[cb]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################  Testing  ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = [AGraph(g) for g in glob.glob('nb_graphs/rs/*.dot')]\n",
    "_, comp_graph = cell_graph.generate_comp_graph(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = arch_config(comp_graph=comp_graph,\n",
    "                   depth_coeff=1.0,\n",
    "                   width_coeff=1.0,\n",
    "                   channels=32,\n",
    "                   repeat_list=[3, 2, 1, 1],\n",
    "                   classes=45)\n",
    "\n",
    "net = get_gepnet(conf)\n",
    "count_parameters(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = get_transforms(do_flip=False)\n",
    "path = Path(\"/home/cliff/NWPU-RESISC45\")\n",
    "\n",
    "bs = 64\n",
    "\n",
    "data = (ImageList.from_folder(path)\n",
    "        .split_by_folder(train='train', valid='test')\n",
    "        .label_from_folder()\n",
    "        .transform(tfms, size=224)\n",
    "        .databunch(bs=bs, num_workers=num_cpus())\n",
    "        .normalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_dir = '/home/cliff/ResearchProjects/models/random_search/'\n",
    "model = Learner(data, net, metrics=[accuracy, error_rate]).load(model_dir+'model_0')\n",
    "_, acc, err = model.validate()\n",
    "print('Accuracy: %.2f | Error: %.2f' %(acc.item()*100, err.item()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.validate(data.valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(model)\n",
    "losses,idxs = interp.top_losses()\n",
    "len(data.valid_ds)==len(losses)==len(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_top_losses(9, figsize=(10,5), heatmap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_confusion_matrix(figsize=(8,6), dpi=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds,y,losses = model.get_preds(ds_type=DatasetType.Valid, with_loss=True)\n",
    "#preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('comp_graphs/experiment_1/best/stats.pkl', 'rb') as f:\n",
    "    stats = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
